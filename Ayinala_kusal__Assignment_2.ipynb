{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kusalsai/Ayinala_INFO5731_Spring2025/blob/main/Ayinala_kusal__Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# Define the search query (e.g., \"machine learning\")\n",
        "query = \"machine learning\", \"data science\", \"artifical intelligence\", \"information extraction\"\n",
        "\n",
        "# Define the API endpoint URL\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Set query parameters\n",
        "query_request = {\n",
        "    \"query\": query,  # Search term\n",
        "    \"offset\": 0,  # Starting point for pagination\n",
        "    \"limit\": 100,  # Number of papers to return per request (maximum 100)\n",
        "    \"fields\": \"paperId,title,abstract\"  # Fields to include in the response\n",
        "}\n",
        "\n",
        "headers = {}\n",
        "\n",
        "# List to store the papers\n",
        "papers = []\n",
        "\n",
        "# Counter to track the number of abstracts retrieved\n",
        "total_abstracts = 0\n",
        "max_abstracts = 10000\n",
        "\n",
        "# Retry parameters\n",
        "max_retries = 5  # Maximum number of retries\n",
        "retry_delay = 5  # Initial delay before retrying (in seconds)\n",
        "\n",
        "# Start making requests to the API\n",
        "while total_abstracts < max_abstracts:\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        response = requests.get(url, params=query_request, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            # Process and collect the papers\n",
        "            for idx, paper in enumerate(response_data['data']):\n",
        "                if total_abstracts < max_abstracts:\n",
        "                    papers.append(paper)\n",
        "                    total_abstracts += 1\n",
        "\n",
        "                    # Print paper title and abstract for each\n",
        "                    print(f\"Paper {total_abstracts}:\")\n",
        "                    print(f\"Title: {paper['title']}\")\n",
        "                    print(f\"Abstract: {paper['abstract']}\")\n",
        "                    print(\"----\")\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # If there are more pages of results, update the offset and continue\n",
        "            if 'next' in response_data and response_data['next']:\n",
        "                query_request['offset'] = response_data['next']  # Update offset for next batch\n",
        "            else:\n",
        "                break  # Exit loop if there are no more results\n",
        "            break  # Successfully retrieved the data, break the retry loop\n",
        "\n",
        "        elif response.status_code == 429:  # Rate limit exceeded\n",
        "            retries += 1\n",
        "            print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
        "            time.sleep(retry_delay)  # Wait before retrying\n",
        "            retry_delay *= 2  # Exponential backoff: increase the wait time after each retry\n",
        "        else:\n",
        "            print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "            break\n",
        "\n",
        "    if retries >= max_retries:\n",
        "        print(\"Max retries reached. Exiting.\")\n",
        "        break\n",
        "\n",
        "# Check if we've collected the desired number of abstracts\n",
        "print(f\"Total papers fetched: {total_abstracts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-BYaw3oZSYwA",
        "outputId": "ba746613-ddae-4fb4-f4f1-f4c37d544e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper 1:\n",
            "Title: Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\n",
            "Abstract: We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL\n",
            "----\n",
            "Paper 2:\n",
            "Title: TensorFlow: A system for large-scale machine learning\n",
            "Abstract: TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.\n",
            "----\n",
            "Paper 3:\n",
            "Title: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\n",
            "Abstract: TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.\n",
            "----\n",
            "Paper 4:\n",
            "Title: Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\n",
            "Abstract: None\n",
            "----\n",
            "Paper 5:\n",
            "Title: An Introduction to Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 6:\n",
            "Title: Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting\n",
            "Abstract: The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.\n",
            "----\n",
            "Paper 7:\n",
            "Title: Pattern Recognition And Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 8:\n",
            "Title: Open Graph Benchmark: Datasets for Machine Learning on Graphs\n",
            "Abstract: We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .\n",
            "----\n",
            "Paper 9:\n",
            "Title: A Survey on Bias and Fairness in Machine Learning\n",
            "Abstract: With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.\n",
            "----\n",
            "Paper 10:\n",
            "Title: Physics-informed machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 11:\n",
            "Title: Membership Inference Attacks Against Machine Learning Models\n",
            "Abstract: We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.\n",
            "----\n",
            "Paper 12:\n",
            "Title: Foundations of Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 13:\n",
            "Title: Machine Learning: Algorithms, Real-World Applications and Research Directions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 14:\n",
            "Title: Scikit-learn: Machine Learning in Python\n",
            "Abstract: Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.\n",
            "----\n",
            "Paper 15:\n",
            "Title: Machine learning: Trends, perspectives, and prospects\n",
            "Abstract: Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.\n",
            "----\n",
            "Paper 16:\n",
            "Title: Towards A Rigorous Science of Interpretable Machine Learning\n",
            "Abstract: As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.\n",
            "----\n",
            "Paper 17:\n",
            "Title: C4.5: Programs for Machine Learning\n",
            "Abstract: From the Publisher: \n",
            "Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n",
            " \n",
            "C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n",
            " \n",
            "This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.\n",
            "----\n",
            "Paper 18:\n",
            "Title: Data Mining Practical Machine Learning Tools and Techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 19:\n",
            "Title: Machine learning - a probabilistic perspective\n",
            "Abstract: All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. Machine learning : a probabilistic perspective / Kevin P. Murphy. p. cm. — (Adaptive computation and machine learning series) Includes bibliographical references and index. Contents Preface xxvii 1 Introduction 1 1.1 Machine learning: what and why? 1 1.1.1 Types of machine learning 2 1.2 Supervised learning 3 1.2.1 Classification 3 1.2.2 Regression 8 1.3 Unsupervised learning 9 1.3.1 Discovering clusters 10 1.3.2 Discovering latent factors 11 1.3.3 Discovering graph structure 13 1.3.4 Matrix completion 14 1.4 Some basic concepts in machine learning 16 1.4.1 Parametric vs non-parametric models 16 1.4.2 A simple non-parametric classifier: K-nearest neighbors 16 1.4.3 The curse of dimensionality 18 1.4.4 Parametric models for classification and regression 19 1.4.5\n",
            "----\n",
            "Paper 20:\n",
            "Title: Genetic Algorithms in Search Optimization and Machine Learning\n",
            "Abstract: From the Publisher: \n",
            "This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n",
            " \n",
            "Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.\n",
            "----\n",
            "Paper 21:\n",
            "Title: Pattern Recognition and Machine Learning\n",
            "Abstract: Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.\n",
            "----\n",
            "Paper 22:\n",
            "Title: Interpretable Machine Learning\n",
            "Abstract: Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.\n",
            "----\n",
            "Paper 23:\n",
            "Title: Practical Black-Box Attacks against Machine Learning\n",
            "Abstract: Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.\n",
            "----\n",
            "Paper 24:\n",
            "Title: Practical Bayesian Optimization of Machine Learning Algorithms\n",
            "Abstract: The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.\n",
            "----\n",
            "Paper 25:\n",
            "Title: Machine Learning Algorithms: A Review\n",
            "Abstract: .\n",
            "----\n",
            "Paper 26:\n",
            "Title: Thumbs up? Sentiment Classification using Machine Learning Techniques\n",
            "Abstract: We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.\n",
            "----\n",
            "Paper 27:\n",
            "Title: UCI Repository of machine learning databases\n",
            "Abstract: None\n",
            "----\n",
            "Paper 28:\n",
            "Title: The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]\n",
            "Abstract: In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.\n",
            "----\n",
            "Paper 29:\n",
            "Title: Machine learning for molecular and materials science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 30:\n",
            "Title: Programs for Machine Learning\n",
            "Abstract: Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.\n",
            "----\n",
            "Paper 31:\n",
            "Title: Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n",
            "Abstract: Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details\n",
            "----\n",
            "Paper 32:\n",
            "Title: Machine learning in automated text categorization\n",
            "Abstract: The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.\n",
            "----\n",
            "Paper 33:\n",
            "Title: Adversarial Machine Learning at Scale\n",
            "Abstract: Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.\n",
            "----\n",
            "Paper 34:\n",
            "Title: Large-Scale Machine Learning with Stochastic Gradient Descent\n",
            "Abstract: None\n",
            "----\n",
            "Paper 35:\n",
            "Title: Small data machine learning in materials science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 36:\n",
            "Title: Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning\n",
            "Abstract: Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series\n",
            "----\n",
            "Paper 37:\n",
            "Title: Optimization Methods for Large-Scale Machine Learning\n",
            "Abstract: This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.\n",
            "----\n",
            "Paper 38:\n",
            "Title: Neural Machine Translation by Jointly Learning to Align and Translate\n",
            "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.\n",
            "----\n",
            "Paper 39:\n",
            "Title: Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\n",
            "Abstract: In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.\n",
            "----\n",
            "Paper 40:\n",
            "Title: Machine Learning for High-Speed Corner Detection\n",
            "Abstract: None\n",
            "----\n",
            "Paper 41:\n",
            "Title: Explainable AI: A Review of Machine Learning Interpretability Methods\n",
            "Abstract: Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.\n",
            "----\n",
            "Paper 42:\n",
            "Title: Double/Debiased Machine Learning for Treatment and Structural Parameters\n",
            "Abstract: We revisit the classic semiparametric problem of inference on a low dimensional parameter θ_0 in the presence of high-dimensional nuisance parameters η_0. We depart from the classical setting by allowing for η_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η_0 cause a heavy bias in estimators of θ_0 that are obtained by naively plugging ML estimators of η_0 into estimating equations for θ_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.\n",
            "----\n",
            "Paper 43:\n",
            "Title: SoilGrids250m: Global gridded soil information based on machine learning\n",
            "Abstract: This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.\n",
            "----\n",
            "Paper 44:\n",
            "Title: Federated Learning: Collaborative Machine Learning without\n",
            "Centralized Training Data\n",
            "Abstract: Federated learning (also known as collaborative learning) is a machine learning technique that trains\n",
            "an algorithm without transferring data samples across numerous decentralized edge devices or\n",
            "servers. This strategy differs from standard centralized machine learning techniques in which all local\n",
            "datasets are uploaded to a single server, as well as more traditional decentralized alternatives, which\n",
            "frequently presume that local data samples are uniformly distributed.\n",
            "Federated learning allows several actors to collaborate on the development of a single, robust\n",
            "machine learning model without sharing data, allowing crucial issues such as data privacy, data\n",
            "security, data access rights, and access to heterogeneous data to be addressed. Defence,\n",
            "telecommunications, internet of things, and pharmaceutical industries are just a few of the sectors\n",
            "where it has applications.\n",
            "----\n",
            "Paper 45:\n",
            "Title: Practical Secure Aggregation for Privacy-Preserving Machine Learning\n",
            "Abstract: We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.\n",
            "----\n",
            "Paper 46:\n",
            "Title: Multimodal Machine Learning: A Survey and Taxonomy\n",
            "Abstract: Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.\n",
            "----\n",
            "Paper 47:\n",
            "Title: ilastik: interactive machine learning for (bio)image analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 48:\n",
            "Title: On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice\n",
            "Abstract: None\n",
            "----\n",
            "Paper 49:\n",
            "Title: Correlation-based Feature Selection for Machine Learning\n",
            "Abstract: A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.\n",
            "----\n",
            "Paper 50:\n",
            "Title: CrypTen: Secure Multi-Party Computation Meets Machine Learning\n",
            "Abstract: Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that\"speak the language\"of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.\n",
            "----\n",
            "Paper 51:\n",
            "Title: Applications of machine learning to machine fault diagnosis: A review and roadmap\n",
            "Abstract: None\n",
            "----\n",
            "Paper 52:\n",
            "Title: Machine learning–accelerated computational fluid dynamics\n",
            "Abstract: Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.\n",
            "----\n",
            "Paper 53:\n",
            "Title: Supervised Machine Learning: A Review of Classification Techniques\n",
            "Abstract: The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.\n",
            "----\n",
            "Paper 54:\n",
            "Title: Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent\n",
            "Abstract: We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.\n",
            "----\n",
            "Paper 55:\n",
            "Title: The use of the area under the ROC curve in the evaluation of machine learning algorithms\n",
            "Abstract: None\n",
            "----\n",
            "Paper 56:\n",
            "Title: Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)\n",
            "Abstract: None\n",
            "----\n",
            "Paper 57:\n",
            "Title: Federated Machine Learning\n",
            "Abstract: Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.\n",
            "----\n",
            "Paper 58:\n",
            "Title: Machine learning-aided engineering of hydrolases for PET depolymerization\n",
            "Abstract: None\n",
            "----\n",
            "Paper 59:\n",
            "Title: A Review on Fairness in Machine Learning\n",
            "Abstract: An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.\n",
            "----\n",
            "Paper 60:\n",
            "Title: Educational data mining: prediction of students' academic performance using machine learning algorithms\n",
            "Abstract: None\n",
            "----\n",
            "Paper 61:\n",
            "Title: Human-in-the-loop machine learning: a state of the art\n",
            "Abstract: None\n",
            "----\n",
            "Paper 62:\n",
            "Title: Challenges and opportunities in quantum machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 63:\n",
            "Title: Classification Based on Decision Tree Algorithm for Machine Learning\n",
            "Abstract: Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.\n",
            "----\n",
            "Paper 64:\n",
            "Title: SecureML: A System for Scalable Privacy-Preserving Machine Learning\n",
            "Abstract: Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.\n",
            "----\n",
            "Paper 65:\n",
            "Title: MoleculeNet: a benchmark for molecular machine learning\n",
            "Abstract: A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.\n",
            "----\n",
            "Paper 66:\n",
            "Title: A guide to machine learning for biologists\n",
            "Abstract: None\n",
            "----\n",
            "Paper 67:\n",
            "Title: Machine learning and deep learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 68:\n",
            "Title: Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges\n",
            "Abstract: Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the\"Rashomon set\"of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.\n",
            "----\n",
            "Paper 69:\n",
            "Title: MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers\n",
            "Abstract: Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.\n",
            "----\n",
            "Paper 70:\n",
            "Title: Understanding Machine Learning - From Theory to Algorithms\n",
            "Abstract: Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability ; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning ; and emerging theoretical concepts such as the PACBayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.\n",
            "----\n",
            "Paper 71:\n",
            "Title: Reconciling modern machine-learning practice and the classical bias–variance trade-off\n",
            "Abstract: Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.\n",
            "----\n",
            "Paper 72:\n",
            "Title: Machine Learning With Python\n",
            "Abstract: None\n",
            "----\n",
            "Paper 73:\n",
            "Title: Adversarial machine learning\n",
            "Abstract: In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.\n",
            "----\n",
            "Paper 74:\n",
            "Title: Power of data in quantum machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 75:\n",
            "Title: Universal Differential Equations for Scientific Machine Learning\n",
            "Abstract: \n",
            " In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring \"big data\". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.\n",
            "----\n",
            "Paper 76:\n",
            "Title: Heart Disease Prediction using Machine Learning Techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 77:\n",
            "Title: Fairness in Machine Learning: A Survey\n",
            "Abstract: When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.\n",
            "----\n",
            "Paper 78:\n",
            "Title: Counterfactual Explanations for Machine Learning: A Review\n",
            "Abstract: Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.\n",
            "----\n",
            "Paper 79:\n",
            "Title: Stable learning establishes some common ground between causal inference and machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 80:\n",
            "Title: Gaussian Processes in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 81:\n",
            "Title: Machine Learning for Fluid Mechanics\n",
            "Abstract: The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.\n",
            "----\n",
            "Paper 82:\n",
            "Title: Definitions, methods, and applications in interpretable machine learning\n",
            "Abstract: Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.\n",
            "----\n",
            "Paper 83:\n",
            "Title: MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems\n",
            "Abstract: MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. \n",
            "This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.\n",
            "----\n",
            "Paper 84:\n",
            "Title: Recent advances and applications of machine learning in solid-state materials science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 85:\n",
            "Title: Applications of machine learning in drug discovery and development\n",
            "Abstract: None\n",
            "----\n",
            "Paper 86:\n",
            "Title: Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.\n",
            "Abstract: The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.\n",
            "----\n",
            "Paper 87:\n",
            "Title: Machine Learning Interpretability: A Survey on Methods and Metrics\n",
            "Abstract: Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.\n",
            "----\n",
            "Paper 88:\n",
            "Title: Swarm Learning for decentralized and confidential clinical machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 89:\n",
            "Title: A survey on missing data in machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 90:\n",
            "Title: Machine Learning in Agriculture: A Comprehensive Updated Review\n",
            "Abstract: The digital transformation of agriculture has evolved various aspects of management into artificial intelligent systems for the sake of making value from the ever-increasing data originated from numerous sources. A subset of artificial intelligence, namely machine learning, has a considerable potential to handle numerous challenges in the establishment of knowledge-based farming systems. The present study aims at shedding light on machine learning in agriculture by thoroughly reviewing the recent scholarly literature based on keywords’ combinations of “machine learning” along with “crop management”, “water management”, “soil management”, and “livestock management”, and in accordance with PRISMA guidelines. Only journal papers were considered eligible that were published within 2018–2020. The results indicated that this topic pertains to different disciplines that favour convergence research at the international level. Furthermore, crop management was observed to be at the centre of attention. A plethora of machine learning algorithms were used, with those belonging to Artificial Neural Networks being more efficient. In addition, maize and wheat as well as cattle and sheep were the most investigated crops and animals, respectively. Finally, a variety of sensors, attached on satellites and unmanned ground and aerial vehicles, have been utilized as a means of getting reliable input data for the data analyses. It is anticipated that this study will constitute a beneficial guide to all stakeholders towards enhancing awareness of the potential advantages of using machine learning in agriculture and contributing to a more systematic research on this topic.\n",
            "----\n",
            "Paper 91:\n",
            "Title: Perspectives in machine learning for wildlife conservation\n",
            "Abstract: None\n",
            "----\n",
            "Paper 92:\n",
            "Title: Machine learning and the physical sciences\n",
            "Abstract: Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.\n",
            "----\n",
            "Paper 93:\n",
            "Title: Combining Machine Learning and Computational Chemistry for Predictive Insights Into Chemical Systems\n",
            "Abstract: Machine learning models are poised to make a transformative impact on chemical sciences by dramatically accelerating computational algorithms and amplifying insights available from computational chemistry methods. However, achieving this requires a confluence and coaction of expertise in computer science and physical sciences. This Review is written for new and experienced researchers working at the intersection of both fields. We first provide concise tutorials of computational chemistry and machine learning methods, showing how insights involving both can be achieved. We follow with a critical review of noteworthy applications that demonstrate how computational chemistry and machine learning can be used together to provide insightful (and useful) predictions in molecular and materials modeling, retrosyntheses, catalysis, and drug design.\n",
            "----\n",
            "Paper 94:\n",
            "Title: Machine Learning Basics\n",
            "Abstract: coined in 1959 by Arthur Samuel [Samuel 1959], Tom Mitchell [Mitchell 1997] provided a more formal definition: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” ML has be applied to many real-world problems or tasks, like medical diagno­ sis, robotics, recommendation systems, facial recognition, stock prices prediction, and sentiment analysis, with great success. We can divide ML algorithms into three main categories (see Figure 4.1): Machine Learning Basics\n",
            "----\n",
            "Paper 95:\n",
            "Title: Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans\n",
            "Abstract: None\n",
            "----\n",
            "Paper 96:\n",
            "Title: Enhancing computational fluid dynamics with machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 97:\n",
            "Title: Machine Learning in Drug Discovery: A Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 98:\n",
            "Title: Machine learning pipeline for battery state-of-health estimation\n",
            "Abstract: None\n",
            "----\n",
            "Paper 99:\n",
            "Title: Fairness in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 100:\n",
            "Title: Machine Learning in Agriculture: A Review\n",
            "Abstract: Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.\n",
            "----\n",
            "Paper 101:\n",
            "Title: Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning\n",
            "Abstract: Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.\n",
            "----\n",
            "Paper 102:\n",
            "Title: Machine learning for alloys\n",
            "Abstract: None\n",
            "----\n",
            "Paper 103:\n",
            "Title: All-optical machine learning using diffractive deep neural networks\n",
            "Abstract: All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.\n",
            "----\n",
            "Paper 104:\n",
            "Title: Empirical Asset Pricing Via Machine Learning\n",
            "Abstract: \n",
            " We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.\n",
            " Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.\n",
            "----\n",
            "Paper 105:\n",
            "Title: The Machine‐Learning Approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 106:\n",
            "Title: Coronavirus disease (COVID-19) cases analysis using machine-learning applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 107:\n",
            "Title: Mathematics for Machine Learning\n",
            "Abstract: Machine learning is a way to study the algorithm and statistical model that is used by computer to perform a specific task through pattern and deduction [1]. It builds a mathematical model from a sample data which may come under either supervised or unsupervised learning. It is closely\n",
            " related to computational statistics which is an interface between statistics and computer science. Also, linear algebra and probability theory are two tools of mathematics which form the basis of machine learning. In general, statistics is a science concerned with collecting, analysing, interpreting\n",
            " the data. Data are the facts and figure that can be classified as either quantitative or qualitative. From the given set of data, we can predict the expected observation, difference between the outcome of two observations and how data look like which can help in better decision making process\n",
            " [2]. Descriptive and inferential statistics are the two methods of data analysis. Descriptive statistics summarize the raw data into information through which common expectation and variation of data can be taken. It also provides graphical methods that can be used to visualize the sample\n",
            " of data and qualitative understanding of observation whereas inferential statistics refers to drawing conclusions from data. Inferences are made under the framework of probability theory. So, understanding of data and interpretation of result are two important aspects of machine learning.\n",
            " In this paper, we have reviewed the different methods of ML, mathematics behind ML, its application in day to day life and future aspects.\n",
            "----\n",
            "Paper 108:\n",
            "Title: Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020\n",
            "Abstract: This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open-source black-box optimization packages as well as random search.\n",
            "----\n",
            "Paper 109:\n",
            "Title: Extreme learning machine: Theory and applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 110:\n",
            "Title: Understanding Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 111:\n",
            "Title: Best practices in machine learning for chemistry\n",
            "Abstract: None\n",
            "----\n",
            "Paper 112:\n",
            "Title: Machine Learning in Healthcare\n",
            "Abstract: None\n",
            "----\n",
            "Paper 113:\n",
            "Title: Extreme Learning Machine for Regression and Multiclass Classification\n",
            "Abstract: Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.\n",
            "----\n",
            "Paper 114:\n",
            "Title: Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning\n",
            "Abstract: Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: this https URL.\n",
            "----\n",
            "Paper 115:\n",
            "Title: Underspecification Presents Challenges for Credibility in Modern Machine Learning\n",
            "Abstract: ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.\n",
            "----\n",
            "Paper 116:\n",
            "Title: Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods\n",
            "Abstract: None\n",
            "----\n",
            "Paper 117:\n",
            "Title: A Survey of Human-in-the-loop for Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 118:\n",
            "Title: Quantum embeddings for machine learning\n",
            "Abstract: Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.\n",
            "----\n",
            "Paper 119:\n",
            "Title: A Survey on the Explainability of Supervised Machine Learning\n",
            "Abstract: Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.\n",
            "----\n",
            "Paper 120:\n",
            "Title: A Review on Linear Regression Comprehensive in Machine Learning\n",
            "Abstract: Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables.\n",
            "----\n",
            "Paper 121:\n",
            "Title: Machine Learning Force Fields\n",
            "Abstract: In recent years, the use of machine learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail, and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.\n",
            "----\n",
            "Paper 122:\n",
            "Title: Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence\n",
            "Abstract: Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.\n",
            "----\n",
            "Paper 123:\n",
            "Title: Introduction to Machine Learning, Neural Networks, and Deep Learning\n",
            "Abstract: Purpose To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.\n",
            "----\n",
            "Paper 124:\n",
            "Title: Predicting the state of charge and health of batteries using data-driven machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 125:\n",
            "Title: Tslearn, A Machine Learning Toolkit for Time Series Data\n",
            "Abstract: tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.\n",
            "----\n",
            "Paper 126:\n",
            "Title: Principles and Practice of Explainable Machine Learning\n",
            "Abstract: Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.\n",
            "----\n",
            "Paper 127:\n",
            "Title: An open source machine learning framework for efficient and transparent systematic reviews\n",
            "Abstract: None\n",
            "----\n",
            "Paper 128:\n",
            "Title: A Comprehensive Survey of Loss Functions in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 129:\n",
            "Title: Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\n",
            "Abstract: Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .\n",
            "----\n",
            "Paper 130:\n",
            "Title: A study of the behavior of several methods for balancing machine learning training data\n",
            "Abstract: There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.\n",
            "----\n",
            "Paper 131:\n",
            "Title: Explaining Explanations: An Overview of Interpretability of Machine Learning\n",
            "Abstract: There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.\n",
            "----\n",
            "Paper 132:\n",
            "Title: Challenges in Deploying Machine Learning: A Survey of Case Studies\n",
            "Abstract: In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.\n",
            "----\n",
            "Paper 133:\n",
            "Title: Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)\n",
            "Abstract: One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.\n",
            "----\n",
            "Paper 134:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 135:\n",
            "Title: Ethical Machine Learning in Health Care\n",
            "Abstract: The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.\n",
            "----\n",
            "Paper 136:\n",
            "Title: Automatic differentiation in machine learning: a survey\n",
            "Abstract: Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply “auto-diff”, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until \n",
            "very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other’s results. Despite its \n",
            "relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational \n",
            "graphs” and “differentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main imple- \n",
            "mentation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiff”, “automatic differentiation”, and “symbolic differentiation” as these are encountered more and more in machine learning settings.\n",
            "----\n",
            "Paper 137:\n",
            "Title: Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification\n",
            "Abstract: Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.\n",
            "----\n",
            "Paper 138:\n",
            "Title: Advancing Biosensors with Machine Learning.\n",
            "Abstract: Chemometrics play a critical role in biosensors-based detection, analysis, and diagnosis. Nowadays, as a branch of artificial intelligence (AI), machine learning (ML) have achieved impressive advances. However, novel advanced ML methods, especially deep learning, which is famous for image analysis, facial recognition, and speech recognition, has remained relatively elusive to the biosensor community. Herein, how ML can be beneficial to biosensors is systematically discussed. The advantages and drawbacks of most popular ML algorithms are summarized on the basis of sensing data analysis. Specially, deep learning methods such as convolutional neural network (CNN) and recurrent neural network (RNN) are emphasized. Diverse ML-assisted electrochemical biosensors, wearable electronics, SERS and other spectra-based biosensors, fluorescence biosensors and colorimetric biosensors are comprehensively discussed. Furthermore, biosensor networks and multibiosensor data fusion are introduced. This review will nicely bridge ML with biosensors, and greatly expand chemometrics for detection, analysis, and diagnosis.\n",
            "----\n",
            "Paper 139:\n",
            "Title: Applications of machine learning to diagnosis and treatment of neurodegenerative diseases\n",
            "Abstract: None\n",
            "----\n",
            "Paper 140:\n",
            "Title: Integrating Physics-Based Modeling with Machine Learning: A Survey\n",
            "Abstract: In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.\n",
            "----\n",
            "Paper 141:\n",
            "Title: Quantum Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 142:\n",
            "Title: Federated Optimization: Distributed Machine Learning for On-Device Intelligence\n",
            "Abstract: We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. \n",
            "A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. \n",
            "We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \\federated optimization.\n",
            "----\n",
            "Paper 143:\n",
            "Title: Implementation of machine-learning classification in remote sensing: an applied review\n",
            "Abstract: ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.\n",
            "----\n",
            "Paper 144:\n",
            "Title: Darts: User-Friendly Modern Machine Learning for Time Series\n",
            "Abstract: We present Darts, a Python machine learning library for time series, with a focus on forecasting. Darts offers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on offering modern machine learning functionalities, such as supporting multidimensional series, meta-learning on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn.\n",
            "----\n",
            "Paper 145:\n",
            "Title: Data Shapley: Equitable Valuation of Data for Machine Learning\n",
            "Abstract: As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.\n",
            "----\n",
            "Paper 146:\n",
            "Title: Reproducibility in machine learning for health research: Still a ways to go\n",
            "Abstract: Machine learning applied to health falls short on several reproducibility metrics compared to other machine learning subfields. Machine learning for health must be reproducible to ensure reliable clinical use. We evaluated 511 scientific papers across several machine learning subfields and found that machine learning for health compared poorly to other areas regarding reproducibility metrics, such as dataset and code accessibility. We propose recommendations to address this problem.\n",
            "----\n",
            "Paper 147:\n",
            "Title: Machine Learning for Chemical Reactions.\n",
            "Abstract: Machine learning (ML) techniques applied to chemical reactions have a long history. The present contribution discusses applications ranging from small molecule reaction dynamics to computational platforms for reaction planning. ML-based techniques can be particularly relevant for problems involving both computation and experiments. For one, Bayesian inference is a powerful approach to develop models consistent with knowledge from experiments. Second, ML-based methods can also be used to handle problems that are formally intractable using conventional approaches, such as exhaustive characterization of state-to-state information in reactive collisions. Finally, the explicit simulation of reactive networks as they occur in combustion has become possible using machine-learned neural network potentials. This review provides an overview of the questions that can and have been addressed using machine learning techniques, and an outlook discusses challenges in this diverse and stimulating field. It is concluded that ML applied to chemistry problems as practiced and conceived today has the potential to transform the way with which the field approaches problems involving chemical reactions, in both research and academic teaching.\n",
            "----\n",
            "Paper 148:\n",
            "Title: DOME: recommendations for supervised machine learning validation in biology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 149:\n",
            "Title: Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\n",
            "Abstract: None\n",
            "----\n",
            "Paper 150:\n",
            "Title: Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting\n",
            "Abstract: Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.\n",
            "----\n",
            "Paper 151:\n",
            "Title: Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques\n",
            "Abstract: Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).\n",
            "----\n",
            "Paper 152:\n",
            "Title: Machine Learning in Medicine\n",
            "Abstract: Spurred by advances in processing power, memory, storage, and an unprecedented wealth of data, computers are being asked to tackle increasingly complex learning tasks, often with astonishing success. Computers have now mastered a popular variant of poker, learned the laws of physics from experimental data, and become experts in video games - tasks that would have been deemed impossible not too long ago. In parallel, the number of companies centered on applying complex data analysis to varying industries has exploded, and it is thus unsurprising that some analytic companies are turning attention to problems in health care. The purpose of this review is to explore what problems in medicine might benefit from such learning approaches and use examples from the literature to introduce basic concepts in machine learning. It is important to note that seemingly large enough medical data sets and adequate learning algorithms have been available for many decades, and yet, although there are thousands of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Thus, part of my effort will be to identify what obstacles there may be to changing the practice of medicine through statistical learning approaches, and discuss how these might be overcome.\n",
            "----\n",
            "Paper 153:\n",
            "Title: Dlib-ml: A Machine Learning Toolkit\n",
            "Abstract: There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.\n",
            "----\n",
            "Paper 154:\n",
            "Title: Secure, privacy-preserving and federated machine learning in medical imaging\n",
            "Abstract: None\n",
            "----\n",
            "Paper 155:\n",
            "Title: A survey on machine learning for data fusion\n",
            "Abstract: None\n",
            "----\n",
            "Paper 156:\n",
            "Title: Stealing Machine Learning Models via Prediction APIs\n",
            "Abstract: Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \n",
            "The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.\n",
            "----\n",
            "Paper 157:\n",
            "Title: PyTorch: An Imperative Style, High-Performance Deep Learning Library\n",
            "Abstract: Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.\n",
            "----\n",
            "Paper 158:\n",
            "Title: Data Mining Practical Machine Learning Tools And Techniques With Java Implementations\n",
            "Abstract: Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their laptop.\n",
            "----\n",
            "Paper 159:\n",
            "Title: Automated Machine Learning - Methods, Systems, Challenges\n",
            "Abstract: None\n",
            "----\n",
            "Paper 160:\n",
            "Title: Machine learning and algorithmic fairness in public and population health\n",
            "Abstract: None\n",
            "----\n",
            "Paper 161:\n",
            "Title: Machine Learning with a Reject Option: A survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 162:\n",
            "Title: Software Engineering for Machine Learning: A Case Study\n",
            "Abstract: Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.\n",
            "----\n",
            "Paper 163:\n",
            "Title: Statistical Learning Theory\n",
            "Abstract: None\n",
            "----\n",
            "Paper 164:\n",
            "Title: Edge Machine Learning for AI-Enabled IoT Devices: A Review\n",
            "Abstract: In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors’ data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning “Hello World”.\n",
            "----\n",
            "Paper 165:\n",
            "Title: Efficient and Robust Automated Machine Learning\n",
            "Abstract: The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.\n",
            "----\n",
            "Paper 166:\n",
            "Title: Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples\n",
            "Abstract: Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.\n",
            "----\n",
            "Paper 167:\n",
            "Title: Feature selection in machine learning: A new perspective\n",
            "Abstract: None\n",
            "----\n",
            "Paper 168:\n",
            "Title: A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection\n",
            "Abstract: Cyber security is that the body of technologies, processes and practices designed to safeguard networks, computers, programs and knowledge from attack, harm or unauthorized access. During a computing context, the term security implies cyber security. This survey paper describes a targeted literature survey of machine learning (ML) and data processing (DM) strategies for cyber analytics in support of intrusion detection. This paper focuses totally on cyber intrusion detection as it applies to wired networks. With a wired network, associate oppose must experience many layers of defense at firewalls and operative systems, or gain physical access to the network. The quality of ML/DM algorithms is addressed, discussion of challenges for victimization ML/DM for cyber security is conferred, and some recommendations on once to use a given methodology area unit provided.\n",
            "----\n",
            "Paper 169:\n",
            "Title: Big Data and Machine Learning in Health Care.\n",
            "Abstract: Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT\n",
            "----\n",
            "Paper 170:\n",
            "Title: ABY3: A Mixed Protocol Framework for Machine Learning\n",
            "Abstract: Machine learning is widely used to produce models for a range of applications and is increasingly offered as a service by major technology companies. However, the required massive data collection raises privacy concerns during both training and prediction stages. In this paper, we design and implement a general framework for privacy-preserving machine learning and use it to obtain new solutions for training linear regression, logistic regression and neural network models. Our protocols are in a three-server model wherein data owners secret share their data among three servers who train and evaluate models on the joint data using three-party computation (3PC). Our main contribution is a new and complete framework ($\\textABY ^3$) for efficiently switching back and forth between arithmetic, binary, and Yao 3PC which is of independent interest. Many of the conversions are based on new techniques that are designed and optimized for the first time in this paper. We also propose new techniques for fixed-point multiplication of shared decimal values that extends beyond the three-party case, and customized protocols for evaluating piecewise polynomial functions. We design variants of each building block that is secure against \\em malicious adversaries who deviate arbitrarily. We implement our system in C++. Our protocols are up to \\em four orders of magnitude faster than the best prior work, hence significantly reducing the gap between privacy-preserving and plaintext training.\n",
            "----\n",
            "Paper 171:\n",
            "Title: Plans and Situated Actions: The Problem of Human-Machine Communication (Learning in Doing: Social,\n",
            "Abstract: Preface Acknowledgements 1. Introduction 2. Interactive artefacts 3. Plans 4. Situated actions 5. Communicative resources 6. Case and methods 7. Human-machine communication 8. Conclusion References Indices.\n",
            "----\n",
            "Paper 172:\n",
            "Title: Machine Learning Testing: Survey, Landscapes and Horizons\n",
            "Abstract: This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.\n",
            "----\n",
            "Paper 173:\n",
            "Title: Quantum machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 174:\n",
            "Title: Quantifying the Carbon Emissions of Machine Learning\n",
            "Abstract: From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.\n",
            "----\n",
            "Paper 175:\n",
            "Title: Introduction to Machine Learning with Python\n",
            "Abstract: None\n",
            "----\n",
            "Paper 176:\n",
            "Title: Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon\n",
            "Abstract: None\n",
            "----\n",
            "Paper 177:\n",
            "Title: AutoML-Zero: Evolving Machine Learning Algorithms From Scratch\n",
            "Abstract: Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.\n",
            "----\n",
            "Paper 178:\n",
            "Title: Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning\n",
            "Abstract: Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.\n",
            "----\n",
            "Paper 179:\n",
            "Title: Machine Learning: An Applied Econometric Approach\n",
            "Abstract: Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.\n",
            "----\n",
            "Paper 180:\n",
            "Title: Comparing different supervised machine learning algorithms for disease prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 181:\n",
            "Title: Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization\n",
            "Abstract: Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.\n",
            "----\n",
            "Paper 182:\n",
            "Title: Some Studies in Machine Learning Using the Game of Checkers\n",
            "Abstract: Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the\n",
            "----\n",
            "Paper 183:\n",
            "Title: The Non-IID Data Quagmire of Decentralized Machine Learning\n",
            "Abstract: Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.\n",
            "----\n",
            "Paper 184:\n",
            "Title: Opportunities and Challenges for Machine Learning in Materials Science\n",
            "Abstract: Advances in machine learning have impacted myriad areas of materials science, such as the discovery of novel materials and the improvement of molecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide a more detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.\n",
            "----\n",
            "Paper 185:\n",
            "Title: A few useful things to know about machine learning\n",
            "Abstract: Tapping into the \"folk knowledge\" needed to advance machine learning applications.\n",
            "----\n",
            "Paper 186:\n",
            "Title: Certified Data Removal from Machine Learning Models\n",
            "Abstract: Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to \"remove\" data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.\n",
            "----\n",
            "Paper 187:\n",
            "Title: Machine Learning for Precision Medicine.\n",
            "Abstract: Precision medicine is an emerging approach to clinical research and patient care that focuses on understanding and treating disease by integrating multimodal or 'multi-omics' data from an individual to make patient-tailored decisions. With the large and complex datasets generated using precision medicine diagnostic approaches, novel techniques to process and understand these complex data were needed. At the same time, computer science has progressed rapidly to develop techniques that enable the storage, processing, and analysis of these complex datasets, a feat that traditional statistics and early computing technologies could not accomplish. Machine learning, a branch of artificial intelligence, is a computer science methodology that aims to identify complex patterns in data that can be used to make predictions or classifications on new unseen data or for advanced exploratory data analysis. Machine learning analysis of precision medicine's multimodal data allows for broad analysis of large datasets and ultimately a greater understanding of human health and disease. This review focuses on machine learning utilization for precision medicine's \"big data\", in the context of genetics, genomics, and beyond.\n",
            "----\n",
            "Paper 188:\n",
            "Title: Machine Learning in Medicine\n",
            "Abstract: Machine Learning in Medicine In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. The...\n",
            "----\n",
            "Paper 189:\n",
            "Title: Machine learning algorithm validation with a limited sample size\n",
            "Abstract: Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.\n",
            "----\n",
            "Paper 190:\n",
            "Title: River: machine learning for streaming data in Python\n",
            "Abstract: River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of the two most popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.\n",
            "----\n",
            "Paper 191:\n",
            "Title: DeltaGrad: Rapid retraining of machine learning models\n",
            "Abstract: Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.\n",
            "----\n",
            "Paper 192:\n",
            "Title: Machine Learning Methods in Drug Discovery\n",
            "Abstract: The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.\n",
            "----\n",
            "Paper 193:\n",
            "Title: Machine Learning\n",
            "Abstract: El sector salud tiene involucrado una gran cantidad de procesos y procedimientos generadores de todo tipo de información que en muchos casos no están disponibles de forma libre para los profesionales de diferentes áreas y en especial de las ciencias computacionales.¿Qué sucedería si toda esta información pudiera estar disponible? La medicina preventiva y predictiva podría desarrollarse con mayor rapidez, desarrollando modelos predictivos a través de algoritmos de Machine Learning, como apoyo a los profesionales de la salud en la toma de decisiones. Este artículo permite conocer la convergencia que existe entre la medicina predictiva y el Machine Learning, sus ventajas y los diferentes algoritmos de Machine Learning que se pueden aplicar dependiendo de los tipos de datos.\n",
            "----\n",
            "Paper 194:\n",
            "Title: Quantum Machine Learning in Feature Hilbert Spaces.\n",
            "Abstract: A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.\n",
            "----\n",
            "Paper 195:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 196:\n",
            "Title: How the machine ‘thinks’: Understanding opacity in machine learning algorithms\n",
            "Abstract: This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.\n",
            "----\n",
            "Paper 197:\n",
            "Title: When Machine Learning Meets Privacy\n",
            "Abstract: The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.\n",
            "----\n",
            "Paper 198:\n",
            "Title: Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology\n",
            "Abstract: Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.\n",
            "----\n",
            "Paper 199:\n",
            "Title: Machine learning for active matter\n",
            "Abstract: None\n",
            "----\n",
            "Paper 200:\n",
            "Title: Parameterized quantum circuits as machine learning models\n",
            "Abstract: Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.\n",
            "----\n",
            "Paper 201:\n",
            "Title: When Machine Learning Meets Privacy\n",
            "Abstract: The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.\n",
            "----\n",
            "Paper 202:\n",
            "Title: Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology\n",
            "Abstract: Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.\n",
            "----\n",
            "Paper 203:\n",
            "Title: Machine learning for active matter\n",
            "Abstract: None\n",
            "----\n",
            "Paper 204:\n",
            "Title: Parameterized quantum circuits as machine learning models\n",
            "Abstract: Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.\n",
            "----\n",
            "Paper 205:\n",
            "Title: Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey\n",
            "Abstract: Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.\n",
            "----\n",
            "Paper 206:\n",
            "Title: A Survey on Distributed Machine Learning\n",
            "Abstract: The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.\n",
            "----\n",
            "Paper 207:\n",
            "Title: Machine Learning Methods That Economists Should Know About\n",
            "Abstract: We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.\n",
            "----\n",
            "Paper 208:\n",
            "Title: A Quick Review of Machine Learning Algorithms\n",
            "Abstract: Machine learning is predominantly an area of Artificial Intelligence which has been a key component of digitalization solutions that has caught major attention in the digital arena. In this paper author intends to do a brief review of various machine learning algorithms which are most frequently used and therefore are the most popular ones. The author intends to highlight the merits and demerits of the machine learning algorithms from their application perspective to aid in an informed decision making towards selecting the appropriate learning algorithm to meet the specific requirement of the application.\n",
            "----\n",
            "Paper 209:\n",
            "Title: Informed Machine Learning – A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems\n",
            "Abstract: Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.\n",
            "----\n",
            "Paper 210:\n",
            "Title: Explainable machine learning in deployment\n",
            "Abstract: Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.\n",
            "----\n",
            "Paper 211:\n",
            "Title: A Survey of Machine Learning Techniques Applied to Software Defined Networking (SDN): Research Issues and Challenges\n",
            "Abstract: In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.\n",
            "----\n",
            "Paper 212:\n",
            "Title: Causality for Machine Learning\n",
            "Abstract: Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. \n",
            "This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.\n",
            "----\n",
            "Paper 213:\n",
            "Title: Machine learning in medicine: a practical introduction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 214:\n",
            "Title: Explainable Machine Learning for Scientific Insights and Discoveries\n",
            "Abstract: Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.\n",
            "----\n",
            "Paper 215:\n",
            "Title: Adversarial attacks on medical machine learning\n",
            "Abstract: Emerging vulnerabilities demand new conversations With public and academic attention increasingly focused on the new role of machine learning in the health information economy, an unusual and no-longer-esoteric category of vulnerabilities in machine-learning systems could prove important. These vulnerabilities allow a small, carefully designed change in how inputs are presented to a system to completely alter its output, causing it to confidently arrive at manifestly wrong conclusions. These advanced techniques to subvert otherwise-reliable machine-learning systems—so-called adversarial attacks—have, to date, been of interest primarily to computer science researchers (1). However, the landscape of often-competing interests within health care, and billions of dollars at stake in systems' outputs, implies considerable problems. We outline motivations that various players in the health care system may have to use adversarial attacks and begin a discussion of what to do about them. Far from discouraging continued innovation with medical machine learning, we call for active engagement of medical, technical, legal, and ethical experts in pursuit of efficient, broadly available, and effective health care that machine learning will enable.\n",
            "----\n",
            "Paper 216:\n",
            "Title: Tackling Climate Change with Machine Learning\n",
            "Abstract: Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.\n",
            "----\n",
            "Paper 217:\n",
            "Title: Combining satellite imagery and machine learning to predict poverty\n",
            "Abstract: Measuring consumption and wealth remotely Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary. Science, this issue p. 790; see also p. 753 Satellites collect data that can be used to measure income and wealth. Reliable data on economic livelihoods remain scarce in the developing world, hampering efforts to study these outcomes and to design policies that improve them. Here we demonstrate an accurate, inexpensive, and scalable method for estimating consumption expenditure and asset wealth from high-resolution satellite imagery. Using survey and satellite data from five African countries—Nigeria, Tanzania, Uganda, Malawi, and Rwanda—we show how a convolutional neural network can be trained to identify image features that can explain up to 75% of the variation in local-level economic outcomes. Our method, which requires only publicly available data, could transform efforts to track and target poverty in developing countries. It also demonstrates how powerful machine learning techniques can be applied in a setting with limited training data, suggesting broad potential application across many scientific domains.\n",
            "----\n",
            "Paper 218:\n",
            "Title: A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 219:\n",
            "Title: A Survey of Optimization Methods From a Machine Learning Perspective\n",
            "Abstract: Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.\n",
            "----\n",
            "Paper 220:\n",
            "Title: Machine Learning and Deep Learning\n",
            "Abstract: Now-a-days artificial intelligence has become an asset for engineering and experimental studies, just like statistics and calculus. Data science is a growing field for researchers and artificial intelligence, machine learning and deep learning are roots of it. This paper describes the relation between these roots of data science. There is a need of machine learning if any kind of analysis is to be performed. This study describes machine learning from the scratch. It also focuses on Deep Learning. Deep learning can also be known as new trend of machine learning. This paper gives a light on basic architecture of Deep learning. A comparative study of machine learning and deep learning is also given in the paper and allows researcher to have a broad view on these techniques so that they can understand which one will be preferable solution for a particular problem.\n",
            "----\n",
            "Paper 221:\n",
            "Title: Do no harm: a roadmap for responsible machine learning for health care\n",
            "Abstract: None\n",
            "----\n",
            "Paper 222:\n",
            "Title: Machine Learning on Graphs: A Model and Comprehensive Taxonomy\n",
            "Abstract: There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.\n",
            "----\n",
            "Paper 223:\n",
            "Title: Understanding the Effect of Accuracy on Trust in Machine Learning Models\n",
            "Abstract: We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.\n",
            "----\n",
            "Paper 224:\n",
            "Title: A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection\n",
            "Abstract: Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.\n",
            "----\n",
            "Paper 225:\n",
            "Title: Machine learning-assisted directed protein evolution with combinatorial libraries\n",
            "Abstract: Significance Proteins often function poorly when used outside their natural contexts; directed evolution can be used to engineer them to be more efficient in new roles. We propose that the expense of experimentally testing a large number of protein variants can be decreased and the outcome can be improved by incorporating machine learning with directed evolution. Simulations on an empirical fitness landscape demonstrate that the expected performance improvement is greater with this approach. Machine learning-assisted directed evolution from a single parent produced enzyme variants that selectively synthesize the enantiomeric products of a new-to-nature chemical transformation. By exploring multiple mutations simultaneously, machine learning efficiently navigates large regions of sequence space to identify improved proteins and also produces diverse solutions to engineering problems. To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si–H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93% and 79% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.\n",
            "----\n",
            "Paper 226:\n",
            "Title: Monte Carlo Gradient Estimation in Machine Learning\n",
            "Abstract: This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.\n",
            "----\n",
            "Paper 227:\n",
            "Title: Enhancing gravitational-wave science with machine learning\n",
            "Abstract: Machine learning has emerged as a popular and powerful approach for solving problems in astrophysics. We review applications of machine learning techniques for the analysis of ground-based gravitational-wave (GW) detector data. Examples include techniques for improving the sensitivity of Advanced Laser Interferometer GW Observatory and Advanced Virgo GW searches, methods for fast measurements of the astrophysical parameters of GW sources, and algorithms for reduction and characterization of non-astrophysical detector noise. These applications demonstrate how machine learning techniques may be harnessed to enhance the science that is possible with current and future GW detectors.\n",
            "----\n",
            "Paper 228:\n",
            "Title: Techniques for interpretable machine learning\n",
            "Abstract: Uncovering the mysterious ways machine learning models make decisions.\n",
            "----\n",
            "Paper 229:\n",
            "Title: How to Read Articles That Use Machine Learning: Users' Guides to the Medical Literature.\n",
            "Abstract: In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.\n",
            "----\n",
            "Paper 230:\n",
            "Title: MLlib: Machine Learning in Apache Spark\n",
            "Abstract: Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.\n",
            "----\n",
            "Paper 231:\n",
            "Title: Hidden stratification causes clinically meaningful failures in machine learning for medical imaging\n",
            "Abstract: Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.\n",
            "----\n",
            "Paper 232:\n",
            "Title: Machine-learning reprogrammable metasurface imager\n",
            "Abstract: None\n",
            "----\n",
            "Paper 233:\n",
            "Title: Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning\n",
            "Abstract: As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.\n",
            "----\n",
            "Paper 234:\n",
            "Title: Tensor Decomposition for Signal Processing and Machine Learning\n",
            "Abstract: Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=\"LaTeX\">$(i,j,k,\\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=\"LaTeX\">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.\n",
            "----\n",
            "Paper 235:\n",
            "Title: A Review of Relational Machine Learning for Knowledge Graphs\n",
            "Abstract: Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.\n",
            "----\n",
            "Paper 236:\n",
            "Title: Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data\n",
            "Abstract: A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.\n",
            "----\n",
            "Paper 237:\n",
            "Title: Machine learning for molecular simulation\n",
            "Abstract: Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\n",
            "----\n",
            "Paper 238:\n",
            "Title: Points of Significance: Statistics versus machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 239:\n",
            "Title: Selection of Relevant Features and Examples in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 240:\n",
            "Title: Implementing Machine Learning in Health Care - Addressing Ethical Challenges.\n",
            "Abstract: Implementing Machine Learning in Health Care We need to consider the ethical challenges inherent in implementing machine learning in health care if its benefits are to be realized. Some of these ch...\n",
            "----\n",
            "Paper 241:\n",
            "Title: Predicting Diabetes Mellitus With Machine Learning Techniques\n",
            "Abstract: Diabetes mellitus is a chronic disease characterized by hyperglycemia. It may cause many complications. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future is suffering from diabetes. There is no doubt that this alarming figure needs great attention. With the rapid development of machine learning, machine learning has been applied to many aspects of medical health. In this study, we used decision tree, random forest and neural network to predict diabetes mellitus. The dataset is the hospital physical examination data in Luzhou, China. It contains 14 attributes. In this study, five-fold cross validation was used to examine the models. In order to verity the universal applicability of the methods, we chose some methods that have the better performance to conduct independent test experiments. We randomly selected 68994 healthy people and diabetic patients’ data, respectively as training set. Due to the data unbalance, we randomly extracted 5 times data. And the result is the average of these five experiments. In this study, we used principal component analysis (PCA) and minimum redundancy maximum relevance (mRMR) to reduce the dimensionality. The results showed that prediction with random forest could reach the highest accuracy (ACC = 0.8084) when all the attributes were used.\n",
            "----\n",
            "Paper 242:\n",
            "Title: Exploiting machine learning for end-to-end drug discovery and development\n",
            "Abstract: None\n",
            "----\n",
            "Paper 243:\n",
            "Title: Machine Learning, Neural and Statistical Classification\n",
            "Abstract: Survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning.\n",
            "----\n",
            "Paper 244:\n",
            "Title: Quantum machine learning in high energy physics\n",
            "Abstract: Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.\n",
            "----\n",
            "Paper 245:\n",
            "Title: The Impact of Machine Learning on Economics\n",
            "Abstract: This paper provides an assessment of the early contributions of machine learning to economics, as well as predictions about its future contributions. It begins by brieﬂy overviewing some themes from the literature on machine learning, and then draws some contrasts with traditional approaches to estimating the impact of counterfactual policies in economics. Next, we review some of the initial “oﬀ-the-shelf” applications of machine learning to economics, including applications in analyzing text and images. We then describe new types of questions that have been posed surrounding the application of machine learning to policy problems, including “prediction policy problems,” as well as considerations of fairness and manipulability. Next, we brieﬂy review of some of the emerging econometric literature combining machine learning and causal inference. Finally, we overview a set of predictions about the future impact of machine learning on economics.\n",
            "----\n",
            "Paper 246:\n",
            "Title: Machine learning methods for solar radiation forecasting: A review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 247:\n",
            "Title: Data mining - practical machine learning tools and techniques, Second Edition\n",
            "Abstract: None\n",
            "----\n",
            "Paper 248:\n",
            "Title: Machine learning for neuroimaging with scikit-learn\n",
            "Abstract: Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.\n",
            "----\n",
            "Paper 249:\n",
            "Title: Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective\n",
            "Abstract: Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.\n",
            "----\n",
            "Paper 250:\n",
            "Title: Machine Learning at the Network Edge: A Survey\n",
            "Abstract: Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e., close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.\n",
            "----\n",
            "Paper 251:\n",
            "Title: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\n",
            "Abstract: The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.\n",
            "----\n",
            "Paper 252:\n",
            "Title: Delayed Impact of Fair Machine Learning\n",
            "Abstract: Static classification has been the predominant focus of the study of fairness in machine learning. While most models do not consider how decisions change populations over time, it is conventional wisdom that fairness criteria promote the long-term well-being of groups they aim to protect. This work studies the interaction of static fairness criteria with temporal indicators of well-being. We show a simple one-step feedback model in which common criteria do not generally promote improvement over time, and may in fact cause harm. Our results highlight the importance of temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.\n",
            "----\n",
            "Paper 253:\n",
            "Title: SoK: Security and Privacy in Machine Learning\n",
            "Abstract: Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.\n",
            "----\n",
            "Paper 254:\n",
            "Title: Machine Learning and Deep Learning Methods for Cybersecurity\n",
            "Abstract: With the development of the Internet, cyber-attacks are changing rapidly and the cyber security situation is not optimistic. This survey report describes key literature surveys on machine learning (ML) and deep learning (DL) methods for network analysis of intrusion detection and provides a brief tutorial description of each ML/DL method. Papers representing each method were indexed, read, and summarized based on their temporal or thermal correlations. Because data are so important in ML/DL methods, we describe some of the commonly used network datasets used in ML/DL, discuss the challenges of using ML/DL for cybersecurity and provide suggestions for research directions.\n",
            "----\n",
            "Paper 255:\n",
            "Title: Extreme learning machine: a new learning scheme of feedforward neural networks\n",
            "Abstract: It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradient-based learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on real-world benchmarking function approximation and classification problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.\n",
            "----\n",
            "Paper 256:\n",
            "Title: Machine Learning Approaches for Clinical Psychology and Psychiatry.\n",
            "Abstract: Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.\n",
            "----\n",
            "Paper 257:\n",
            "Title: Ensuring Fairness in Machine Learning to Advance Health Equity\n",
            "Abstract: Machine learning can identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients to train computers to perform specific tasks with sometimes superhuman ability, such as detecting diabetic eye disease better than retinal specialists (1). However, historical data also capture patterns of health care disparities, and machine-learning models trained on these data may perpetuate these inequities. This concern is not just academic. In a model used to predict future crime on the basis of historical arrest records, African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend (2, 3). Similar biases have been observed in predictive policing (4) and identifying which calls to a child protective services agency required an in-person investigation (5, 6). The implications for health care led the American Medical Association to pass policy recommendations to promote development of thoughtfully designed, high-quality, clinically validated health care AI [artificial or augmented intelligence, such as machine learning] that . . . identifies and takes steps to address bias and avoids introducing or exacerbating health care disparities including when testing or deploying new AI tools on vulnerable populations (7). We argue that health care organizations and policymakers should go beyond the American Medical Association's position of doing no harm and instead proactively design and use machine-learning systems to advance health equity. Whereas much health disparities work has focused on discriminatory decision making and implicit biases by clinicians, policymakers, organizational leaders, and researchers are increasingly focusing on the ill health effects of structural racism and classismhow systems are shaped in ways that harm the health of disempowered, marginalized populations (8). For example, the United States has a shameful history of purposive decisions by government and private businesses to segregate housing. Zoning laws, discrimination in mortgage lending, prejudicial practices by real estate agents, and the ghettoization of public housing all contributed to the concentration of urban African Americans in inferior housing that has led to poor health (9, 10). Even when the goal of decision makers is not outright discrimination against disadvantaged groups, actions may lead to inequities. For example, if the goal of a machine-learning system is to maximize efficiency, that might come at the expense of disadvantaged populations. As a society, we value health equity. For example, the Healthy People 2020 vision statement aims for a society in which all people live long, healthy lives, and one of the mission's goals is to achieve health equity, eliminate disparities, and improve the health of all groups (11). The 4 classic principles of Western clinical medical ethics are justice, autonomy, beneficence, and nonmaleficence. However, health equity will not be attained unless we purposely design our health and social systems, which increasingly will be infused with machine learning (12), to achieve this goal. To ensure fairness in machine learning, we recommend a participatory process that involves key stakeholders, including frequently marginalized populations, and considers distributive justice within specific clinical and organizational contexts. Different technical approaches can configure the mathematical properties of machine-learning models to render predictions that are equitable in various ways. The existence of mathematical levers must be supplemented with criteria for when and why they should be usedeach tool comes with tradeoffs that require ethical reasoning to decide what is best for a given application. We propose incorporating fairness into the design, deployment, and evaluation of machine-learning models. We discuss 2 clinical applications in which machine learning might harm protected groups by being inaccurate, diverting resources, or worsening outcomes, especially if the models are built without consideration for these patients. We then describe the mechanisms by which a model's design, data, and deployment may lead to disparities; explain how different approaches to distributive justice in machine learning can advance health equity; and explore what contexts are more appropriate for different equity approaches in machine learning. Case Study 1: Intensive Care Unit Monitoring A common area of predictive modeling research focuses on creating a monitoring systemfor example, to warn a rapid response team about inpatients at high risk for deterioration (1315), requiring their transfer to an intensive care unit within 6 hours. How might such a system inadvertently result in harm to a protected group? In this thought experiment, we consider African Americans as a protected group. To build the model, our hypothetical researchers collected historical records of patients who had clinical deterioration and those who did not. The model acts like a diagnostic test of risk for intensive care unit transfer. However, if too few African American patients were included in the training datathe data used to construct the modelthe model might be inaccurate for them. For example, it might have a lower sensitivity and miss more patients at risk for deterioration. African American patients might be harmed if clinical teams started relying on alerts to identify at-risk patients without realizing that the prediction system underdetects patients in that group (automation bias) (16). If the model had a lower positive predictive value for African Americans, it might also disproportionately harm them through dismissal biasa generalization of alert fatigue in which clinicians may learn to discount or dismiss alerts for African Americans because they are more likely to be false-positive (17). Case Study 2: Reducing Length of Stay Imagine that a hospital created a model with clinical and social variables to predict which inpatients might be discharged earliest so that it could direct limited case management resources to them to prevent delays. If residence in ZIP codes of socioeconomically depressed or predominantly African American neighborhoods predicted greater lengths of stay (18), this model might disproportionately allocate case management resources to patients from richer, predominantly white neighborhoods and away from African Americans in poorer ones. What Is Machine Learning? Traditionally, computer systems map inputs to outputs according to manually specified ifthen rules. With increasingly complex tasks, such as language translation, manually specifying rules becomes infeasible, and instead the mapping (or model) is learned by the system given only input examples represented through a set of features together with their desired output, referred to as labels. The quality of a model is assessed by computing evaluation metrics on data not used to build the model, such as sensitivity, specificity, or the c-statistic, which measures the ability of a model to distinguish patients with a condition from those without it (19, 20). Once the model's quality is deemed satisfactory, it can be deployed to make predictions on new examples for which the label is unknown when the prediction is made. The quality of the models on retrospective data must be followed with tests of clinical effectiveness, safety, and comparison with current practice, which may require clinical trials (21). Traditionally, statistical models for prediction, such as the pooled-cohort equation (22), have used few variables to predict clinical outcomes, such as cardiovascular risk (23). Modern machine-learning techniques, however, can consider many more features. For example, a recent model to predict hospital readmissions examined hundreds of thousands of pieces of information, including the free text of clinical notes (24). Complex data and models can drive more personalized and accurate predictions but may also make algorithms hard to understand and trust (25). What Can Cause a Machine-Learning System to Be Unfair? The Glossary lists key biases in the design, data, and deployment of a machine-learning model that may perpetuate or exacerbate health care disparities if left unchecked. The Figure reveals how the various biases relate to one another and how the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Biases may arise during the design of a model. For example, if the label is marred by health care disparities, such as predicting the onset of clinical depression in environments where protected groups have been systematically misdiagnosed, then the model will learn to perpetuate this disparity. This represents a generalization of test-referral bias (26) that we refer to as label bias. Moreover, the data on which the model is developed may be biased. Data on patients in the protected group might be distributed differently from those in the nonprotected group because of biological or nonbiological variation (9, 27). For example, the data may not contain enough examples from a group to properly tailor the predictions to them (minority bias) (28), or the data set of the protected group may be less informative because features are missing not at random as a result of more fragmented care (29, 30). Glossary Figure. Conceptual framework of how various biases relate to one another. During model development, differences in the distribution of features used to predict a label between the protected and nonprotected groups may bias a model to be less accurate for protected groups. Moreover, the data used to develop a model may not generalize to the data used during model deployment (trainingserving skew). Biases in model design and data affect patient outcomes through the model's interaction with clinicians and patients. The immediate effect of these differences is that the model may \n",
            "----\n",
            "Paper 258:\n",
            "Title: A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective\n",
            "Abstract: Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.\n",
            "----\n",
            "Paper 259:\n",
            "Title: Machine-learning-guided directed evolution for protein engineering\n",
            "Abstract: None\n",
            "----\n",
            "Paper 260:\n",
            "Title: Tunability: Importance of Hyperparameters of Machine Learning Algorithms\n",
            "Abstract: Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to chose adequate hyperparameter spaces for tuning.\n",
            "----\n",
            "Paper 261:\n",
            "Title: Interpretable Machine Learning in Healthcare\n",
            "Abstract: This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.\n",
            "----\n",
            "Paper 262:\n",
            "Title: Current Applications and Future Impact of Machine Learning in Radiology.\n",
            "Abstract: Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.\n",
            "----\n",
            "Paper 263:\n",
            "Title: Stealing Hyperparameters in Machine Learning\n",
            "Abstract: Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.\n",
            "----\n",
            "Paper 264:\n",
            "Title: Machine Learning from Theory to Algorithms: An Overview\n",
            "Abstract: The current SMAC (Social, Mobile, Analytic, Cloud) technology trend paves the way to a future in which intelligent machines, networked processes and big data are brought together. This virtual world has generated vast amount of data which is accelerating the adoption of machine learning solutions & practices. Machine Learning enables computers to imitate and adapt human-like behaviour. Using machine learning, each interaction, each action performed, becomes something the system can learn and use as experience for the next time. This work is an overview of this data analytics method which enables computers to learn and do what comes naturally to humans, i.e. learn from experience. It includes the preliminaries of machine learning, the definition, nomenclature and applications’ describing it’s what, how and why. The technology roadmap of machine learning is discussed to understand and verify its potential as a market & industry practice. The primary intent of this work is to give insight into why machine learning is the future.\n",
            "----\n",
            "Paper 265:\n",
            "Title: eDoctor: machine learning and the future of medicine\n",
            "Abstract: Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer‐aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine.\n",
            "----\n",
            "Paper 266:\n",
            "Title: Machine learning in acoustics: Theory and applications.\n",
            "Abstract: Acoustic data provide scientific and engineering insights in fields ranging from biology and communications to ocean and Earth science. We survey the recent advances and transformative potential of machine learning (ML), including deep learning, in the field of acoustics. ML is a broad family of techniques, which are often based in statistics, for automatically detecting and utilizing patterns in data. Relative to conventional acoustics and signal processing, ML is data-driven. Given sufficient training data, ML can discover complex relationships between features and desired labels or actions, or between features themselves. With large volumes of training data, ML can discover models describing complex acoustic phenomena such as human speech and reverberation. ML in acoustics is rapidly developing with compelling results and significant future promise. We first introduce ML, then highlight ML developments in four acoustics research areas: source localization in speech processing, source localization in ocean acoustics, bioacoustics, and environmental sounds in everyday scenes.\n",
            "----\n",
            "Paper 267:\n",
            "Title: An Introduction to MCMC for Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 268:\n",
            "Title: Super-resolution reconstruction of turbulent flows with machine learning\n",
            "Abstract: We use machine learning to perform super-resolution analysis of grossly under-resolved turbulent flow field data to reconstruct the high-resolution flow field. Two machine learning models are developed, namely, the convolutional neural network (CNN) and the hybrid downsampled skip-connection/multi-scale (DSC/MS) models. These machine learning models are applied to a two-dimensional cylinder wake as a preliminary test and show remarkable ability to reconstruct laminar flow from low-resolution flow field data. We further assess the performance of these models for two-dimensional homogeneous turbulence. The CNN and DSC/MS models are found to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy. For the turbulent flow problem, the machine-leaning-based super-resolution analysis can greatly enhance the spatial resolution with as little as 50 training snapshot data, holding great potential to reveal subgrid-scale physics of complex turbulent flows. With the growing availability of flow field data from high-fidelity simulations and experiments, the present approach motivates the development of effective super-resolution models for a variety of fluid flows.\n",
            "----\n",
            "Paper 269:\n",
            "Title: Automated Machine Learning: Methods, Systems, Challenges\n",
            "Abstract: None\n",
            "----\n",
            "Paper 270:\n",
            "Title: Auto-sklearn: Efficient and Robust Automated Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 271:\n",
            "Title: Coresets for Data-efficient Training of Machine Learning Models\n",
            "Abstract: Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.\n",
            "----\n",
            "Paper 272:\n",
            "Title: A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials\n",
            "Abstract: None\n",
            "----\n",
            "Paper 273:\n",
            "Title: Machine learning phases of matter\n",
            "Abstract: None\n",
            "----\n",
            "Paper 274:\n",
            "Title: Machine learning in materials science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 275:\n",
            "Title: A strategy to apply machine learning to small datasets in materials science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 276:\n",
            "Title: The Marginal Value of Adaptive Gradient Methods in Machine Learning\n",
            "Abstract: Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n",
            "----\n",
            "Paper 277:\n",
            "Title: Support Vector Machine Active Learning with Applications to Text Classification\n",
            "Abstract: Support vector machines have met with signif-icant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classiﬁed in advance. In many settings, we also have the option of using pool-based active learning . Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce an new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm. We present experimental results showing that employing our active learning method can signiﬁcantly reduce the need for labeled training instances in both the standard inductive and transductive settings.\n",
            "----\n",
            "Paper 278:\n",
            "Title: Benchmark and Survey of Automated Machine Learning Frameworks\n",
            "Abstract: Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to automatically build machine learning applications without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 different data sets.\n",
            "----\n",
            "Paper 279:\n",
            "Title: What Is Machine Learning: a Primer for the Epidemiologist.\n",
            "Abstract: Machine learning is a branch of computer science that has the potential to transform epidemiological sciences. Amid a growing focus on \"Big Data,\" it offers epidemiologists new tools to tackle problems for which classical methods are not well-suited. In order to critically evaluate the value of integrating machine learning algorithms and existing methods, however, it is essential to address language and technical barriers between the two fields that can make it difficult for epidemiologists to read and assess machine learning studies. Here, we provide an overview of the concepts and terminology used in machine learning literature, which encompasses a diverse set of tools with goals ranging from prediction, to classification, to clustering. We provide a brief introduction to five common machine learning algorithms and four ensemble-based approaches. We then summarize epidemiological applications of machine learning techniques in the published literature. We recommend approaches to incorporate machine learning in epidemiological research and discuss opportunities and challenges for integrating machine learning and existing epidemiological research methods.\n",
            "----\n",
            "Paper 280:\n",
            "Title: Machine Learning Made Easy: A Review of Scikit-learn Package in Python Programming Language\n",
            "Abstract: Machine learning is a popular topic in data analysis and modeling. Many different machine learning algorithms have been developed and implemented in a variety of programming languages over the past 20 years. In this article, we first provide an overview of machine learning and clarify its difference from statistical inference. Then, we review Scikit-learn, a machine learning package in the Python programming language that is widely used in data science. The Scikit-learn package includes implementations of a comprehensive list of machine learning methods under unified data and modeling procedure conventions, making it a convenient toolkit for educational and behavior statisticians.\n",
            "----\n",
            "Paper 281:\n",
            "Title: Machine Learning for Survival Analysis\n",
            "Abstract: Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.\n",
            "----\n",
            "Paper 282:\n",
            "Title: Can machine-learning improve cardiovascular risk prediction using routine clinical data?\n",
            "Abstract: Background Current approaches to predict cardiovascular risk fail to identify many people who would benefit from preventive treatment, while others receive unnecessary intervention. Machine-learning offers opportunity to improve accuracy by exploiting complex interactions between risk factors. We assessed whether machine-learning can improve cardiovascular risk prediction. Methods Prospective cohort study using routine clinical data of 378,256 patients from UK family practices, free from cardiovascular disease at outset. Four machine-learning algorithms (random forest, logistic regression, gradient boosting machines, neural networks) were compared to an established algorithm (American College of Cardiology guidelines) to predict first cardiovascular event over 10-years. Predictive accuracy was assessed by area under the ‘receiver operating curve’ (AUC); and sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) to predict 7.5% cardiovascular risk (threshold for initiating statins). Findings 24,970 incident cardiovascular events (6.6%) occurred. Compared to the established risk prediction algorithm (AUC 0.728, 95% CI 0.723–0.735), machine-learning algorithms improved prediction: random forest +1.7% (AUC 0.745, 95% CI 0.739–0.750), logistic regression +3.2% (AUC 0.760, 95% CI 0.755–0.766), gradient boosting +3.3% (AUC 0.761, 95% CI 0.755–0.766), neural networks +3.6% (AUC 0.764, 95% CI 0.759–0.769). The highest achieving (neural networks) algorithm predicted 4,998/7,404 cases (sensitivity 67.5%, PPV 18.4%) and 53,458/75,585 non-cases (specificity 70.7%, NPV 95.7%), correctly predicting 355 (+7.6%) more patients who developed cardiovascular disease compared to the established algorithm. Conclusions Machine-learning significantly improves accuracy of cardiovascular risk prediction, increasing the number of patients identified who could benefit from preventive treatment, while avoiding unnecessary treatment of others.\n",
            "----\n",
            "Paper 283:\n",
            "Title: Machine learning and soil sciences: a review aided by machine learning tools\n",
            "Abstract: Abstract. The application of machine learning (ML) techniques in various fields of science has increased rapidly, especially in the last 10 years. The increasing availability of soil data that can be efficiently acquired remotely and proximally, and freely available open-source algorithms, have led to an accelerated adoption of ML techniques to analyse soil data. Given the large number of publications, it is an impossible task to manually review all papers on the application of ML in soil science without narrowing down a narrative of ML application in a specific research question. This paper aims to provide a comprehensive review of the application of ML techniques in soil science aided by a ML algorithm (latent Dirichlet allocation) to find patterns in a large collection of text corpora. The objective is to gain insight into publications of ML applications in soil science and to discuss the research gaps in this topic. We found that (a) there is an increasing usage of ML methods in soil sciences, mostly concentrated in developed countries,\n",
            "(b) the reviewed publications can be grouped into 12 topics, namely remote sensing, soil organic carbon, water, contamination, methods (ensembles), erosion and parent material, methods (NN, neural networks, SVM, support vector machines), spectroscopy, modelling (classes), crops, physical, and modelling (continuous),\n",
            "and (c) advanced ML methods usually perform better than simpler approaches thanks to their capability to capture non-linear relationships.\n",
            "From these findings, we found research gaps, in particular, about the precautions that should be taken (parsimony) to avoid overfitting, and that the interpretability of the ML models is an important aspect to consider when applying advanced ML methods in order to improve our knowledge and understanding of soil. We foresee that a large number of studies will focus on the latter topic.\n",
            "\n",
            "----\n",
            "Paper 284:\n",
            "Title: Machine Learning Paradigms for Next-Generation Wireless Networks\n",
            "Abstract: Next-generation wireless networks are expected to support extremely high data rates and radically new applications, which require a new wireless radio technology paradigm. The challenge is that of assisting the radio in intelligent adaptive learning and decision making, so that the diverse requirements of next-generation wireless networks can be satisfied. Machine learning is one of the most promising artificial intelligence tools, conceived to support smart radio terminals. Future smart 5G mobile terminals are expected to autonomously access the most meritorious spectral bands with the aid of sophisticated spectral efficiency learning and inference, in order to control the transmission power, while relying on energy efficiency learning/inference and simultaneously adjusting the transmission protocols with the aid of quality of service learning/inference. Hence we briefly review the rudimentary concepts of machine learning and propose their employment in the compelling applications of 5G networks, including cognitive radios, massive MIMOs, femto/small cells, heterogeneous networks, smart grid, energy harvesting, device-todevice communications, and so on. Our goal is to assist the readers in refining the motivation, problem formulation, and methodology of powerful machine learning algorithms in the context of future networks in order to tap into hitherto unexplored applications and services.\n",
            "----\n",
            "Paper 285:\n",
            "Title: Machine learning of accurate energy-conserving molecular force fields\n",
            "Abstract: The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields. Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol−1 for energies and 1 kcal mol−1 Å̊−1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.\n",
            "----\n",
            "Paper 286:\n",
            "Title: iml: An R package for Interpretable Machine Learning\n",
            "Abstract: Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.\n",
            "----\n",
            "Paper 287:\n",
            "Title: Hands-On Machine Learning with R\n",
            "Abstract: None\n",
            "----\n",
            "Paper 288:\n",
            "Title: Probabilistic machine learning and artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 289:\n",
            "Title: The Frontiers of Fairness in Machine Learning\n",
            "Abstract: The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.\n",
            "----\n",
            "Paper 290:\n",
            "Title: A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 291:\n",
            "Title: Survey on SDN based network intrusion detection system using machine learning approaches\n",
            "Abstract: None\n",
            "----\n",
            "Paper 292:\n",
            "Title: Machine learning in chemoinformatics and drug discovery.\n",
            "Abstract: None\n",
            "----\n",
            "Paper 293:\n",
            "Title: Scaling Distributed Machine Learning with the Parameter Server\n",
            "Abstract: We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. \n",
            " \n",
            "To demonstrate the scalability of the proposed framework, we show experimental results on petabytes of real data with billions of examples and parameters on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.\n",
            "----\n",
            "Paper 294:\n",
            "Title: Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods\n",
            "Abstract: Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).\n",
            "----\n",
            "Paper 295:\n",
            "Title: Machine learning applications in epilepsy\n",
            "Abstract: Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre‐surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.\n",
            "----\n",
            "Paper 296:\n",
            "Title: TensorFlow.js: Machine Learning for the Web and Beyond\n",
            "Abstract: TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.\n",
            "----\n",
            "Paper 297:\n",
            "Title: Machine learning applications in cancer prognosis and prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 298:\n",
            "Title: Machine learning and complex biological data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 299:\n",
            "Title: Genetic algorithms and Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 300:\n",
            "Title: DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning\n",
            "Abstract: Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.\n",
            "----\n",
            "Rate limit exceeded. Retrying in 5 seconds... (Attempt 1/5)\n",
            "Paper 301:\n",
            "Title: SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient\n",
            "Abstract: In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.\n",
            "----\n",
            "Paper 302:\n",
            "Title: Deep learning and its applications to machine health monitoring\n",
            "Abstract: None\n",
            "----\n",
            "Paper 303:\n",
            "Title: Machine learning in manufacturing: advantages, challenges, and applications\n",
            "Abstract: The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.\n",
            "----\n",
            "Paper 304:\n",
            "Title: Artificial intelligence to deep learning: machine intelligence approach for drug discovery\n",
            "Abstract: None\n",
            "----\n",
            "Paper 305:\n",
            "Title: Machine Learning Techniques for Biomedical Image Segmentation: An Overview of Technical Aspects and Introduction to State-of-Art Applications\n",
            "Abstract: In recent years, significant progress has been made in developing more accurate and efficient machine learning algorithms for segmentation of medical and natural images. In this review article, we highlight the imperative role of machine learning algorithms in enabling efficient and accurate segmentation in the field of medical imaging. We specifically focus on several key studies pertaining to the application of machine learning methods to biomedical image segmentation. We review classical machine learning algorithms such as Markov random fields, k-means clustering, random forest, etc. Although such classical learning models are often less accurate compared to the deep-learning techniques, they are often more sample efficient and have a less complex structure. We also review different deep-learning architectures, such as the artificial neural networks (ANNs), the convolutional neural networks (CNNs), and the recurrent neural networks (RNNs), and present the segmentation results attained by those learning models that were published in the past 3 yr. We highlight the successes and limitations of each machine learning paradigm. In addition, we discuss several challenges related to the training of different machine learning models, and we present some heuristics to address those challenges.\n",
            "----\n",
            "Paper 306:\n",
            "Title: Data Mining: Practical Machine Learning Tools and Techniques, 3/E\n",
            "Abstract: Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. \n",
            " \n",
            " Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. \n",
            " \n",
            " *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks—in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization\n",
            "----\n",
            "Paper 307:\n",
            "Title: Perspective: Machine learning potentials for atomistic simulations.\n",
            "Abstract: Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.\n",
            "----\n",
            "Paper 308:\n",
            "Title: Machine learning applications in genetics and genomics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 309:\n",
            "Title: Machine-learning-assisted materials discovery using failed experiments\n",
            "Abstract: None\n",
            "----\n",
            "Paper 310:\n",
            "Title: A survey on evolutionary machine learning\n",
            "Abstract: ABSTRACT Artificial intelligence (AI) emphasises the creation of intelligent machines/systems that function like humans. AI has been applied to many real-world applications. Machine learning is a branch of AI based on the idea that systems can learn from data, identify hidden patterns, and make decisions with little/minimal human intervention. Evolutionary computation is an umbrella of population-based intelligent/learning algorithms inspired by nature, where New Zealand has a good international reputation. This paper provides a review on evolutionary machine learning, i.e. evolutionary computation techniques for major machine learning tasks such as classification, regression and clustering, and emerging topics including combinatorial optimisation, computer vision, deep learning, transfer learning, and ensemble learning. The paper also provides a brief review of evolutionary learning applications, such as supply chain and manufacturing for milk/dairy, wine and seafood industries, which are important to New Zealand. Finally, the paper presents current issues with future perspectives in evolutionary machine learning.\n",
            "----\n",
            "Paper 311:\n",
            "Title: A Survey of Machine Learning for Big Code and Naturalness\n",
            "Abstract: Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.\n",
            "----\n",
            "Paper 312:\n",
            "Title: Using Machine Learning to Advance Personality Assessment and Theory\n",
            "Abstract: Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.\n",
            "----\n",
            "Paper 313:\n",
            "Title: A Review of Machine Learning and Deep Learning Applications\n",
            "Abstract: Machine learning is one of the fields in the modern computing world. A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same. Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms. Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.\n",
            "----\n",
            "Paper 314:\n",
            "Title: Machine learning for Internet of Things data analysis: A survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 315:\n",
            "Title: A Proposal on Machine Learning via Dynamical Systems\n",
            "Abstract: None\n",
            "----\n",
            "Paper 316:\n",
            "Title: Accelerating the Machine Learning Lifecycle with MLflow\n",
            "Abstract: Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLﬂow, an open source platform we recently launched to streamline the machine learning lifecycle. MLﬂow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.\n",
            "----\n",
            "Paper 317:\n",
            "Title: Machine Learning and Data Mining Methods in Diabetes Research\n",
            "Abstract: None\n",
            "----\n",
            "Paper 318:\n",
            "Title: Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution\n",
            "Abstract: Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.\n",
            "----\n",
            "Paper 319:\n",
            "Title: Machine Learning-Based Sentiment Analysis for Twitter Accounts\n",
            "Abstract: Growth in the area of opinion mining and sentiment analysis has been rapid and aims to explore the opinions or text present on different platforms of social media through machine-learning techniques with sentiment, subjectivity analysis or polarity calculations. Despite the use of various machine-learning techniques and tools for sentiment analysis during elections, there is a dire need for a state-of-the-art approach. To deal with these challenges, the contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naive Bayes and support vector machines (SVM).\n",
            "----\n",
            "Paper 320:\n",
            "Title: Attractor reconstruction by machine learning.\n",
            "Abstract: A machine-learning approach called \"reservoir computing\" has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.\n",
            "----\n",
            "Paper 321:\n",
            "Title: Data Validation for Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 322:\n",
            "Title: Machine learning for image based species identification\n",
            "Abstract: Accurate species identification is the basis for all aspects of taxonomic research and is an essential component of workflows in biological research. Biologists are asking for more efficient methods to meet the identification demand. Smart mobile devices, digital cameras as well as the mass digitisation of natural history collections led to an explosion of openly available image data depicting living organisms. This rapid increase in biological image data in combination with modern machine learning methods, such as deep learning, offers tremendous opportunities for automated species identification. In this paper, we focus on deep learning neural networks as a technology that enabled breakthroughs in automated species identification in the last 2 years. In order to stimulate more work in this direction, we provide a brief overview of machine learning frameworks applicable to the species identification problem. We review selected deep learning approaches for image based species identification and introduce publicly available applications. Eventually, this article aims to provide insights into the current state‐of‐the‐art in automated identification and to serve as a starting point for researchers willing to apply novel machine learning techniques in their biological studies. While modern machine learning approaches only slowly pave their way into the field of species identification, we argue that we are going to see a proliferation of these techniques being applied to the problem in the future. Artificial intelligence systems will provide alternative tools for taxonomic identification in the near future.\n",
            "----\n",
            "Paper 323:\n",
            "Title: Machine Learning With Big Data: Challenges and Approaches\n",
            "Abstract: The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.\n",
            "----\n",
            "Paper 324:\n",
            "Title: Ten quick tips for machine learning in computational biology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 325:\n",
            "Title: Materials discovery and design using machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 326:\n",
            "Title: Fairness in Machine Learning: Lessons from Political Philosophy\n",
            "Abstract: What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.\n",
            "----\n",
            "Paper 327:\n",
            "Title: Evasion Attacks against Machine Learning at Test Time\n",
            "Abstract: None\n",
            "----\n",
            "Paper 328:\n",
            "Title: Machine learning in cardiovascular medicine: are we there yet?\n",
            "Abstract: Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.\n",
            "----\n",
            "Paper 329:\n",
            "Title: enchmark for molecular machine learning †\n",
            "Abstract: Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.\n",
            "----\n",
            "Paper 330:\n",
            "Title: Data Mining and Analytics in the Process Industry: The Role of Machine Learning\n",
            "Abstract: Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computational engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.\n",
            "----\n",
            "Paper 331:\n",
            "Title: Model-Agnostic Interpretability of Machine Learning\n",
            "Abstract: Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.\n",
            "----\n",
            "Paper 332:\n",
            "Title: Explanation and Justification in Machine Learning : A Survey Or\n",
            "Abstract: We present a survey of the research concerning explanation and justiﬁcation in the Machine Learning literature and several adjacent ﬁelds. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justiﬁcation\n",
            "----\n",
            "Paper 333:\n",
            "Title: What can machine learning do? Workforce implications\n",
            "Abstract: Profound change is coming, but roles for humans remain Digital computers have transformed work in almost every sector of the economy over the past several decades (1). We are now at the beginning of an even larger and more rapid transformation due to recent advances in machine learning (ML), which is capable of accelerating the pace of automation itself. However, although it is clear that ML is a “general purpose technology,” like the steam engine and electricity, which spawns a plethora of additional innovations and capabilities (2), there is no widely shared agreement on the tasks where ML systems excel, and thus little agreement on the specific expected impacts on the workforce and on the economy more broadly. We discuss what we see to be key implications for the workforce, drawing on our rubric of what the current generation of ML systems can and cannot do [see the supplementary materials (SM)]. Although parts of many jobs may be “suitable for ML” (SML), other tasks within these same jobs do not fit the criteria for ML well; hence, effects on employment are more complex than the simple replacement and substitution story emphasized by some. Although economic effects of ML are relatively limited today, and we are not facing the imminent “end of work” as is sometimes proclaimed, the implications for the economy and the workforce going forward are profound.\n",
            "----\n",
            "Paper 334:\n",
            "Title: Machine learning at the energy and intensity frontiers of particle physics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 335:\n",
            "Title: Review: machine learning techniques applied to cybersecurity\n",
            "Abstract: None\n",
            "----\n",
            "Paper 336:\n",
            "Title: Survey of Machine Learning Algorithms for Disease Diagnostic\n",
            "Abstract: In medical imaging, Computer Aided Diagnosis (CAD) is a rapidly growing dynamic area of research. In recent years, significant attempts are made for the enhancement of computer aided diagnosis applications because errors in medical diagnostic systems can result in seriously misleading medical treatments. Machine learning is important in Computer Aided Diagnosis. After using an easy equation, objects such as organs may not be indicated accurately. So, pattern recognition fundamentally involves learning from examples. In the field of bio-medical, pattern recognition and machine learning promise the improved accuracy of perception and diagnosis of disease. They also promote the objectivity of decision-making process. For the analysis of high-dimensional and multimodal bio-medical data, machine learning offers a worthy approach for making classy and automatic algorithms. This survey paper provides the comparative analysis of different machine learning algorithms for diagnosis of different diseases such as heart disease, diabetes disease, liver disease, dengue disease and hepatitis disease. It brings attention towards the suite of machine learning algorithms and tools that are used for the analysis of diseases and decision-making process accordingly.\n",
            "----\n",
            "Paper 337:\n",
            "Title: Machine Learning Algorithms\n",
            "Abstract: None\n",
            "----\n",
            "Paper 338:\n",
            "Title: TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 339:\n",
            "Title: Machine Learning and Materials Informatics: Recent Applications and Prospects\n",
            "Abstract: Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods--due to the cost, time or effort involved--but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative--extending into new materials spaces--provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven \"materials informatics\" strategies undertaken in the last decade, and identifies some challenges the community is facing and those that should be overcome in the near future.\n",
            "----\n",
            "Paper 340:\n",
            "Title: Challenges in representation learning: A report on three machine learning contests\n",
            "Abstract: None\n",
            "----\n",
            "Paper 341:\n",
            "Title: Machine learning in catalysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 342:\n",
            "Title: Machine learning unifies the modeling of materials and molecules\n",
            "Abstract: Statistical learning based on a local representation of atomic structures provides a universal model of chemical stability. Determining the stability of molecules and condensed phases is the cornerstone of atomistic modeling, underpinning our understanding of chemical and materials properties and transformations. We show that a machine-learning model, based on a local description of chemical environments and Bayesian statistical learning, provides a unified framework to predict atomic-scale properties. It captures the quantum mechanical effects governing the complex surface reconstructions of silicon, predicts the stability of different classes of molecules with chemical accuracy, and distinguishes active and inactive protein ligands with more than 99% reliability. The universality and the systematic nature of our framework provide new insight into the potential energy surface of materials and molecules.\n",
            "----\n",
            "Paper 343:\n",
            "Title: Unintended consequences of machine learning in medicine?\n",
            "Abstract: Machine learning (ML) has the potential to significantly aid medical practice. However, a recent article highlighted some negative consequences that may arise from using ML decision support in medicine. We argue here that whilst the concerns raised by the authors may be appropriate, they are not specific to ML, and thus the article may lead to an adverse perception about this technique in particular. Whilst ML is not without its limitations like any methodology, a balanced view is needed in order to not hamper its use in potentially enabling better patient care.\n",
            "----\n",
            "Paper 344:\n",
            "Title: DaDianNao: A Machine-Learning Supercomputer\n",
            "Abstract: Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.\n",
            "----\n",
            "Paper 345:\n",
            "Title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
            "Abstract: Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.\n",
            "----\n",
            "Paper 346:\n",
            "Title: A review on extreme learning machine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 347:\n",
            "Title: Oblivious Multi-Party Machine Learning on Trusted Processors\n",
            "Abstract: Privacy-preserving multi-party machine learning allows multiple organizations to perform collaborative data analytics while guaranteeing the privacy of their individual datasets. Using trusted SGX-processors for this task yields high performance, but requires a careful selection, adaptation, and implementation of machine-learning algorithms to provably prevent the exploitation of any side channels induced by data-dependent access patterns. \n",
            " \n",
            "We propose data-oblivious machine learning algorithms for support vector machines, matrix factorization, neural networks, decision trees, and k-means clustering. We show that our efficient implementation based on Intel Skylake processors scales up to large, realistic datasets, with overheads several orders of magnitude lower than with previous approaches based on advanced cryptographic multi-party computation schemes.\n",
            "----\n",
            "Paper 348:\n",
            "Title: mlr: Machine Learning in R\n",
            "Abstract: The MLR package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.\n",
            "----\n",
            "Paper 349:\n",
            "Title: A Review of Challenges and Opportunities in Machine Learning for Health.\n",
            "Abstract: Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.\n",
            "----\n",
            "Paper 350:\n",
            "Title: A survey on application of machine learning for Internet of Things\n",
            "Abstract: None\n",
            "----\n",
            "Paper 351:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 352:\n",
            "Title: Matching Networks for One Shot Learning\n",
            "Abstract: Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.\n",
            "----\n",
            "Paper 353:\n",
            "Title: Speeding Up Distributed Machine Learning Using Codes\n",
            "Abstract: Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({\\alpha + \\frac {1}{n}}\\right)\\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n) \\simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.\n",
            "----\n",
            "Paper 354:\n",
            "Title: Neural Networks for Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 355:\n",
            "Title: Machine Learning in Healthcare: A Review\n",
            "Abstract: Machine Learning is modern and highly sophisticated technological applications became a huge trend in the industry. Machine Learning is Omni present and is widely used in various applications. It is playing a vital role in many fields like finance, Medical science and in security. Machine learning is used to discover patterns from medical data sources and provide excellent capabilities to predict diseases. In this paper, we review various machine learning algorithms used for developing efficient decision support for healthcare applications. This paper helps in reducing the research gap for building efficient decision support system for medical applications.\n",
            "----\n",
            "Paper 356:\n",
            "Title: Interactive machine learning for health informatics: when do we need the human-in-the-loop?\n",
            "Abstract: None\n",
            "----\n",
            "Paper 357:\n",
            "Title: A survey of machine learning for big data processing\n",
            "Abstract: None\n",
            "----\n",
            "Paper 358:\n",
            "Title: Non-convex Optimization for Machine Learning\n",
            "Abstract: A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately, structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks.  The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve.  A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization.  On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties.  This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.\n",
            "----\n",
            "Paper 359:\n",
            "Title: Introduction to machine learning: k-nearest neighbors.\n",
            "Abstract: Machine learning techniques have been widely used in many scientific fields, but its use in medical literature is limited partly because of technical difficulties. k-nearest neighbors (kNN) is a simple method of machine learning. The article introduces some basic ideas underlying the kNN algorithm, and then focuses on how to perform kNN modeling with R. The dataset should be prepared before running the knn() function in R. After prediction of outcome with kNN algorithm, the diagnostic performance of the model should be checked. Average accuracy is the mostly widely used statistic to reflect the kNN algorithm. Factors such as k value, distance calculation and choice of appropriate predictors all have significant impact on the model performance.\n",
            "----\n",
            "Paper 360:\n",
            "Title: Advances and Open Problems in Federated Learning\n",
            "Abstract: Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.\n",
            "----\n",
            "Paper 361:\n",
            "Title: Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View\n",
            "Abstract: Background As more and more researchers are turning to big data for new opportunities of biomedical discoveries, machine learning models, as the backbone of big data analysis, are mentioned more often in biomedical journals. However, owing to the inherent complexity of machine learning methods, they are prone to misuse. Because of the flexibility in specifying machine learning models, the results are often insufficiently reported in research articles, hindering reliable assessment of model validity and consistent interpretation of model outputs. Objective To attain a set of guidelines on the use of machine learning predictive models within clinical settings to make sure the models are correctly applied and sufficiently reported so that true discoveries can be distinguished from random coincidence. Methods A multidisciplinary panel of machine learning experts, clinicians, and traditional statisticians were interviewed, using an iterative process in accordance with the Delphi method. Results The process produced a set of guidelines that consists of (1) a list of reporting items to be included in a research article and (2) a set of practical sequential steps for developing predictive models. Conclusions A set of guidelines was generated to enable correct application of machine learning models and consistent reporting of model specifications and results in biomedical research. We believe that such guidelines will accelerate the adoption of big data analysis, particularly with machine learning methods, in the biomedical research community.\n",
            "----\n",
            "Paper 362:\n",
            "Title: Introduction to Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 363:\n",
            "Title: Smart Machining Process Using Machine Learning: A Review and Perspective on Machining Industry\n",
            "Abstract: None\n",
            "----\n",
            "Paper 364:\n",
            "Title: OpenML: networked science in machine learning\n",
            "Abstract: Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.\n",
            "----\n",
            "Paper 365:\n",
            "Title: Machine learning on big data: Opportunities and challenges\n",
            "Abstract: None\n",
            "----\n",
            "Paper 366:\n",
            "Title: Bypassing the Kohn-Sham equations with machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 367:\n",
            "Title: Machine learning models and bankruptcy prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 368:\n",
            "Title: Deep Learning with Differential Privacy\n",
            "Abstract: Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.\n",
            "----\n",
            "Paper 369:\n",
            "Title: Hidden Technical Debt in Machine Learning Systems\n",
            "Abstract: Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.\n",
            "----\n",
            "Paper 370:\n",
            "Title: Diversity in Machine Learning\n",
            "Abstract: Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.\n",
            "----\n",
            "Paper 371:\n",
            "Title: Neural Discrete Representation Learning\n",
            "Abstract: Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n",
            "----\n",
            "Paper 372:\n",
            "Title: Bayesian reasoning and machine learning\n",
            "Abstract: Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.\n",
            "----\n",
            "Paper 373:\n",
            "Title: Lifelong Machine Learning, Second Edition\n",
            "Abstract: None\n",
            "----\n",
            "Paper 374:\n",
            "Title: Machine learning in geosciences and remote sensing\n",
            "Abstract: None\n",
            "----\n",
            "Paper 375:\n",
            "Title: Federated Learning: Challenges, Methods, and Future Directions\n",
            "Abstract: Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.\n",
            "----\n",
            "Paper 376:\n",
            "Title: A review of supervised machine learning algorithms\n",
            "Abstract: Supervised machine learning is the construction of algorithms that are able to produce general patterns and hypotheses by using externally supplied instances to predict the fate of future instances. Supervised machine learning classification algorithms aim at categorizing data from prior information. Classification is carried out very frequently in data science problems. Various successful techniques have been proposed to solve such problems viz. Rule-based techniques, Logic-based techniques, Instance-based techniques, stochastic techniques. This paper discusses the efficacy of supervised machine learning algorithms in terms of the accuracy, speed of learning, complexity and risk of over fitting measures. The main objective of this paper is to provide a general comparison with state of art machine learning algorithms.\n",
            "----\n",
            "Paper 377:\n",
            "Title: Machine Learning for Networking: Workflow, Advances and Opportunities\n",
            "Abstract: Recently, machine learning has been used in every possible field to leverage its amazing power. For a long time, the networking and distributed computing system is the key infrastructure to provide efficient computational resources for machine learning. Networking itself can also benefit from this promising technology. This article focuses on the application of MLN, which can not only help solve the intractable old network questions but also stimulate new network applications. In this article, we summarize the basic workflow to explain how to apply machine learning technology in the networking domain. Then we provide a selective survey of the latest representative advances with explanations of their design principles and benefits. These advances are divided into several network design objectives and the detailed information of how they perform in each step of MLN workflow is presented. Finally, we shed light on the new opportunities in networking design and community building of this new inter-discipline. Our goal is to provide a broad research guideline on networking with machine learning to help motivate researchers to develop innovative algorithms, standards and frameworks.\n",
            "----\n",
            "Paper 378:\n",
            "Title: Machine Learning for High-Throughput Stress Phenotyping in Plants.\n",
            "Abstract: None\n",
            "----\n",
            "Paper 379:\n",
            "Title: PMLB: a large benchmark suite for machine learning evaluation and comparison\n",
            "Abstract: None\n",
            "----\n",
            "Paper 380:\n",
            "Title: Lifelong machine learning: a paradigm for continuous learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 381:\n",
            "Title: Python Machine Learning\n",
            "Abstract: Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.\n",
            "----\n",
            "Paper 382:\n",
            "Title: Machine learning, social learning and the governance of self-driving cars\n",
            "Abstract: Self-driving cars, a quintessentially ‘smart’ technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking ‘Who is learning, what are they learning and how are they learning?’ Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. ‘Self-driving’ or ‘autonomous’ cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.\n",
            "----\n",
            "Paper 383:\n",
            "Title: A Comprehensive Survey on Transfer Learning\n",
            "Abstract: Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.\n",
            "----\n",
            "Paper 384:\n",
            "Title: Quantum machine learning: a classical perspective\n",
            "Abstract: Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning (ML) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical ML algorithms. Here we review the literature in quantum ML and discuss perspectives for a mixed readership of classical ML and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in ML are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.\n",
            "----\n",
            "Paper 385:\n",
            "Title: Genetic Algorithms in Search, Optimization & Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 386:\n",
            "Title: Python machine learning : machine learning and deep learning with Python, scikit-learn, and TensorFlow\n",
            "Abstract: None\n",
            "----\n",
            "Paper 387:\n",
            "Title: Radio Machine Learning Dataset Generation with GNU Radio\n",
            "Abstract: This paper surveys emerging applications of Machine Learning (ML) to the Radio Signal Processing domain.  Provides some brief background on enabling methods and discusses some of the potential advancements for the field.  It discusses the critical importance of good datasets for model learning, testing, and evaluation and introduces several public open source synthetic datasets for various radio machine learning tasks.  These are intended to provide a robust common baselines for those working in the field and to provide a benchmark measure against which many techniques can be rapidly evaluated and compared.\n",
            "----\n",
            "Paper 388:\n",
            "Title: Machine Learning for the Geosciences: Challenges and Opportunities\n",
            "Abstract: Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)—that has been widely successful in commercial domains—offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.\n",
            "----\n",
            "Paper 389:\n",
            "Title: Unintended Consequences of Machine Learning in Medicine\n",
            "Abstract: Over the past decade, machine learning techniques have made substantial advances in many domains. In health care, global interest in the potential of machine learning has increased; for example, a deep learning algorithm has shown high accuracy in detecting diabetic retinopathy.1 There have been suggestions that machine learning will drive changes in health care within a few years, specifically in medical disciplines that require more accurate prognostic models (eg, oncology) and those based on pattern recognition (eg, radiology and pathology). However, comparative studies on the effectiveness of machine learning–based decision support systems (ML-DSS) in medicine are lacking, especially regarding the effects on health outcomes. Moreover, the introduction of new technologies in health care has not always been straightforward or without unintended and adverse effects.2 In this Viewpoint we consider the potential unintended consequences that may result from the application of ML-DSS in clinical practice.\n",
            "----\n",
            "Paper 390:\n",
            "Title: Faster and Better: A Machine Learning Approach to Corner Detection\n",
            "Abstract: The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.\n",
            "----\n",
            "Paper 391:\n",
            "Title: Machine Learning for Neural Decoding\n",
            "Abstract: Abstract Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outperform traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help to advance engineering applications such as brain–machine interfaces. Our code package is available at github.com/kordinglab/neural_decoding.\n",
            "----\n",
            "Paper 392:\n",
            "Title: Unified representation of molecules and crystals for machine learning\n",
            "Abstract: Accurate simulations of atomistic systems from first principles are limited by computational cost. In high-throughput settings, machine learning can reduce these costs significantly by accurately interpolating between reference calculations. For this, kernel learning approaches crucially require a representation that accommodates arbitrary atomistic systems. We introduce a many-body tensor representation that is invariant to translations, rotations, and nuclear permutations of same elements, unique, differentiable, can represent molecules and crystals, and is fast to compute. Empirical evidence for competitive energy and force prediction errors is presented for changes in molecular structure, crystal chemistry, and molecular dynamics using kernel regression and symmetric gradient-domain machine learning as models. Applicability is demonstrated for phase diagrams of Pt-group/transition-metal binary systems.\n",
            "----\n",
            "Paper 393:\n",
            "Title: Machine Learning methods for Quantitative Radiomic Biomarkers\n",
            "Abstract: None\n",
            "----\n",
            "Paper 394:\n",
            "Title: Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.\n",
            "Abstract: Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.\n",
            "----\n",
            "Paper 395:\n",
            "Title: Artificial intelligence, machine learning and deep learning\n",
            "Abstract: It is increasingly recognized that artificial intelligence has been touted as a new mobile. Because of the high volume of data that being generated by devices, sensors and social media users, the machine can learn to distinguish the pattern and makes a reasonably good prediction. This article will explore the use of machine learning and its methodologies. Furthermore, the field of deep learning which is being exploited in many leading IT providers will be clarified and discussed.\n",
            "----\n",
            "Paper 396:\n",
            "Title: Lifelong Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 397:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 398:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 399:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 400:\n",
            "Title: Machine Learning Classification over Encrypted Data\n",
            "Abstract: Machine learning classification is used for numerous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Naïve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.\n",
            "----\n",
            "Rate limit exceeded. Retrying in 10 seconds... (Attempt 1/5)\n",
            "Paper 401:\n",
            "Title: Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges\n",
            "Abstract: Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.\n",
            "----\n",
            "Paper 402:\n",
            "Title: Encyclopedia of Machine Learning and Data Mining\n",
            "Abstract: None\n",
            "----\n",
            "Paper 403:\n",
            "Title: Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning\n",
            "Abstract: Algorithms for feature selection fall into two broad categories: wrappers that use the learning algorithm itself to evaluate the usefulness of features and filters that evaluate features according to heuristics based on general characteristics of the data. For application to large databases, filters have proven to be more practical than wrappers because they are much faster. However, most existing filter algorithms only work with discrete classification problems. This paper describes a fast, correlation-based filter algorithm that can be applied to continuous and discrete problems. The algorithm often outperforms the well-known ReliefF attribute estimator when used as a preprocessing step for naive Bayes, instance-based learning, decision trees, locally weighted regression, and model trees. It performs more feature selection than ReliefF does—reducing the data dimensionality by fifty percent in most cases. Also, decision and model trees built from the preprocessed data are often significantly smaller.\n",
            "----\n",
            "Paper 404:\n",
            "Title: A brief survey of machine learning methods and their sensor and IoT applications\n",
            "Abstract: This paper provides a brief survey of the basic concepts and algorithms used for Machine Learning and its applications. We begin with a broader definition of machine learning and then introduce various learning modalities including supervised and unsupervised methods and deep learning paradigms. In the rest of the paper, we discuss applications of machine learning algorithms in various fields including pattern recognition, sensor networks, anomaly detection, Internet of Things (IoT) and health monitoring. In the final sections, we present some of the software tools and an extensive bibliography.\n",
            "----\n",
            "Paper 405:\n",
            "Title: Machine Teaching: A New Paradigm for Building Machine Learning Systems\n",
            "Abstract: The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. \n",
            "While machine learning focuses on creating new algorithms and improving the accuracy of \"learners\", the machine teaching discipline focuses on the efficacy of the \"teachers\". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. \n",
            "In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.\n",
            "----\n",
            "Paper 406:\n",
            "Title: Quantum-enhanced machine learning\n",
            "Abstract: The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.\n",
            "----\n",
            "Paper 407:\n",
            "Title: Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies\n",
            "Abstract: Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context. After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.\n",
            "----\n",
            "Paper 408:\n",
            "Title: Machine Learning and the Profession of Medicine.\n",
            "Abstract: This Viewpoint discusses the opportunities and ethical implications of using machine learning technologies, which can rapidly collect and learn from large amounts of personal data, to provide individalized patient care.\n",
            "----\n",
            "Paper 409:\n",
            "Title: Federated Learning: Strategies for Improving Communication Efficiency\n",
            "Abstract: Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.\n",
            "----\n",
            "Paper 410:\n",
            "Title: Principles of Explanatory Debugging to Personalize Interactive Machine Learning\n",
            "Abstract: How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.\n",
            "----\n",
            "Paper 411:\n",
            "Title: Instance spaces for machine learning classification\n",
            "Abstract: None\n",
            "----\n",
            "Paper 412:\n",
            "Title: A review of automatic selection methods for machine learning algorithms and hyper-parameter values\n",
            "Abstract: None\n",
            "----\n",
            "Paper 413:\n",
            "Title: Deep Learning\n",
            "Abstract: Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.\n",
            "----\n",
            "Paper 414:\n",
            "Title: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\n",
            "Abstract: Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.\n",
            "----\n",
            "Paper 415:\n",
            "Title: Extreme Learning Machine for Multilayer Perceptron\n",
            "Abstract: Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.\n",
            "----\n",
            "Paper 416:\n",
            "Title: Machine Learning Topological States\n",
            "Abstract: Machine learning, the core of artificial intelligence and data science, is a very active field, with vast applications throughout science and technology. Recently, machine learning techniques have been adopted to tackle intricate quantum many-body problems and phase transitions. In this work, the authors construct exact mappings from exotic quantum states to machine learning network models. This work shows for the first time that the restricted Boltzmann machine can be used to study both symmetry-protected topological phases and intrinsic topological order. The exact results are expected to provide a substantial boost to the field of machine learning of phases of matter.\n",
            "----\n",
            "Paper 417:\n",
            "Title: Explainable and Interpretable Models in Computer Vision and Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 418:\n",
            "Title: Machine Learning for Predictive Maintenance: A Multiple Classifier Approach\n",
            "Abstract: In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.\n",
            "----\n",
            "Paper 419:\n",
            "Title: Torch7: A Matlab-like Environment for Machine Learning\n",
            "Abstract: Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.\n",
            "----\n",
            "Paper 420:\n",
            "Title: Machine Learning: A Bayesian and Optimization Perspective\n",
            "Abstract: This tutorial text gives a unifying perspective on machine learning by covering bothprobabilistic and deterministic approaches -which are based on optimization techniques together with the Bayesian inference approach, whose essence liesin the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.\n",
            "----\n",
            "Paper 421:\n",
            "Title: Kernel methods in machine learning\n",
            "Abstract: We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.\n",
            "----\n",
            "Paper 422:\n",
            "Title: Learning Curves in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 423:\n",
            "Title: A large annotated corpus for learning natural language inference\n",
            "Abstract: Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.\n",
            "----\n",
            "Paper 424:\n",
            "Title: Dataset Shift in Machine Learning\n",
            "Abstract: Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series\n",
            "----\n",
            "Paper 425:\n",
            "Title: The Boosting Approach to Machine Learning An Overview\n",
            "Abstract: None\n",
            "----\n",
            "Paper 426:\n",
            "Title: Fast and accurate modeling of molecular atomization energies with machine learning.\n",
            "Abstract: We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.\n",
            "----\n",
            "Paper 427:\n",
            "Title: Support vector machine learning for interdependent and structured output spaces\n",
            "Abstract: Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.\n",
            "----\n",
            "Paper 428:\n",
            "Title: Transfer Learning for Low-Resource Neural Machine Translation\n",
            "Abstract: The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.\n",
            "----\n",
            "Paper 429:\n",
            "Title: An introduction to statistical learning with applications in R\n",
            "Abstract: The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an\n",
            "----\n",
            "Paper 430:\n",
            "Title: Outside the Closed World: On Using Machine Learning for Network Intrusion Detection\n",
            "Abstract: In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational \"real world\" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.\n",
            "----\n",
            "Paper 431:\n",
            "Title: Power to the People: The Role of Humans in Interactive Machine Learning\n",
            "Abstract: Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.\n",
            "----\n",
            "Paper 432:\n",
            "Title: C4.5: Programs for Machine Learning (書評)\n",
            "Abstract: None\n",
            "----\n",
            "Paper 433:\n",
            "Title: Machine Learning Methods for Attack Detection in the Smart Grid\n",
            "Abstract: Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semisupervised) are employed with decision- and feature-level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than attack detection algorithms that employ state vector estimation methods in the proposed attack detection framework.\n",
            "----\n",
            "Paper 434:\n",
            "Title: An introduction to quantum machine learning\n",
            "Abstract: Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.\n",
            "----\n",
            "Paper 435:\n",
            "Title: Dual Learning for Machine Translation\n",
            "Abstract: While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English ↔ French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.\n",
            "----\n",
            "Paper 436:\n",
            "Title: Structural Health Monitoring: A Machine Learning Perspective\n",
            "Abstract: This book focuses on structural health monitoring in the context of machine learning. The authors review the technical literature and include case studies. Chapters include: operational evaluation, sensing and data acquisition, introduction to probability and statistics, machine learning and statistical pattern recognition, and data prognosis.\n",
            "----\n",
            "Paper 437:\n",
            "Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n",
            "Abstract: A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.\n",
            "----\n",
            "Paper 438:\n",
            "Title: Scikit-learn: Machine Learning Without Learning the Machinery\n",
            "Abstract: Machine learning is a pervasive development at the intersection of statistics and computer science. While it can benefit many data-related applications, the technical nature of the research literature and the corresponding algorithms slows down its adoption. Scikit-learn is an open-source software project that aims at making machine learning accessible to all, whether it be in academia or in industry. It benefits from the general-purpose Python language, which is both broadly adopted in the scientific world, and supported by a thriving ecosystem of contributors. Here we give a quick introduction to scikit-learn as well as to machine-learning basics.\n",
            "----\n",
            "Paper 439:\n",
            "Title: Hyperparameter Search in Machine Learning\n",
            "Abstract: We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.\n",
            "----\n",
            "Paper 440:\n",
            "Title: Machine learning classifiers and fMRI: A tutorial overview\n",
            "Abstract: None\n",
            "----\n",
            "Paper 441:\n",
            "Title: A survey of techniques for internet traffic classification using machine learning\n",
            "Abstract: The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.\n",
            "----\n",
            "Paper 442:\n",
            "Title: A survey of feature selection and feature extraction techniques in machine learning\n",
            "Abstract: Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.\n",
            "----\n",
            "Paper 443:\n",
            "Title: MLaaS: Machine Learning as a Service\n",
            "Abstract: The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.\n",
            "----\n",
            "Paper 444:\n",
            "Title: MACHINE LEARNING An Artificial Intelligence Approach\n",
            "Abstract: Research in the area of learning structural descriptions from examples is reviewed, giving primary attention to methods of learning characteristic descrip­ tions of single concepts. In particular, we examine methods for finding the maximally-specific conjunctive generalizations (MSC-generalizations) that cover all of the training examples of a given concept. Various important aspects of structural learning in general are examined, and several criteria for evaluating structural learning methods are presented. Briefly, these criteria include (i) ade­ quacy of the representation language, (ii) generalization rules employed, computational efficiency, and (iv) flexibility and extensibility. Selected learning methods developed by Buchanan, et al., Hayes-Roth, Vere, Winston, and the authors are analyzed according to these criteria. Finally, some goals are sug­ gested for future research.\n",
            "----\n",
            "Paper 445:\n",
            "Title: Programs for Machine Learning. Part I\n",
            "Abstract: None\n",
            "----\n",
            "Paper 446:\n",
            "Title: Machine learning for quantum mechanics in a nutshell\n",
            "Abstract: Models that combine quantum mechanics (QM) with machine learning (ML) promise to deliver the accuracy of QM at the speed of ML. This hands-on tutorial introduces the reader to QM/ML models based on kernel learning, an elegant, systematically nonlinear form of ML. Pseudocode and a reference implementation are provided, enabling the reader to reproduce results from recent publications where atomization energies of small organic molecules are predicted using kernel ridge regression. © 2015 Wiley Periodicals, Inc.\n",
            "----\n",
            "Paper 447:\n",
            "Title: Learning a Multi-View Stereo Machine\n",
            "Abstract: We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.\n",
            "----\n",
            "Paper 448:\n",
            "Title: Encyclopedia of Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 449:\n",
            "Title: Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications\n",
            "Abstract: Wireless sensor networks (WSNs) monitor dynamic environments that change rapidly over time. This dynamic behavior is either caused by external factors or initiated by the system designers themselves. To adapt to such conditions, sensor networks often adopt machine learning techniques to eliminate the need for unnecessary redesign. Machine learning also inspires many practical solutions that maximize resource utilization and prolong the lifespan of the network. In this paper, we present an extensive literature review over the period 2002-2013 of machine learning methods that were used to address common issues in WSNs. The advantages and disadvantages of each proposed algorithm are evaluated against the corresponding problem. We also provide a comparative guide to aid WSN designers in developing suitable machine learning solutions for their specific application challenges.\n",
            "----\n",
            "Paper 450:\n",
            "Title: Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare\n",
            "Abstract: Machine learning is being used in a wide range of application domains to discover patterns in large datasets. Increasingly, the results of machine learning drive critical decisions in applications related to healthcare and biomedicine. Such health-related applications are often sensitive, and thus, any security breach would be catastrophic. Naturally, the integrity of the results computed by machine learning is of great importance. Recent research has shown that some machine-learning algorithms can be compromised by augmenting their training datasets with malicious data, leading to a new class of attacks called poisoning attacks. Hindrance of a diagnosis may have life-threatening consequences and could cause distrust. On the other hand, not only may a false diagnosis prompt users to distrust the machine-learning algorithm and even abandon the entire system but also such a false positive classification may cause patient distress. In this paper, we present a systematic, algorithm-independent approach for mounting poisoning attacks across a wide range of machine-learning algorithms and healthcare datasets. The proposed attack procedure generates input data, which, when added to the training set, can either cause the results of machine learning to have targeted errors (e.g., increase the likelihood of classification into a specific class), or simply introduce arbitrary errors (incorrect classification). These attacks may be applied to both fixed and evolving datasets. They can be applied even when only statistics of the training dataset are available or, in some cases, even without access to the training dataset, although at a lower efficacy. We establish the effectiveness of the proposed attacks using a suite of six machine-learning algorithms and five healthcare datasets. Finally, we present countermeasures against the proposed generic attacks that are based on tracking and detecting deviations in various accuracy metrics, and benchmark their effectiveness.\n",
            "----\n",
            "Paper 451:\n",
            "Title: Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 452:\n",
            "Title: UCI Repository of Machine Learning Databases\n",
            "Abstract: None\n",
            "----\n",
            "Paper 453:\n",
            "Title: Machine Learning\n",
            "Abstract: Machine learning is more fashionable than ever for problems in data science, predictive modeling, and quantitative asset management. Developments in the field have revolutionized many aspects of modern life. Nevertheless, it is sometimes difficult to understand where real value ends and speculative hype begins. Here we attempt to demystify the topic. We provide a historical perspective on artificial intelligence and give a light, semi-technical overview of prevailing tools and techniques. We conclude with a brief discussion of the implications for investment management.\n",
            "----\n",
            "Paper 454:\n",
            "Title: Determinantal Point Processes for Machine Learning\n",
            "Abstract: Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.\n",
            "----\n",
            "Paper 455:\n",
            "Title: API design for machine learning software: experiences from the scikit-learn project\n",
            "Abstract: Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.\n",
            "----\n",
            "Paper 456:\n",
            "Title: Communication Efficient Distributed Machine Learning with the Parameter Server\n",
            "Abstract: This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l1 -regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.\n",
            "----\n",
            "Paper 457:\n",
            "Title: Distributed GraphLab: A Framework for Machine Learning in the Cloud\n",
            "Abstract: While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. \n",
            " \n",
            "We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.\n",
            "----\n",
            "Paper 458:\n",
            "Title: Towards Federated Learning at Scale: System Design\n",
            "Abstract: Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.\n",
            "----\n",
            "Paper 459:\n",
            "Title: Understanding Machine Learning: From Theory to Algorithms\n",
            "Abstract: Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.\n",
            "----\n",
            "Paper 460:\n",
            "Title: Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary\n",
            "Abstract: None\n",
            "----\n",
            "Paper 461:\n",
            "Title: Machine Learning for Aerial Image Labeling\n",
            "Abstract: Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. \n",
            "We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. \n",
            "We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.\n",
            "----\n",
            "Paper 462:\n",
            "Title: Incremental and Decremental Support Vector Machine Learning\n",
            "Abstract: An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental \"unlearning\" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.\n",
            "----\n",
            "Paper 463:\n",
            "Title: Automating the Construction of Internet Portals with Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 464:\n",
            "Title: Machine learning methods in chemoinformatics\n",
            "Abstract: Machine learning algorithms are generally developed in computer science or adjacent disciplines and find their way into chemical modeling by a process of diffusion. Though particular machine learning methods are popular in chemoinformatics and quantitative structure–activity relationships (QSAR), many others exist in the technical literature. This discussion is methods‐based and focused on some algorithms that chemoinformatics researchers frequently use. It makes no claim to be exhaustive. We concentrate on methods for supervised learning, predicting the unknown property values of a test set of instances, usually molecules, based on the known values for a training set. Particularly relevant approaches include Artificial Neural Networks, Random Forest, Support Vector Machine, k‐Nearest Neighbors and naïve Bayes classifiers. WIREs Comput Mol Sci 2014, 4:468–481.\n",
            "----\n",
            "Paper 465:\n",
            "Title: A survey of multi-view machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 466:\n",
            "Title: Deep learning and process understanding for data-driven Earth system science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 467:\n",
            "Title: Quantum Machine Learning: What Quantum Computing Means to Data Mining\n",
            "Abstract: Quantum Machine Learning bridges the gap between abstract developments in quantum computing and the applied research on machine learning. Paring down the complexity of the disciplines involved, it ...\n",
            "----\n",
            "Paper 468:\n",
            "Title: Quantum algorithms for supervised and unsupervised machine learning\n",
            "Abstract: Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.\n",
            "----\n",
            "Paper 469:\n",
            "Title: The Limitations of Deep Learning in Adversarial Settings\n",
            "Abstract: Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.\n",
            "----\n",
            "Paper 470:\n",
            "Title: Opposition-Based Learning: A New Scheme for Machine Intelligence\n",
            "Abstract: Opposition-based learning as a new scheme for machine intelligence is introduced. Estimates and counter-estimates, weights and opposite weights, and actions versus counter-actions are the foundation of this new approach. Examples are provided. Possibilities for extensions of existing learning algorithms are discussed. Preliminary results are provided\n",
            "----\n",
            "Paper 471:\n",
            "Title: Sparse Bayesian Learning and the Relevance Vector Machine\n",
            "Abstract: This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classi cation tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the `relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art `support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while o ering a number of additional advantages. These include the bene ts of probabilistic predictions, automatic estimation of `nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-`Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We o er some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.\n",
            "----\n",
            "Paper 472:\n",
            "Title: Mastering Machine Learning With scikit-learn\n",
            "Abstract: Apply effective learning algorithms to real-world problems using scikit-learn About This BookDesign and troubleshoot machine learning systems for common tasks including regression, classification, and clusteringAcquaint yourself with popular machine learning algorithms, including decision trees, logistic regression, and support vector machinesA practical example-based guide to help you gain expertise in implementing and evaluating machine learning systems using scikit-learnWho This Book Is ForIf you are a software developer who wants to learn how machine learning models work and how to apply them effectively, this book is for you. Familiarity with machine learning fundamentals and Python will be helpful, but is not essential. In Detail This book examines machine learning models including logistic regression, decision trees, and support vector machines, and applies them to common problems such as categorizing documents and classifying images. It begins with the fundamentals of machine learning, introducing you to the supervised-unsupervised spectrum, the uses of training and test data, and evaluating models. You will learn how to use generalized linear models in regression problems, as well as solve problems with text and categorical features.You will be acquainted with the use of logistic regression, regularization, and the various loss functions that are used by generalized linear models. The book will also walk you through an example project that prompts you to label the most uncertain training examples. You will also use an unsupervised Hidden Markov Model to predict stock prices.By the end of the book, you will be an expert in scikit-learn and will be well versed in machine learning\n",
            "----\n",
            "Paper 473:\n",
            "Title: Kernel Methods and Machine Learning\n",
            "Abstract: Part I. Machine Learning and Kernel Vector Spaces: 1. Fundamentals of machine learning 2. Kernel-induced vector spaces Part II. Dimension-Reduction: Feature Selection and PCA/KPCA: 3. Feature selection 4. PCA and Kernel-PCA Part III. Unsupervised Learning Models for Cluster Analysis: 5. Unsupervised learning for cluster discovery 6. Kernel methods for cluster discovery Part IV. Kernel Ridge Regressors and Variants: 7. Kernel-based regression and regularization analysis 8. Linear regression and discriminant analysis for supervised classification 9. Kernel ridge regression for supervised classification Part V. Support Vector Machines and Variants: 10. Support vector machines 11. Support vector learning models for outlier detection 12. Ridge-SVM learning models Part VI. Kernel Methods for Green Machine Learning Technologies: 13. Efficient kernel methods for learning and classifcation Part VII. Kernel Methods and Statistical Estimation Theory: 14. Statistical regression analysis and errors-in-variables models 15: Kernel methods for estimation, prediction, and system identification Part VIII. Appendices: Appendix A. Validation and test of learning models Appendix B. kNN, PNN, and Bayes classifiers References Index.\n",
            "----\n",
            "Paper 474:\n",
            "Title: The master algorithm: how the quest for the ultimate learning machine will remake our world\n",
            "Abstract: Nowadays, “machine learning” is present in several aspects of the current world, internet advisors, advertisements and “smart” devices that seem to know what we need in a given moment. These are some examples of the problems solved by machine learning. This book presents the past, the present and the future of the different types of machine learning algorithms. At the beginning of the book, the author takes us to the first years of the computing science, where a programmer had to do absolutely everything by himself to make an algorithm do a certain task. As time passes, there appeared the first algorithms that were capable of programming themselves learning from the available data. The author presents what he himself calls the five “tribes” of machine learning, the essence that defends each one and the kind of problems that are able to solve without problems. With a great amount of simple examples, the author depicts which advantages and disadvantages of the “master” algorithms of each “tribes” are, saying that the problem that a tribe solves perfectly well, another one cannot do it, and the other way about. The author suggests to get the best out of each “tribe” and make a unique learning algorithm able to learn without caring about the problem: the master algorithm.\n",
            "----\n",
            "Paper 475:\n",
            "Title: Machine Learning - The Art and Science of Algorithms that Make Sense of Data\n",
            "Abstract: As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.\n",
            "----\n",
            "Paper 476:\n",
            "Title: Multiagent Systems: A Survey from a Machine Learning Perspective\n",
            "Abstract: None\n",
            "----\n",
            "Paper 477:\n",
            "Title: DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\n",
            "Abstract: Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.\n",
            "----\n",
            "Paper 478:\n",
            "Title: Deep Learning for Computer Vision: A Brief Review\n",
            "Abstract: Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.\n",
            "----\n",
            "Paper 479:\n",
            "Title: Ensemble Methods in Machine Learning\n",
            "Abstract: Ensemble methods are learning algorithms that construct a set of classiiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging , but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classiier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overrt rapidly.\n",
            "----\n",
            "Paper 480:\n",
            "Title: The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n",
            "Abstract: In the words of the authors, the goal of this book was to “bring together many of the important new ideas in learning, and explain them in a statistical framework.” The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse  elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related  elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a “data modeling culture,” where the model is paramount. Breiman argued instead for an “algorithmic modeling culture,” with emphasis on black-box types of prediction. Breiman’s article is controversial, and in his discussion, Efron objects that “prediction is certainly an interesting subject, but Leo’s paper overstates both its role and our profession’s lack of interest in it.” Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a  nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called “market basket analysis.” The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book’s usefulness as a reference, and so I am happier with the book as it was written.\n",
            "----\n",
            "Paper 481:\n",
            "Title: Map-Reduce for Machine Learning on Multicore\n",
            "Abstract: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.\n",
            "----\n",
            "Paper 482:\n",
            "Title: Representation Learning: A Review and New Perspectives\n",
            "Abstract: The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.\n",
            "----\n",
            "Paper 483:\n",
            "Title: Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 484:\n",
            "Title: A Survey on Transfer Learning\n",
            "Abstract: A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.\n",
            "----\n",
            "Paper 485:\n",
            "Title: Accelerating materials property predictions using machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 486:\n",
            "Title: Machine learning: a review of classification and combining techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 487:\n",
            "Title: Ensemble Machine Learning: Methods and Applications\n",
            "Abstract: It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.\n",
            "----\n",
            "Paper 488:\n",
            "Title: Support vector machine active learning for image retrieval\n",
            "Abstract: Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.\n",
            "----\n",
            "Paper 489:\n",
            "Title: Shortcut learning in deep neural networks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 490:\n",
            "Title: Machine Learning in Medical Imaging\n",
            "Abstract: None\n",
            "----\n",
            "Paper 491:\n",
            "Title: Machine Unlearning\n",
            "Abstract: Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.\n",
            "----\n",
            "Paper 492:\n",
            "Title: Machine Learning for the Detection of Oil Spills in Satellite Radar Images\n",
            "Abstract: None\n",
            "----\n",
            "Paper 493:\n",
            "Title: Supplementary for: Deep learning with convolutional neural networks for EEG decoding and visualization\n",
            "Abstract: Translational Neurotechnology Lab, Epilepsy Center, Medical Center – University of Freiburg, Engelberger Str. 21, 79106 Freiburg, Germany BrainLinks-BrainTools Cluster of Excellence, University of Freiburg, Georges-Köhler-Allee 79, 79110 Freiburg, Germany Machine Learning Lab, Computer Science Dept., University of Freiburg, Georges-Köhler-Allee 79, 79110 Freiburg, Germany Neurobiology and Biophysics, Faculty of Biology, University of Freiburg, Hansastr. 9a, 79104 Freiburg, Germany Machine Learning for Automated Algorithm Design Lab, Computer Science Dept., University of Freiburg, Georges-Köhler-Allee 52, 79110 Freiburg im Breisgau, Germany Brain State Decoding Lab, Computer Science Dept., University of Freiburg, Albertstr. 23, 79104 Freiburg, Germany Autonomous Intelligent Systems Lab, Computer Science Dept., University of Freiburg, Georges-Köhler-Allee 79, 79110 Freiburg, Germany\n",
            "----\n",
            "Paper 494:\n",
            "Title: Deep Learning with Python\n",
            "Abstract: Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learninga combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Franois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.\n",
            "----\n",
            "Paper 495:\n",
            "Title: Revisiting the Nystrom Method for Improved Large-scale Machine Learning\n",
            "Abstract: We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds--e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.\n",
            "----\n",
            "Paper 496:\n",
            "Title: Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies.\n",
            "Abstract: The accurate and reliable prediction of properties of molecules typically requires computationally intensive quantum-chemical calculations. Recently, machine learning techniques applied to ab initio calculations have been proposed as an efficient approach for describing the energies of molecules in their given ground-state structure throughout chemical compound space (Rupp et al. Phys. Rev. Lett. 2012, 108, 058301). In this paper we outline a number of established machine learning techniques and investigate the influence of the molecular representation on the methods performance. The best methods achieve prediction errors of 3 kcal/mol for the atomization energies of a wide variety of molecules. Rationales for this performance improvement are given together with pitfalls and challenges when applying machine learning approaches to the prediction of quantum-mechanical observables.\n",
            "----\n",
            "Paper 497:\n",
            "Title: The future of digital health with federated learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 498:\n",
            "Title: Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]\n",
            "Abstract: This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and \"weaknesses, depending on the application and context in \"which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.\n",
            "----\n",
            "Paper 499:\n",
            "Title: Ensemble Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 500:\n",
            "Title: A Survey on Machine-Learning Techniques in Cognitive Radios\n",
            "Abstract: In this survey paper, we characterize the learning problem in cognitive radios (CRs) and state the importance of artificial intelligence in achieving real cognitive communications systems. We review various learning problems that have been studied in the context of CRs classifying them under two main categories: Decision-making and feature classification. Decision-making is responsible for determining policies and decision rules for CRs while feature classification permits identifying and classifying different observation models. The learning algorithms encountered are categorized as either supervised or unsupervised algorithms. We describe in detail several challenging learning issues that arise in cognitive radio networks (CRNs), in particular in non-Markovian environments and decentralized networks, and present possible solution methods to address them. We discuss similarities and differences among the presented algorithms and identify the conditions under which each of the techniques may be applied.\n",
            "----\n",
            "Paper 501:\n",
            "Title: Deep Learning with Python\n",
            "Abstract: Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learninga combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Franois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.\n",
            "----\n",
            "Paper 502:\n",
            "Title: Revisiting the Nystrom Method for Improved Large-scale Machine Learning\n",
            "Abstract: We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds--e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.\n",
            "----\n",
            "Paper 503:\n",
            "Title: Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies.\n",
            "Abstract: The accurate and reliable prediction of properties of molecules typically requires computationally intensive quantum-chemical calculations. Recently, machine learning techniques applied to ab initio calculations have been proposed as an efficient approach for describing the energies of molecules in their given ground-state structure throughout chemical compound space (Rupp et al. Phys. Rev. Lett. 2012, 108, 058301). In this paper we outline a number of established machine learning techniques and investigate the influence of the molecular representation on the methods performance. The best methods achieve prediction errors of 3 kcal/mol for the atomization energies of a wide variety of molecules. Rationales for this performance improvement are given together with pitfalls and challenges when applying machine learning approaches to the prediction of quantum-mechanical observables.\n",
            "----\n",
            "Paper 504:\n",
            "Title: The future of digital health with federated learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 505:\n",
            "Title: Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]\n",
            "Abstract: This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and \"weaknesses, depending on the application and context in \"which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.\n",
            "----\n",
            "Paper 506:\n",
            "Title: Ensemble Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 507:\n",
            "Title: Ensemble learning: A survey\n",
            "Abstract: Ensemble methods are considered the state‐of‐the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state‐of‐the‐art ensemble methods and discusses current challenges and trends in the field.\n",
            "----\n",
            "Paper 508:\n",
            "Title: Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations\n",
            "Abstract: Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.\n",
            "----\n",
            "Paper 509:\n",
            "Title: Toward Causal Representation Learning\n",
            "Abstract: The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.\n",
            "----\n",
            "Paper 510:\n",
            "Title: An Overview of Multi-Task Learning in Deep Neural Networks\n",
            "Abstract: Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.\n",
            "----\n",
            "Paper 511:\n",
            "Title: Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers\n",
            "Abstract: Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.\n",
            "----\n",
            "Paper 512:\n",
            "Title: GraphLab: A New Framework For Parallel Machine Learning\n",
            "Abstract: Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.\n",
            "----\n",
            "Paper 513:\n",
            "Title: Machine Learning Paradigms for Speech Recognition: An Overview\n",
            "Abstract: Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem - for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.\n",
            "----\n",
            "Paper 514:\n",
            "Title: MLbase: A Distributed Machine-learning System\n",
            "Abstract: Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming|many users do not understand the trade-os and challenges of parameterizing and choosing between dierent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.\n",
            "----\n",
            "Paper 515:\n",
            "Title: Lifelong Machine Learning Systems: Beyond Learning Algorithms\n",
            "Abstract: Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime. The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies. Following a review of prior work on LML, we propose that it is now appropriate for the AI community to move beyond learning algorithms to more seriously consider the nature of systems that are capable of learning over a lifetime. Reasons for our position are presented and potential counter-arguments are discussed. The remainder of the paper contributes by defining LML, presenting a reference framework that considers all forms of machine learning, and listing several key challenges for and benefits from LML research. We conclude with ideas for next steps to advance the field.\n",
            "----\n",
            "Paper 516:\n",
            "Title: Geometric Deep Learning: Going beyond Euclidean data\n",
            "Abstract: Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.\n",
            "----\n",
            "Paper 517:\n",
            "Title: Machine Learning - An Algorithmic Perspective\n",
            "Abstract: None\n",
            "----\n",
            "Paper 518:\n",
            "Title: Unsupervised Machine Translation Using Monolingual Corpora Only\n",
            "Abstract: Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.\n",
            "----\n",
            "Paper 519:\n",
            "Title: Machine learning in cell biology – teaching computers to recognize phenotypes\n",
            "Abstract: Summary Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.\n",
            "----\n",
            "Paper 520:\n",
            "Title: Pylearn2: a machine learning research library\n",
            "Abstract: Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.\n",
            "----\n",
            "Paper 521:\n",
            "Title: Noise2Noise: Learning Image Restoration without Clean Data\n",
            "Abstract: We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data.\n",
            "----\n",
            "Paper 522:\n",
            "Title: An Introduction to Deep Learning for the Physical Layer\n",
            "Abstract: We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.\n",
            "----\n",
            "Paper 523:\n",
            "Title: Distributed GraphLab: A Framework for Machine Learning in the Cloud\n",
            "Abstract: While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.\n",
            "----\n",
            "Paper 524:\n",
            "Title: The immune system, adaptation, and machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 525:\n",
            "Title: Gaussian Processes for Machine Learning (GPML) Toolbox\n",
            "Abstract: The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.\n",
            "----\n",
            "Paper 526:\n",
            "Title: Human Decisions and Machine Predictions\n",
            "Abstract: Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; and these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals. JEL Codes: C10 (Econometric and statistical methods and methodology), C55 (Large datasets: Modeling and analysis), K40 (Legal procedure, the legal system, and illegal behavior).\n",
            "----\n",
            "Paper 527:\n",
            "Title: Towards Personalized Federated Learning\n",
            "Abstract: In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.\n",
            "----\n",
            "Paper 528:\n",
            "Title: On the Variance of the Adaptive Learning Rate and Beyond\n",
            "Abstract: The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.\n",
            "----\n",
            "Paper 529:\n",
            "Title: Learning scikit-learn: Machine Learning in Python\n",
            "Abstract: Experience the benefits of machine learning techniques by applying them to real-world problems using Python and the open source scikit-learn library Overview Use Python and scikit-learn to create intelligent applications Apply regression techniques to predict future behaviour and learn to cluster items in groups by their similarities Make use of classification techniques to perform image recognition and document classification In Detail Machine learning, the art of creating applications that learn from experience and data, has been around for many years. However, in the era of big data, huge amounts of information is being generated. This makes machine learning an unavoidable source of new data-based approximations for problem solving. With Learning scikit-learn: Machine Learning in Python, you will learn to incorporate machine learning in your applications. The book combines an introduction to some of the main concepts and methods in machine learning with practical, hands-on examples of real-world problems. Ranging from handwritten digit recognition to document classification, examples are solved step by step using Scikit-learn and Python. The book starts with a brief introduction to the core concepts of machine learning with a simple example. Then, using real-world applications and advanced features, it takes a deep dive into the various machine learning techniques. You will learn to evaluate your results and apply advanced techniques for preprocessing data. You will also be able to select the best set of features and the best methods for each problem. With Learning scikit-learn: Machine Learning in Python you will learn how to use the Python programming language and the scikit-learn library to build applications that learn from experience, applying the main concepts and techniques of machine learning. What you will learn from this book Set up scikit-learn inside your Python environment Classify objects (from documents to human faces and flower species) based on some of their features, using a variety of methods from Support Vector Machines to Nave Bayes Use Decision Trees to explain the main causes of certain phenomenon such as the Titanic passengers survival Predict house prices using regression techniques Display and analyse groups in your data using dimensionality reduction Make use of different tools to preprocess, extract, and select the learning features Select the best parameters for your models using model selection Improve the way you build your models using parallelization techniques Approach The book adopts a tutorial-based approach to introduce the user to Scikit-learn. Who this book is written for If you are a programmer who wants to explore machine learning and data-based methods to build intelligent applications and enhance your programming skills, this the book for you. No previous experience with machine-learning algorithms is required.\n",
            "----\n",
            "Paper 530:\n",
            "Title: Model-based machine learning\n",
            "Abstract: Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.\n",
            "----\n",
            "Paper 531:\n",
            "Title: A survey on semi-supervised learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 532:\n",
            "Title: Machine learning for targeted display advertising: transfer learning in action\n",
            "Abstract: None\n",
            "----\n",
            "Paper 533:\n",
            "Title: A Machine Learning Framework for Programming by Example\n",
            "Abstract: Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.\n",
            "----\n",
            "Paper 534:\n",
            "Title: Machine learning on Big Data\n",
            "Abstract: Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.\n",
            "----\n",
            "Paper 535:\n",
            "Title: Supervised learning with quantum-enhanced feature spaces\n",
            "Abstract: None\n",
            "----\n",
            "Paper 536:\n",
            "Title: Adaptive Federated Learning in Resource Constrained Edge Computing Systems\n",
            "Abstract: Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.\n",
            "----\n",
            "Paper 537:\n",
            "Title: Machine learning for science and society\n",
            "Abstract: None\n",
            "----\n",
            "Paper 538:\n",
            "Title: ADADELTA: An Adaptive Learning Rate Method\n",
            "Abstract: We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.\n",
            "----\n",
            "Paper 539:\n",
            "Title: Deep Bayesian Active Learning with Image Data\n",
            "Abstract: Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).\n",
            "----\n",
            "Paper 540:\n",
            "Title: The security of machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 541:\n",
            "Title: Scaling Neural Machine Translation\n",
            "Abstract: Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT’14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT’14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.\n",
            "----\n",
            "Paper 542:\n",
            "Title: Applications of Machine Learning in Cancer Prediction and Prognosis\n",
            "Abstract: Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.\n",
            "----\n",
            "Paper 543:\n",
            "Title: Deep Learning in Medical Image Analysis.\n",
            "Abstract: This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.\n",
            "----\n",
            "Paper 544:\n",
            "Title: Improving propensity score weighting using machine learning\n",
            "Abstract: Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART‐based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non‐linear and non‐additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non‐linearity or non‐additivity alone. However, under conditions of both moderate non‐additivity and moderate non‐linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright © 2009 John Wiley & Sons, Ltd.\n",
            "----\n",
            "Paper 545:\n",
            "Title: U-Net: deep learning for cell counting, detection, and morphometry\n",
            "Abstract: None\n",
            "----\n",
            "Paper 546:\n",
            "Title: Representation Learning on Graphs: Methods and Applications\n",
            "Abstract: Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.\n",
            "----\n",
            "Paper 547:\n",
            "Title: Density Ratio Estimation in Machine Learning\n",
            "Abstract: Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.\n",
            "----\n",
            "Paper 548:\n",
            "Title: A State-of-the-Art Survey on Deep Learning Theory and Architectures\n",
            "Abstract: In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.\n",
            "----\n",
            "Paper 549:\n",
            "Title: Weighted extreme learning machine for imbalance learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 550:\n",
            "Title: Deep learning for sentiment analysis: A survey\n",
            "Abstract: Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.\n",
            "----\n",
            "Paper 551:\n",
            "Title: Unsupervised Neural Machine Translation\n",
            "Abstract: In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.\n",
            "----\n",
            "Paper 552:\n",
            "Title: A Survey on Curriculum Learning\n",
            "Abstract: Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.\n",
            "----\n",
            "Paper 553:\n",
            "Title: A Survey on Multi-Task Learning\n",
            "Abstract: Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.\n",
            "----\n",
            "Paper 554:\n",
            "Title: Machine Learning in Action\n",
            "Abstract: SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce\n",
            "----\n",
            "Paper 555:\n",
            "Title: Federated Multi-Task Learning\n",
            "Abstract: Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.\n",
            "----\n",
            "Paper 556:\n",
            "Title: Tensor2Tensor for Neural Machine Translation\n",
            "Abstract: Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.\n",
            "----\n",
            "Paper 557:\n",
            "Title: Machine Health Monitoring Using Local Feature-Based Gated Recurrent Unit Networks\n",
            "Abstract: In modern industries, machine health monitoring systems (MHMS) have been applied wildly with the goal of realizing predictive maintenance including failures tracking, downtime reduction, and assets preservation. In the era of big machinery data, data-driven MHMS have achieved remarkable results in the detection of faults after the occurrence of certain failures (diagnosis) and prediction of the future working conditions and the remaining useful life (prognosis). The numerical representation for raw sensory data is the key stone for various successful MHMS. Conventional methods are the labor-extensive as they usually depend on handcrafted features, which require expert knowledge. Inspired by the success of deep learning methods that redefine representation learning from raw data, we propose local feature-based gated recurrent unit (LFGRU) networks. It is a hybrid approach that combines handcrafted feature design with automatic feature learning for machine health monitoring. First, features from windows of input time series are extracted. Then, an enhanced bidirectional GRU network is designed and applied on the generated sequence of local features to learn the representation. A supervised learning layer is finally trained to predict machine condition. Experiments on three machine health monitoring tasks: tool wear prediction, gearbox fault diagnosis, and incipient bearing fault detection verify the effectiveness and generalization of the proposed LFGRU.\n",
            "----\n",
            "Paper 558:\n",
            "Title: Deep Learning--based Text Classification\n",
            "Abstract: Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.\n",
            "----\n",
            "Paper 559:\n",
            "Title: An Overview of Multi-task Learning\n",
            "Abstract: As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.\n",
            "----\n",
            "Paper 560:\n",
            "Title: Meta-learning with differentiable closed-form solvers\n",
            "Abstract: Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.\n",
            "----\n",
            "Paper 561:\n",
            "Title: Intrusion detection by machine learning: A review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 562:\n",
            "Title: Deploying an interactive machine learning system in an evidence-based practice center: abstrackr\n",
            "Abstract: Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.\n",
            "----\n",
            "Paper 563:\n",
            "Title: ML Confidential: Machine Learning on Encrypted Data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 564:\n",
            "Title: A survey on ensemble learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 565:\n",
            "Title: Machine Learning Strategies for Time Series Forecasting\n",
            "Abstract: None\n",
            "----\n",
            "Paper 566:\n",
            "Title: Automatic analysis of malware behavior using machine learning\n",
            "Abstract: Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. \n",
            " \n",
            "In this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.\n",
            "----\n",
            "Paper 567:\n",
            "Title: A survey of transfer learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 568:\n",
            "Title: A Machine Learning Approach to Coreference Resolution of Noun Phrases\n",
            "Abstract: In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.\n",
            "----\n",
            "Paper 569:\n",
            "Title: SplitFed: When Federated Learning Meets Split Learning\n",
            "Abstract: Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings.\n",
            "----\n",
            "Paper 570:\n",
            "Title: Neural Networks and Learning Machines\n",
            "Abstract: For graduate-level neural network courses offered in the departments of Computer Engineering, Electrical Engineering, and Computer Science. Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely upto-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists. Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/ Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.\n",
            "----\n",
            "Paper 571:\n",
            "Title: Machine learning and radiology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 572:\n",
            "Title: Reinforcement Learning: A Survey\n",
            "Abstract: This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\n",
            "----\n",
            "Paper 573:\n",
            "Title: Meta-Learning with Memory-Augmented Neural Networks\n",
            "Abstract: Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.\n",
            "----\n",
            "Paper 574:\n",
            "Title: Supervised Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 575:\n",
            "Title: A Machine Learning Approach to Twitter User Classification\n",
            "Abstract: \n",
            " \n",
            " This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.\n",
            " \n",
            "\n",
            "----\n",
            "Paper 576:\n",
            "Title: Machine Theory of Mind\n",
            "Abstract: Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the \"Sally-Anne\" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.\n",
            "----\n",
            "Paper 577:\n",
            "Title: Active Learning Literature Survey\n",
            "Abstract: The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difﬁcult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.\n",
            "----\n",
            "Paper 578:\n",
            "Title: Deep Reinforcement Learning: An Overview\n",
            "Abstract: We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. \n",
            "Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.\n",
            "----\n",
            "Paper 579:\n",
            "Title: Quantum circuit learning\n",
            "Abstract: We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, which we call quantum circuit learning. A quantum circuit driven by our framework learns a given task by tuning parameters implemented on it. The iterative optimization of the parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that a quantum circuit can approximate nonlinear functions, which is further confirmed by numerical simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for quantum machine learning.\n",
            "----\n",
            "Paper 580:\n",
            "Title: Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation\n",
            "Abstract: As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.\n",
            "----\n",
            "Paper 581:\n",
            "Title: Deep learning in remote sensing: a review\n",
            "Abstract: Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a 'black-box' solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.\n",
            "----\n",
            "Paper 582:\n",
            "Title: Deep Learning Approach for Intelligent Intrusion Detection System\n",
            "Abstract: Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01–0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.\n",
            "----\n",
            "Paper 583:\n",
            "Title: Biological underpinnings for lifelong learning machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 584:\n",
            "Title: A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks\n",
            "Abstract: Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.\n",
            "----\n",
            "Paper 585:\n",
            "Title: An Introduction to Deep Reinforcement Learning\n",
            "Abstract: Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.\n",
            "----\n",
            "Paper 586:\n",
            "Title: Curriculum learning\n",
            "Abstract: Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).\n",
            "----\n",
            "Paper 587:\n",
            "Title: Federated Learning with Personalization Layers\n",
            "Abstract: The emerging paradigm of federated learning strives to enable collaborative training of machine learning models on the network edge without centrally aggregating raw data and hence, improving data privacy. This sharply deviates from traditional machine learning and necessitates the design of algorithms robust to various sources of heterogeneity. Specifically, statistical heterogeneity of data across user devices can severely degrade the performance of standard federated averaging for traditional machine learning applications like personalization with deep learning. This paper pro-posesFedPer, a base + personalization layer approach for federated training of deep feedforward neural networks, which can combat the ill-effects of statistical heterogeneity. We demonstrate effectiveness ofFedPerfor non-identical data partitions ofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr.\n",
            "----\n",
            "Paper 588:\n",
            "Title: Machine Learning that Matters\n",
            "Abstract: Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.\n",
            "----\n",
            "Paper 589:\n",
            "Title: An Introduction to Support Vector Machines and Other Kernel-based Learning Methods\n",
            "Abstract: From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.\n",
            "----\n",
            "Paper 590:\n",
            "Title: Making machine learning models interpretable\n",
            "Abstract: Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.\n",
            "----\n",
            "Paper 591:\n",
            "Title: The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches\n",
            "Abstract: Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].\n",
            "----\n",
            "Paper 592:\n",
            "Title: MLPACK: a scalable C++ machine learning library\n",
            "Abstract: MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.\n",
            "----\n",
            "Paper 593:\n",
            "Title: Machine Learning for the New York City Power Grid\n",
            "Abstract: Power companies can benefit from the use of knowledge discovery methods and statistical machine learning for preventive maintenance. We introduce a general process for transforming historical electrical grid data into models that aim to predict the risk of failures for components and systems. These models can be used directly by power companies to assist with prioritization of maintenance and repair work. Specialized versions of this process are used to produce (1) feeder failure rankings, (2) cable, joint, terminator, and transformer rankings, (3) feeder Mean Time Between Failure (MTBF) estimates, and (4) manhole events vulnerability rankings. The process in its most general form can handle diverse, noisy, sources that are historical (static), semi-real-time, or real-time, incorporates state-of-the-art machine learning algorithms for prioritization (supervised ranking or MTBF), and includes an evaluation of results via cross-validation and blind test. Above and beyond the ranked lists and MTBF estimates are business management interfaces that allow the prediction capability to be integrated directly into corporate planning and decision support; such interfaces rely on several important properties of our general modeling approach: that machine learning features are meaningful to domain experts, that the processing of data is transparent, and that prediction results are accurate enough to support sound decision making. We discuss the challenges in working with historical electrical grid data that were not designed for predictive purposes. The “rawness” of these data contrasts with the accuracy of the statistical models that can be obtained from the process; these models are sufficiently accurate to assist in maintaining New York City's electrical grid.\n",
            "----\n",
            "Paper 594:\n",
            "Title: The random forest algorithm for statistical learning\n",
            "Abstract: Random forests (Breiman, 2001, Machine Learning 45: 5–32) is a statistical- or machine-learning algorithm for prediction. In this article, we introduce a corresponding new command, rforest. We overview the random forest algorithm and illustrate its use with two examples: The first example is a classification problem that predicts whether a credit card holder will default on his or her debt. The second example is a regression problem that predicts the logscaled number of shares of online news articles. We conclude with a discussion that summarizes key points demonstrated in the examples.\n",
            "----\n",
            "Paper 595:\n",
            "Title: Findings of the 2017 Conference on Machine Translation (WMT17)\n",
            "Abstract: This paper presents the results of the WMT17 shared tasks, which included \n",
            "three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task.\n",
            "----\n",
            "Paper 596:\n",
            "Title: Introduction to machine learning for brain imaging\n",
            "Abstract: None\n",
            "----\n",
            "Paper 597:\n",
            "Title: Machine Learning Methods for Ecological Applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 598:\n",
            "Title: Deep learning for neural networks\n",
            "Abstract: Machine learning algorithms are designed to improve as they encounter more data, making them a versatile technology for understanding large sets of photos such as those accessible from Google Images. Elizabeth Holm, professor of materials science and engineering at Carnegie Mellon University, is leveraging this technology to better understand the enormous number of research images accumulated in the field of materials science. [13]\n",
            "----\n",
            "Paper 599:\n",
            "Title: Foundations of Machine Learning\n",
            "Abstract: This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.\n",
            "----\n",
            "Paper 600:\n",
            "Title: Deep Learning for Health Informatics\n",
            "Abstract: With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.\n",
            "----\n",
            "Paper 601:\n",
            "Title: Federated Learning\n",
            "Abstract: How is it possible to allow multiple data owners to collaboratively train and use a shared prediction model while keeping all the local training data private? Traditional machine learning approaches need to combine all data at one location, typically a data center, which may very well violate the laws on user privacy and data confidentiality. Today, many parts of the world demand that technology companies treat user data carefully according to user-privacy laws. The European Union’s General Data Protection Regulation (GDPR) is a prime example. In this book, we describe how federated machine learning addresses this problem with novel solutions combining distributed machine learning, cryptography and security, and incentive mechanism design based on economic principles and game theory. We explain different types of privacypreserving machine learning solutions and their technological backgrounds, and highlight some representative practical use cases.We show how federated learning can become the foundation of next-generation machine learning that caters to technological and societal needs for responsible AI development and application.\n",
            "----\n",
            "Paper 602:\n",
            "Title: ADASYN: Adaptive synthetic sampling approach for imbalanced learning\n",
            "Abstract: This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.\n",
            "----\n",
            "Paper 603:\n",
            "Title: Scalable Private Learning with PATE\n",
            "Abstract: The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a \"student\" model the knowledge of an ensemble of \"teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. \n",
            "In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\\varepsilon$ < 1.0).\n",
            "----\n",
            "Paper 604:\n",
            "Title: The elements of statistical learning: data mining, inference and prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 605:\n",
            "Title: Deep learning: new computational modelling techniques for genomics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 606:\n",
            "Title: Advances in kernel methods: support vector learning\n",
            "Abstract: Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.\n",
            "----\n",
            "Paper 607:\n",
            "Title: Deep Learning Applications in Medical Image Analysis\n",
            "Abstract: The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.\n",
            "----\n",
            "Paper 608:\n",
            "Title: Meta-Learning: A Survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 609:\n",
            "Title: The MLIP package: moment tensor potentials with MPI and active learning\n",
            "Abstract: The subject of this paper is the technology (the ‘how’) of constructing machine-learning interatomic potentials, rather than science (the ‘what’ and ‘why’) of atomistic simulations using machine-learning potentials. Namely, we illustrate how to construct moment tensor potentials using active learning as implemented in the MLIP package, focusing on the efficient ways to automatically sample configurations for the training set, how expanding the training set changes the error of predictions, how to set up ab initio calculations in a cost-effective manner, etc. The MLIP package (short for Machine-Learning Interatomic Potentials) is available at https://mlip.skoltech.ru/download/.\n",
            "----\n",
            "Paper 610:\n",
            "Title: Deep learning for cellular image analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 611:\n",
            "Title: Gaussian Processes for Machine Learning\n",
            "Abstract: Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes.\n",
            "----\n",
            "Paper 612:\n",
            "Title: Finding Density Functionals with Machine Learning\n",
            "Abstract: Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed.\n",
            "----\n",
            "Paper 613:\n",
            "Title: Using Machine Learning to Detect Cyberbullying\n",
            "Abstract: Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy.\n",
            "----\n",
            "Paper 614:\n",
            "Title: Can machine learning be secure?\n",
            "Abstract: Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, \"Can machine learning be secure?\" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.\n",
            "----\n",
            "Paper 615:\n",
            "Title: Learning from positive and unlabeled data: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 616:\n",
            "Title: Text Categorization with Support Vector Machines: Learning with Many Relevant Features\n",
            "Abstract: None\n",
            "----\n",
            "Paper 617:\n",
            "Title: A Review on Multi-Label Learning Algorithms\n",
            "Abstract: Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.\n",
            "----\n",
            "Paper 618:\n",
            "Title: Uncovering social spammers: social honeypots + machine learning\n",
            "Abstract: Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.\n",
            "----\n",
            "Paper 619:\n",
            "Title: Scaling up machine learning: parallel and distributed approaches\n",
            "Abstract: This tutorial gives a broad view of modern approaches for scaling up machine learning and data mining methods on parallel/distributed platforms. Demand for scaling up machine learning is task-specific: for some tasks it is driven by the enormous dataset sizes, for others by model complexity or by the requirement for real-time prediction. Selecting a task-appropriate parallelization platform and algorithm requires understanding their benefits, trade-offs and constraints. This tutorial focuses on providing an integrated overview of state-of-the-art platforms and algorithm choices. These span a range of hardware options (from FPGAs and GPUs to multi-core systems and commodity clusters), programming frameworks (including CUDA, MPI, MapReduce, and DryadLINQ), and learning settings (e.g., semi-supervised and online learning). The tutorial is example-driven, covering a number of popular algorithms (e.g., boosted trees, spectral clustering, belief propagation) and diverse applications (e.g., recommender systems and object recognition in vision).\n",
            " The tutorial is based on (but not limited to) the material from our upcoming Cambridge U. Press edited book which is currently in production.\n",
            " Visit the tutorial website at http://hunch.net/~large_scale_survey/\n",
            "----\n",
            "Paper 620:\n",
            "Title: Transfer learning using VGG-16 with Deep Convolutional Neural Network for Classifying Images\n",
            "Abstract: — Traditionally, data mining algorithms and machine learning algorithms are engineered to approach the problems in isolation. These algorithms are employed to train the model in separation on a specific feature space and same distribution. Depending on the business case, a model is trained by applying a machine learning algorithm for a specific task. A widespread assumption in the field of machine learning is that training data and test data must have identical feature spaces with the underlying distribution. On the contrary, in real world this assumption may not hold and thus models need to be rebuilt from the scratch if features and distribution changes. It is an arduous process to collect related training data and rebuild the models. In such cases, Transferring of Knowledge or transfer learning from disparate domains would be desirable. Transfer learning is a method of reusing a pre-trained model knowledge for another task. Transfer learning can be used for classification, regression and clustering problems. This paper uses one of the pre-trained models – VGG - 16 with Deep Convolutional Neural Network to classify images.\n",
            "----\n",
            "Paper 621:\n",
            "Title: Data Mining and Machine Learning in Cybersecurity\n",
            "Abstract: With the rapid advancement of information discovery techniques, machine learning and data mining continue to play a significant role in cybersecurity. Although several conferences, workshops, and journals focus on the fragmented research topics in this area, there has been no single interdisciplinary resource on past and current works and possible paths for future research in this area. This book fills this need. From basic concepts in machine learning and data mining to advanced problems in the machine learning domain, Data Mining and Machine Learning in Cybersecurity provides a unified reference for specific machine learning solutions to cybersecurity problems. It supplies a foundation in cybersecurity fundamentals and surveys contemporary challengesdetailing cutting-edge machine learning and data mining techniques. It also: Unveils cutting-edge techniques for detectingnew attacks Contains in-depth discussions of machine learning solutions to detection problems Categorizes methods for detecting, scanning, and profiling intrusions and anomalies Surveys contemporary cybersecurity problems and unveils state-of-the-art machine learning and data mining solutions Details privacy-preserving data mining methods This interdisciplinary resource includes technique review tables that allow for speedy access to common cybersecurity problems and associated data mining methods. Numerous illustrative figures help readers visualize the workflow of complex techniques and more than forty case studies provide a clear understanding of the design and application of data mining and machine learning techniques in cybersecurity.\n",
            "----\n",
            "Paper 622:\n",
            "Title: Deep Learning: A Primer for Radiologists.\n",
            "Abstract: Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. ©RSNA, 2017.\n",
            "----\n",
            "Paper 623:\n",
            "Title: Machine-Learning Research Four Current Directions\n",
            "Abstract: Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.\n",
            "----\n",
            "Paper 624:\n",
            "Title: SystemML: Declarative machine learning on MapReduce\n",
            "Abstract: MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.\n",
            "----\n",
            "Paper 625:\n",
            "Title: A Review of Machine Learning Algorithms for Text-Documents Classification\n",
            "Abstract: With the increasing availability of electronic documents and the rapid growth of the World Wide Web, the task of automatic categorization of documents became the key method for organizing the information and know- ledge discovery. Proper classification of e-documents, online news, blogs, e-mails and digital libraries need text mining, machine learning and natural language processing tech- niques to get meaningful knowledge. The aim of this paper is to highlight the important techniques and methodologies that are employed in text documents classification, while at the same time making awareness of some of the interesting challenges that remain to be solved, focused mainly on text representation and machine learning techniques. This paper provides a review of the theory and methods of document classification and text mining, focusing on the existing litera- ture.\n",
            "----\n",
            "Paper 626:\n",
            "Title: Model-based reinforcement learning: A survey\n",
            "Abstract: Reinforcement learning is an important branch of machine learning and artificial intelligence. Compared with traditional reinforcement learning, model-based reinforcement learning obtains the action of the next state by the model that has been learned\n",
            "----\n",
            "Paper 627:\n",
            "Title: A Survey on Deep Learning\n",
            "Abstract: The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.\n",
            "----\n",
            "Paper 628:\n",
            "Title: Application of Machine Learning To Epileptic Seizure Detection\n",
            "Abstract: We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96% of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.\n",
            "----\n",
            "Paper 629:\n",
            "Title: Making large scale SVM learning practical\n",
            "Abstract: Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.\n",
            "----\n",
            "Paper 630:\n",
            "Title: Adversarial Machine Learning\n",
            "Abstract: The author briefly introduces the emerging field of adversarial machine learning, in which opponents can cause traditional machine learning algorithms to behave poorly in security applications. He gives a high-level overview and mentions several types of attacks, as well as several types of defenses, and theoretical limits derived from a study of near-optimal evasion.\n",
            "----\n",
            "Paper 631:\n",
            "Title: UCI Repository of Machine Learning Database\n",
            "Abstract: None\n",
            "----\n",
            "Paper 632:\n",
            "Title: Machine Learning in Bioinformatics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 633:\n",
            "Title: Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples\n",
            "Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.\n",
            "----\n",
            "Paper 634:\n",
            "Title: Optimization for machine learning\n",
            "Abstract: This is a draft containing only sra chapter.tex and an abbreviated front matter. Please check that the formatting and small changes have been performed correctly. Please verify the affiliation. Please use this version for sending us future modifications.\n",
            "----\n",
            "Paper 635:\n",
            "Title: Never-Ending Learning\n",
            "Abstract: Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.\n",
            "----\n",
            "Paper 636:\n",
            "Title: The changing science of machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 637:\n",
            "Title: Machine Learning: The ingredients of machine learning\n",
            "Abstract: MACHINE LEARNING IS ALL ABOUT using the right features to build the right models that achieve the right tasks – this is the slogan, visualised in Figure 3 on p.11, with which we ended the Prologue. In essence, features define a ‘language’ in which we describe the relevant objects in our domain, be they e-mails or complex organic molecules. We should not normally have to go back to the domain objects themselves once we have a suitable feature representation, which is why features play such an important role in machine learning. We will take a closer look at them in Section 1.3. A task is an abstract representation of a problem we want to solve regarding those domain objects: the most common form of these is classifying them into two or more classes, but we shall encounter other tasks throughout the book. Many of these tasks can be represented as a mapping from data points to outputs. This mapping or model is itself produced as the output of a machine learning algorithm applied to training data; there is a wide variety of models to choose from, as we shall see in Section 1.2. We start this chapter by discussing tasks, the problems that can be solved with machine learning. No matter what variety of machine learning models you may encounter, you will find that they are designed to solve one of only a small number of tasks and use only a few different types of features.\n",
            "----\n",
            "Paper 638:\n",
            "Title: Extreme learning machine: algorithm, theory and applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 639:\n",
            "Title: From machine learning to machine reasoning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 640:\n",
            "Title: A survey of deep learning-based network anomaly detection\n",
            "Abstract: None\n",
            "----\n",
            "Paper 641:\n",
            "Title: Online Learning and Online Convex Optimization\n",
            "Abstract: Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.\n",
            "----\n",
            "Paper 642:\n",
            "Title: Learning with Kernels: support vector machines, regularization, optimization, and beyond\n",
            "Abstract: None\n",
            "----\n",
            "Paper 643:\n",
            "Title: Applications of Deep Learning and Reinforcement Learning to Biological Data\n",
            "Abstract: Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)–machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.\n",
            "----\n",
            "Paper 644:\n",
            "Title: Adaptive computation and machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 645:\n",
            "Title: Optimization method based extreme learning machine for classification\n",
            "Abstract: None\n",
            "----\n",
            "Paper 646:\n",
            "Title: Convex incremental extreme learning machine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 647:\n",
            "Title: Improving Machine Learning Approaches to Coreference Resolution\n",
            "Abstract: We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.\n",
            "----\n",
            "Paper 648:\n",
            "Title: Types of Machine Learning Algorithms\n",
            "Abstract: • Supervised learning --where the algorithm generates a function that maps inputs to desired outputs. One standard formulation of the supervised learning task is the classification problem: the learner is required to learn (to approximate the behavior of) a function which maps a vector into one of several classes by looking at several input-output examples of the function. • Unsupervised learning --which models a set of inputs: labeled examples are not available. • Semi-supervised learning --which combines both labeled and unlabeled examples to generate an appropriate function or classifier. • Reinforcement learning --where the algorithm learns a policy of how to act given an observation of the world. Every action has some impact in the environment, and the environment provides feedback that guides the learning algorithm. • Transduction --similar to supervised learning, but does not explicitly construct a function: instead, tries to predict new outputs based on training inputs, training outputs, and new inputs. • Learning to learn --where the algorithm learns its own inductive bias based on previous experience.\n",
            "----\n",
            "Paper 649:\n",
            "Title: The SHOGUN Machine Learning Toolbox\n",
            "Abstract: We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. \n",
            " \n",
            "SHOGUN is implemented in C++ and interfaces to MATLABTM, R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.\n",
            "----\n",
            "Paper 650:\n",
            "Title: Machine Learning Methods Without Tears: A Primer for Ecologists\n",
            "Abstract: Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise for the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modeling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical modeling approaches with which most ecologists are familiar. In this paper, we provide an introduction to three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical software, and provide an illustrative example. Although the ecological application of machine learning approaches has increased, there remains considerable skepticism with respect to the role of these techniques in ecology. Our review encourages a greater understanding of machine learning approaches and promotes their future application and utilization, while also providing a basis from which ecologists can make informed decisions about whether to select or avoid these approaches in their future modeling endeavors.\n",
            "----\n",
            "Paper 651:\n",
            "Title: Towards Making Systems Forget with Machine Unlearning\n",
            "Abstract: Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly. This paper focuses on making learning systems forget, the process of which we call machine unlearning, or simply unlearning. We present a general, efficient unlearning approach by transforming learning algorithms used by a system into a summation form. To forget a training data sample, our approach simply updates a small number of summations -- asymptotically faster than retraining from scratch. Our approach is general, because the summation form is from the statistical query learning in which many machine learning algorithms can be implemented. Our approach also applies to all stages of machine learning, including feature selection and modeling. Our evaluation, on four diverse learning systems and real-world workloads, shows that our approach is general, effective, fast, and easy to use.\n",
            "----\n",
            "Paper 652:\n",
            "Title: Machine learning\n",
            "Abstract: Machine learning is already a mature field with significant theoretical work and an impressive suite of applications. I will discuss learning algorithms together with some example applications, as well as the current challenges and research areas. WIREs Comp Stat 2011 3 195–203 DOI: 10.1002/wics.166\n",
            "----\n",
            "Paper 653:\n",
            "Title: Machine Learning in Medical Imaging\n",
            "Abstract: This article will discuss very different ways of using machine learning that may be less familiar, and we will demonstrate through examples the role of these concepts in medical imaging. Although the term machine learning is relatively recent, the ideas of machine learning have been applied to medical imaging for decades, perhaps most notably in the areas of computer-aided diagnosis (CAD) and functional brain mapping. We will not attempt in this brief article to survey the rich literature of this field. Instead our goals will be 1) to acquaint the reader with some modern techniques that are now staples of the machine-learning field and 2) to illustrate how these techniques can be employed in various ways in medical imaging.\n",
            "----\n",
            "Paper 654:\n",
            "Title: Online Learning for Matrix Factorization and Sparse Coding\n",
            "Abstract: Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.\n",
            "----\n",
            "Paper 655:\n",
            "Title: Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory\n",
            "Abstract: In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.\n",
            "----\n",
            "Paper 656:\n",
            "Title: Unsupervised Learning by Probabilistic Latent Semantic Analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 657:\n",
            "Title: Learning the Kernel Matrix with Semidefinite Programming\n",
            "Abstract: Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.\n",
            "----\n",
            "Paper 658:\n",
            "Title: Machine learning in adversarial environments\n",
            "Abstract: None\n",
            "----\n",
            "Paper 659:\n",
            "Title: IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 660:\n",
            "Title: Weka: Practical machine learning tools and techniques with Java implementations\n",
            "Abstract: The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform.\n",
            "----\n",
            "Paper 661:\n",
            "Title: Classes of Kernels for Machine Learning: A Statistics Perspective\n",
            "Abstract: In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.\n",
            "----\n",
            "Paper 662:\n",
            "Title: Online dictionary learning for sparse coding\n",
            "Abstract: Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.\n",
            "----\n",
            "Paper 663:\n",
            "Title: Selection of Relevant Features in Machine Learning\n",
            "Abstract: In this paper, we review the problem of selecting rele- vant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features accuracy) to grow slowly with the number of irrele- vant attributes. Theoretical results for algorithms that search restricted hypothesis spaces are encouraging. For instance, the worst-case number of errors made by Littlestone's (1987) WINNOW method grows only logarithmically with the number of irrelevant features. Pazzani and Sarrett's (1992) average-case analysis for WHOLIST, a simple conjunctive algorithm, and Lang- ley and Iba's (1993) treatment of the naive Bayesian classifier, suggest that their sample complexities grow at most linearly with the number of irrelevant features. However, the theoretical results are less optimistic for induction methods that search a larger space of concept descriptions. For example, Langley and Iba's (1993) average-case analysis of simple nearest neighbor indicates that its sample complexity grows exponen- tially with the number of irrelevant attributes, even for conjunctive target concepts. Experimental stud- ies of nearest neighbor are consistent with this conclu- sion, and other experiments suggest that similar results hold even for induction algorithms that explicitly se- lect features. For example, the sample complexity for decision-tree methods appears to grow linearly with the number of irrelevants for conjunctive concepts, but exponentially for parity concepts, since the evaluation metric cannot distinguish relevant from irrelevant fea- tures in the latter situation (Langley & Sage, in press). Results of this sort have encouraged machine learn- ing researchers to explore more sophisticated methods for selecting relevant features. In the sections that fol- low, we present a general framework for this task, and then consider some recent examples of work on this important problem.\n",
            "----\n",
            "Paper 664:\n",
            "Title: Archetypal analysis for machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 665:\n",
            "Title: Ensemble deep learning in bioinformatics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 666:\n",
            "Title: Machine Learning for Sequential Data: A Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 667:\n",
            "Title: Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies\n",
            "Abstract: Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested.\n",
            "----\n",
            "Paper 668:\n",
            "Title: Editorial: On Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 669:\n",
            "Title: Multi-Task Learning for Multiple Language Translation\n",
            "Abstract: In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.\n",
            "----\n",
            "Paper 670:\n",
            "Title: Deep Learning and Its Application to LHC Physics\n",
            "Abstract: Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.\n",
            "----\n",
            "Paper 671:\n",
            "Title: Machine Learning and Its Applications to Biology\n",
            "Abstract: The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data. \n",
            " \n",
            "The history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data. \n",
            " \n",
            "This tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).\n",
            "----\n",
            "Paper 672:\n",
            "Title: Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi\n",
            "Abstract: Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. However MLELM is significantly faster than any state−of−the−art deep networks.\n",
            "----\n",
            "Paper 673:\n",
            "Title: Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners\n",
            "Abstract: \n",
            " \n",
            " We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support VectorMachines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.\n",
            " \n",
            "\n",
            "----\n",
            "Paper 674:\n",
            "Title: Bioinformatics - The Machine Learning Approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 675:\n",
            "Title: Java-ML: A Machine Learning Library\n",
            "Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license.\n",
            "----\n",
            "Paper 676:\n",
            "Title: Machine learning and data mining\n",
            "Abstract: Over the past decade many organizations have begun to routinely capture huge volumes of historical data describing their operations, their products, and their customers. At the same time, scientists and engineers in many elds nd themselves capturing increasingly complex experimental datasets, such as the gigabytes of functional MRI data that describe brain activity in humans. The eld of data mining addresses the question of how best to use this historical data to discover general regularities and to improve future decisions.\n",
            "----\n",
            "Paper 677:\n",
            "Title: Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning\n",
            "Abstract: In this paper we propose Reward Machines – a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.\n",
            "----\n",
            "Paper 678:\n",
            "Title: What do you mean by collaborative learning\n",
            "Abstract: This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.\n",
            "----\n",
            "Paper 679:\n",
            "Title: Deep Learning for Classification of Malware System Call Sequences\n",
            "Abstract: None\n",
            "----\n",
            "Paper 680:\n",
            "Title: Machine Learning Benchmarks and Random Forest Regression\n",
            "Abstract: Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called ‘random forests’. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.\n",
            "----\n",
            "Paper 681:\n",
            "Title: Searching for exotic particles in high-energy physics with deep learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 682:\n",
            "Title: Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper\n",
            "Abstract: Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a fllter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs much more slowly than a fllter. This paper describes a new fllter approach to feature selection that uses a correlation based heuristic to evaluate the worth of feature subsets When applied as a data preprocessing step for two common machine learning algorithms, the new method compares favourably with the wrapper but requires much less computation.\n",
            "----\n",
            "Paper 683:\n",
            "Title: Statistical machine translation\n",
            "Abstract: Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.\n",
            "----\n",
            "Paper 684:\n",
            "Title: Weka-A Machine Learning Workbench for Data Mining\n",
            "Abstract: None\n",
            "----\n",
            "Paper 685:\n",
            "Title: A comparison of machine learning techniques for phishing detection\n",
            "Abstract: There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.\n",
            "----\n",
            "Paper 686:\n",
            "Title: Flow Clustering Using Machine Learning Techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 687:\n",
            "Title: Text Classification Using Machine Learning Techniques\n",
            "Abstract: Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.\n",
            "----\n",
            "Paper 688:\n",
            "Title: Orange: From Experimental Machine Learning to Interactive Data Mining\n",
            "Abstract: None\n",
            "----\n",
            "Paper 689:\n",
            "Title: Introduction to machine learning\n",
            "Abstract: The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods. Adaptive Computation and Machine Learning series\n",
            "----\n",
            "Paper 690:\n",
            "Title: Deep Learning in Microscopy Image Analysis: A Survey\n",
            "Abstract: Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.\n",
            "----\n",
            "Paper 691:\n",
            "Title: A Primer on PAC-Bayesian Learning\n",
            "Abstract: Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.\n",
            "----\n",
            "Paper 692:\n",
            "Title: Practical feature subset selection for machine learning\n",
            "Abstract: Machine learning algorithms automatically extract knowledge from machine readable information. Unfortunately, their success is usually dependant on the quality of the data that they operate on. If the data is inadequate, or contains extraneous and irrelevant information, machine learning algorithms may produce less accurate and less understandable results, or may fail to discover anything of use at all. Feature subset selectors are algorithms that attempt to identify and remove as much irrelevant and redundant information as possible prior to learning. Feature subset selection can result in enhanced performance, a reduced hypothesis search space, and, in some cases, reduced storage requirement. This paper describes a new feature selection algorithm that uses a correlation based heuristic to determine the “goodness” of feature subsets, and evaluates its effectiveness with three common machine learning algorithms. Experiments using a number of standard machine learning data sets are presented. Feature subset selection gave significant improvement for all three algorithms.\n",
            "----\n",
            "Paper 693:\n",
            "Title: Ontology Matching: A Machine Learning Approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 694:\n",
            "Title: Drug Design by Machine Learning: Support Vector Machines for Pharmaceutical Data Analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 695:\n",
            "Title: Overfitting and undercomputing in machine learning\n",
            "Abstract: A central problem in machine learning is supervised learning—that is, learning from labeled training data. For example, a learning system for medical diagnosis might be trained with examples of patients whose case records (medical tests, clinical observations) and diagnoses were known. The task of the learning system is to infer a function that predicts the diagnosis of a patient from his or her case records. The function to be learned might be represented as a set of rules, a decision tree, a Bayes network, or a neural network. Learning algorithms essentially operate by searching some space of functions (usually called the hypothesis class) for a function that fits the given data. Because there are usually exponentially many functions, this search cannot actually examine individual hypothesis functions but instead must use some more direct method of constructing the hypothesis functions from the data. This search can usually be formalized by defining an objective function (e.g., number of data points predicted incorrectly) and applying various algorithms to find a function that minimizes this objective function is NP-hard. For example, fitting the weights of a neural network or finding the smallest decision tree are both NP-complete problems [Blum and Rivest, 1989; Quinlan and Rivest 1989]. Hence, heuristic algorithms such as gradient descent (for neural networks) and greedy search (for decision trees) have been applied with great success. Of course, the suboptimality of such heuristic algorithms ~mmediately suggests a reas&able line of research: find ~lgorithms that can search the hypothesis class better. Hence, there has been extensive research in applying secondorder methods to fit neural networks and in conducting much more thorough searches in learning decision trees and rule sets. Ironically, when these algorithms were tested on real datasets, it was found that their performance was often worse than simrde szradient descent or greedy sear~h [&inlan and Cameron-Jones 1995; Weigend 1994]. In short: it appears to be bet~er not to optimize! One of the other important trends in machine-learning research has been the establishment and nurturing of connections between various previously disparate fields, including computational learning theory, connectionist learning, symbolic learning. and statistics. The . connection to statistics was crucial in resolvins$ this naradox. The-key p~oblem arises from the structure of the machine-learning task, A learning algorithm is trained on a set of training data, but then it is applied to make predictions on new data points. The goal is to maximize its predictive accuracy on the new data points—not necessarily its accuracy on the trammg data. Indeed, if we work too hard to find the very best fit to the training data, there is a risk that we will fit the noise in the data by memorizing various peculiarities\n",
            "----\n",
            "Paper 696:\n",
            "Title: Elements of Machine Learning\n",
            "Abstract: Elements of Machine Learning by Pat Langley Preface 1. An overview of machine learning 1.1 The science of machine learning 1.2 Nature of the environment 1.3 Nature of representation and performance 1.4 Nature of the learning component 1.5 Five paradigms for machine learning 1.6 Summary of the chapter 2. The induction of logical conjunctions 2.1 General issues in logical induction 2.2 Nonincremental induction of logical conjunctions 2.3 Heuristic induction of logical conjunctions 2.4 Incremental induction of logical conjunctions 2.5 Incremental hill climbing for logical conjunctions 2.6 Genetic algorithms for logical concept induction 2.7 Summary of the chapter 3. The induction of threshold concepts 3.1 General issues for threshold concepts 3.2 Induction of criteria tables 3.3 Induction of linear threshold units 3.4 Induction of spherical threshold units 3.5 Summary of the chapter 4. The induction of competitive concepts 4.1 Instance-based learning 4.2 Learning probabilistic concept descriptions 4.3 Summary of the chapter 5. The construction of decision lists 5.1 General issues in disjunctive concept induction 5.2 Nonincremental learning using separate and conquer 5.3 Incremental induction using separate and conquer 5.4 Induction of decision lists through exceptions 5.5 Induction of competitive disjunctions 5.6 Instance-storing algorithms 5.7 Complementary beam search for disjunctive concepts 5.8 Summary of the chapter 6. Revision and extension of inference networks 6.1 General issues surrounding inference network 6.2 Extending an incomplete inference network 6.3 Inducing specialized concepts with inference networks 6.4 Revising an incorrect inference network 6.5 Network construction and term generation 6.6 Summary of the chapter 7. The formation of concept hierarchies 7.1 General issues concerning concept hierarchies 7.2 Nonincremental divisive formation of hierarchies 7.3 Incremental formation of concept hierarchies 7.4 Agglomerative formation of concept hierarchies 7.5 Variations on hierarchies into other structures 7.7 Summary of the chapter 8. Other issues in concept induction 8.1 Overfitting and pruning 8.2 Selecting useful features 8.3 Induction for numeric prediction 8.4 Unsupervised concept induction 8.5 Inducing relational concepts 8.6 Handling missing features 8.7 Summary of the chapter 9. The formation of transition networks 9.1 General issues for state-transition networks 9.2 Constructing finite-state transition networks 9.3 Forming recursive transition networks 9.4 Learning rules and networks for prediction 9.5 Summary of the chapter 10. The acquisition of search-control knowledge 10.1 General issues in search control 10.2 Reinforcement learning 10.3 Learning state-space heuristics from solution traces 10.4 Learning control knowledge for problem reduction 10.5 Learning control knowledge for means-ends analysis 10.6 The utility of search-control knowledge 10.7 Summary of the chapter 11. The formation of macro-operators 11.1 General issues related to macro-operators 11.2 The creation of simple macro-operators 11.3 The formation of flexible macro-operators 11.4 Problem solving by analogy 11.5 The utility of macro-operators 11.6 Summary of the chapter 12. Prospects for machine learning 12.1 Additional areas of machine learning 12.2 Methodological trends in machine learning 12.3 The future of machine learning References Index\n",
            "----\n",
            "Paper 697:\n",
            "Title: Crafting Papers on Machine Learning\n",
            "Abstract: This essay gives advice to authors of papers on machine learning, although much of it car-ries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in di(cid:11)erent approaches to evaluation, and ways to improve a submission's ability to communicate ideas to its readers.\n",
            "----\n",
            "Paper 698:\n",
            "Title: Applications of machine learning and rule induction\n",
            "Abstract: Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere.\n",
            "----\n",
            "Paper 699:\n",
            "Title: OP-ELM: Optimally Pruned Extreme Learning Machine\n",
            "Abstract: In this brief, the optimally pruned extreme learning machine (OP-ELM) methodology is presented. It is based on the original extreme learning machine (ELM) algorithm with additional steps to make it more robust and generic. The whole methodology is presented in detail and then applied to several regression and classification problems. Results for both computational time and accuracy (mean square error) are compared to the original ELM and to three other widely used methodologies: multilayer perceptron (MLP), support vector machine (SVM), and Gaussian process (GP). As the experiments for both regression and classification illustrate, the proposed OP-ELM methodology performs several orders of magnitude faster than the other algorithms used in this brief, except the original ELM. Despite the simplicity and fast performance, the OP-ELM is still able to maintain an accuracy that is comparable to the performance of the SVM. A toolbox for the OP-ELM is publicly available online.\n",
            "----\n",
            "Paper 700:\n",
            "Title: Machine Learning and Data Mining: Introduction to Principles and Algorithms\n",
            "Abstract: Introduction Learning and intelligence Machine learning basics Knowledge representation Learning as search Attribute quality measures Data pre-processing Constructive induction Symbolic learning Statistical learning Artificial neural networks Cluster analysis Learning theory Computational learning theory Definitions References and index.\n",
            "----\n",
            "Rate limit exceeded. Retrying in 20 seconds... (Attempt 1/5)\n",
            "Paper 701:\n",
            "Title: Machine Learning and Data Mining: Introduction to Principles and Algorithms\n",
            "Abstract: Introduction Learning and intelligence Machine learning basics Knowledge representation Learning as search Attribute quality measures Data pre-processing Constructive induction Symbolic learning Statistical learning Artificial neural networks Cluster analysis Learning theory Computational learning theory Definitions References and index.\n",
            "----\n",
            "Paper 702:\n",
            "Title: Machine learning for detection and diagnosis of disease.\n",
            "Abstract: Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.\n",
            "----\n",
            "Paper 703:\n",
            "Title: The Need for Open Source Software in Machine Learning\n",
            "Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.\n",
            "----\n",
            "Paper 704:\n",
            "Title: Proceedings of the 24th international conference on Machine learning\n",
            "Abstract: This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. \n",
            " \n",
            "This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. \n",
            " \n",
            "ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. \n",
            " \n",
            "In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Scholkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. \n",
            " \n",
            "We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.\n",
            "----\n",
            "Paper 705:\n",
            "Title: The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics\n",
            "Abstract: Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not -- can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.\n",
            "----\n",
            "Paper 706:\n",
            "Title: Machine Learning Approaches to Estimating Software Development Effort\n",
            "Abstract: Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >\n",
            "----\n",
            "Paper 707:\n",
            "Title: Interactive machine learning\n",
            "Abstract: Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning\n",
            "----\n",
            "Paper 708:\n",
            "Title: TensorLy: Tensor Learning in Python\n",
            "Abstract: Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly\n",
            "----\n",
            "Paper 709:\n",
            "Title: Semi-Supervised Learning (Adaptive Computation and Machine Learning)\n",
            "Abstract: If searched for a ebook Semi-Supervised Learning (Adaptive Computation and Machine Learning series) in pdf format, then you have come on to right website. We presented utter variation of this ebook in DjVu, PDF, txt, doc, ePub forms. You may read Semi-Supervised Learning (Adaptive Computation and Machine Learning series) online or downloading. Further, on our site you can read the instructions and diverse artistic eBooks online, or downloading them. We like draw on your regard what our website does not store the eBook itself, but we give ref to the site wherever you can download or read online. If have necessity to downloading Semi-Supervised Learning (Adaptive Computation and Machine Learning series) pdf, in that case you come on to loyal website. We own Semi-Supervised Learning (Adaptive Computation and Machine Learning series) ePub, txt, PDF, DjVu, doc forms. We will be glad if you revert to us over.\n",
            "----\n",
            "Paper 710:\n",
            "Title: Machine Learning of Temporal Relations\n",
            "Abstract: This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.\n",
            "----\n",
            "Paper 711:\n",
            "Title: Machine Learning for Information Extraction in Informal Domains\n",
            "Abstract: None\n",
            "----\n",
            "Paper 712:\n",
            "Title: An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods\n",
            "Abstract: Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.\n",
            "----\n",
            "Paper 713:\n",
            "Title: Machine Learning for User Modeling\n",
            "Abstract: None\n",
            "----\n",
            "Paper 714:\n",
            "Title: Multimodal learning with deep Boltzmann machines\n",
            "Abstract: Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.\n",
            "----\n",
            "Paper 715:\n",
            "Title: Advanced Lectures on Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 716:\n",
            "Title: Bayesian Inference: An Introduction to Principles and Practice in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 717:\n",
            "Title: Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within Misuse Detection Context\n",
            "Abstract: A small subset of machine learning algorithms, mostly inductive learning based, applied to the KDD 1999 Cup intrusion detection dataset resulted in dismal performance for user-to-root and remote-to-local attack categories as reported in the recent literature. The uncertainty to explore if other machine learning algorithms can demonstrate better performance compared to the ones already employed constitutes the motivation for the study reported herein. Specifically, exploration of if certain algorithms perform better for certain attack classes and consequently, if a multi-expert classifier design can deliver desired performance measure is of high interest. This paper evaluates performance of a comprehensive set of pattern recognition and machine learning algorithms on four attack categories as found in the KDD 1999 Cup intrusion detection dataset. Results of simulation study implemented to that effect indicated that certain classification algorithms perform better for certain attack categories: a specific algorithm specialized for a given attack category . Consequently, a multi-classifier model, where a specific detection algorithm is associated with an attack category for which it is the most promising, was built. Empirical results obtained through simulation indicate that noticeable performance improvement was achieved for probing, denial of service, and user-to-root\n",
            "----\n",
            "Paper 718:\n",
            "Title: Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)\n",
            "Abstract: Machine learning is often used to automatically solve human tasks. In this paper, we look for tasks where machine learning algorithms are not as good as humans with the hope of gaining insight into their current limitations. We studied various Human Interactive Proofs (HIPs) on the market, because they are systems designed to tell computers and humans apart by posing challenges presumably too hard for computers. We found that most HIPs are pure recognition tasks which can easily be broken using machine learning. The harder HIPs use a combination of segmentation and recognition tasks. From this observation, we found that building segmentation tasks is the most effective way to confuse machine learning algorithms. This has enabled us to build effective HIPs (which we deployed in MSN Passport), as well as design challenging segmentation tasks for machine learning algorithms.\n",
            "----\n",
            "Paper 719:\n",
            "Title: Readings in Machine Learning\n",
            "Abstract: From the Publisher: \n",
            "The ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. \n",
            " \n",
            "Readings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide range of learning tasks, and that introduce a variety of techniques for giving machines the ability to learn. The editors, in cooperation with a group of expert referees, have chosen important papers that empirically study, theoretically analyze, or psychologically justify machine learning algorithms. The papers are grouped into a dozen categories, each of which is introduced by the editors.\n",
            "----\n",
            "Paper 720:\n",
            "Title: Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993\n",
            "Abstract: None\n",
            "----\n",
            "Paper 721:\n",
            "Title: Guest Editors' Introduction: On Applied Research in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 722:\n",
            "Title: On-line Algorithms in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 723:\n",
            "Title: A Machine Learning Approach to Workflow Management\n",
            "Abstract: None\n",
            "----\n",
            "Paper 724:\n",
            "Title: Machine Learning and Software Engineering\n",
            "Abstract: None\n",
            "----\n",
            "Paper 725:\n",
            "Title: The Interplay of Optimization and Machine Learning Research\n",
            "Abstract: The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.\n",
            "----\n",
            "Paper 726:\n",
            "Title: An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels\n",
            "Abstract: None\n",
            "----\n",
            "Paper 727:\n",
            "Title: Computational complexity of machine learning\n",
            "Abstract: This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model. \n",
            "We are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability. \n",
            "This thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems.\n",
            "----\n",
            "Paper 728:\n",
            "Title: Reliable Classifications with Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 729:\n",
            "Title: Machine learning as an experimental science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 730:\n",
            "Title: Machine learning: an artificial intelligence approach volume III\n",
            "Abstract: This book reflects the expansion of machine learning research through presentation of recent advances in the field. The book provides an account of current research directions. Major topics covered include the following: learning concepts and rules from examples; cognitive aspects of learning; learning by analogy; learning by observation and discovery; and an exploration of general aspects of learning.\n",
            "----\n",
            "Paper 731:\n",
            "Title: Evaluation and selection of biases in machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 732:\n",
            "Title: An Overview of Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 733:\n",
            "Title: Student Modeling and Machine Learning\n",
            "Abstract: After identifying essential student modeling issues and machine learning approaches, this paper examines how machine learning techniques have been used to automate the construction of student models as well as the background knowledge necessary for student modeling. In the process, the paper sheds light on the difficulty, suitability and potential of using machine learning for student modeling processes, and, to a lesser extent, the potential of using student modeling techniques in machine learning. (http://aied.inf.ed.ac.uk/members98/archive/vol_9/sison/full.html)\n",
            "----\n",
            "Paper 734:\n",
            "Title: Introduction to Semi-Supervised Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 735:\n",
            "Title: Machine Learning: Neural Networks, Genetic Algorithms, and Fuzzy Systems\n",
            "Abstract: Perceptron Learning with a Hidden Layer An Object-Oriented Backpropagation Learning Model Concurrent Backpropagation Learning Algorithms An Adaptive Conjugate Gradient Learning Algorithm for Efficient Training of Neural Networks A Concurrent Adaptive Conjugate Gradient Learning Algorithm on MIMD Shared Memory Machines A Concurrent Genetic/Neural Network Learning Algorithm for MIMD Shared Memory Machines A Hybrid Learning Algorithm for Distributed Memory Multicomputers A Fuzzy Neural Network Learning Model Appendices References Index.\n",
            "----\n",
            "Paper 736:\n",
            "Title: “Memo” Functions and Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 737:\n",
            "Title: Learning Algorithms for the Classification Restricted Boltzmann Machine\n",
            "Abstract: Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.\n",
            "----\n",
            "Paper 738:\n",
            "Title: Multi-Objective Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 739:\n",
            "Title: Introduction to Machine Learning (Adaptive Computation and Machine Learning)\n",
            "Abstract: None\n",
            "----\n",
            "Paper 740:\n",
            "Title: Machine-Learning Applications of Algorithmic Randomness\n",
            "Abstract: Machine-LearningApplicationsofAlgorithmicRandomnessVolodyaovk,AlexGammerman,CraigSaundersComputerLearningResearchCentreandDepartmentofScienceRoyalHollowa,UniversitofLondon,Egham,SurreyTW200EX,Englandfvovk,alex,craigg@dcs.rhbnc.ac.ukAbstractMostmachinelearningalgorithmssharethefollowingdrawback:theyonlyoutputbarepredictionsbutnotthecon denceinthosepredictions.Inthe1960salgorithmicinfor-mationtheorysupplieduniversalmeasuresofcon dencebuttheseare,unfortunately,non-computable.Inthispap erwecombinetheideasofalgorithmicinformationtheorywiththetheoryofSupp ortVectormachinestoobtainpracticableapproximationsuni-versalmeasuresofcon dence.Weshowthatinsomestandardproblemsofpatternrecog-nitionourapproximationsworkell.1INTRODUCTIONTwoimp ortantdi erencesofmostmo dernmetho dsmachinelearning(suchasstatisticaltheory,seeVapnik[21],1998,orPACtheory)fromclassicalstatisticalmetho dsarethat:\u000fmachinelearningmetho dspro ducebarepredic-tions,withoutestimatingcon denceinthosepre-dictions(unlike,eg,predictionoffutureobser-vationsintraditionalstatistics(Guttman[5],1970));\u000fmanymachinelearningmetho dsaredesignedtowork(andtheirp erformanceisanalysed)un-derthegeneraliidassumption(unlikeclas-sicalparametricstatistics)andtheyareabletodealwithextremelyhigh-dimensionalhyp othesisspaces;cfVapnik[21](1998).Inthispap erwewillfurtherdeveloptheapproachofGammermanetal[4](1998)andSaunders[17Figure1:Ifthetrainingsetonlycontainsclear2sand7s,weouldliktoattachmucloercon dencethemiddleimagethantorightandleftones(1999),wherethegoalistoobtaincon dencesforpredictionsunderthegeneraliidassumptioninhigh-dimensionalsituations.Figure1demonstratesthede-sirabilityofcon dences.Themaincontributionthispap erisemb eddingtheapproachesofGammermanetal[4](1998)andSaunderset[17(1999)intoagen-eralschemebasedonthenotionofalgorithmicran-domness.Aswillb ecomeclearlater,theproblemofassigningcon dencestopredictionsiscloselyconnectedtheproblemofde ningrandomsequences.ThelatterproblemwassolvedbyKolmogorov[8](1965),whobasedhisde nitionontheexistenceUniver-salTuringMachine(thoughitb ecameclearthatKol-mogorov'sde nitiondo essolvetheproblemofde ningrandomsequencesonlyafterMartin-Lof 'spap er[15],1966);Kolmogorov'sde nitionmovedthenotionofrandomnessfromthegreyareasurroundingprobabil-itytheoryandstatisticstomathematicalcomputersci-ence.Kolmogorovb elievedhisnotionofrandomnesstob easuitablebasisforapplicationsofprobability.Unfor-tunately,fateideaasdi erentfromKol-mogorov's1933axioms(Kolmogorov[7],1933),which\n",
            "----\n",
            "Paper 741:\n",
            "Title: Big Data Deep Learning: Challenges and Perspectives\n",
            "Abstract: Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.\n",
            "----\n",
            "Paper 742:\n",
            "Title: Deep Learning for Content-Based Image Retrieval: A Comprehensive Study\n",
            "Abstract: Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.\n",
            "----\n",
            "Paper 743:\n",
            "Title: The children's machine: rethinking school in the age of the computer\n",
            "Abstract: Yearners and Schoolers Personal Thinking School: Change and Resistance to Change Teachers A World for Learning An Anthology of Learning Stories Instructionism versus Constructionism Computerists Yearners and Schoolers Cybernetics What can be done?\n",
            "----\n",
            "Paper 744:\n",
            "Title: Machine Learning: A Theoretical Approach\n",
            "Abstract: Chapter 1 Introduction Chapter 2 Learning Concept on Countable Domains Chapter 3 Time Complexity of Concept Learning Chapter 4 Learning Concepts on Uncoutable Domains Chapter 5 Learning Functions Chapter 6 Finite Automata Chapter 7 Neural Networks Chapter 8 Generalizing the Learning Model Chapter 9 Conclusion\n",
            "----\n",
            "Paper 745:\n",
            "Title: Genetic Algorithms in Machine Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 746:\n",
            "Title: Boosting for transfer learning\n",
            "Abstract: Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.\n",
            "----\n",
            "Paper 747:\n",
            "Title: Efficient Learning Machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 748:\n",
            "Title: Ontology Learning for the Semantic Web\n",
            "Abstract: The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.\n",
            "----\n",
            "Paper 749:\n",
            "Title: The Extreme Value Machine\n",
            "Abstract: It is often desirable to be able to recognize when inputs to a recognition function learned in a supervised manner correspond to classes unseen at training time. With this ability, new class labels could be assigned to these inputs by a human operator, allowing them to be incorporated into the recognition function—ideally under an efficient incremental update mechanism. While good algorithms that assume inputs from a fixed set of classes exist, e.g. , artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Existing algorithms take little to no distributional information into account when learning recognition functions and lack a strong theoretical foundation. We address this gap by formulating a novel, theoretically sound classifier—the Extreme Value Machine (EVM). The EVM has a well-grounded interpretation derived from statistical Extreme Value Theory (EVT), and is the first classifier to be able to perform nonlinear kernel-free variable bandwidth incremental learning. Compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset.\n",
            "----\n",
            "Paper 750:\n",
            "Title: Extreme learning machines: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 751:\n",
            "Title: Learning using privileged information: similarity control and knowledge transfer\n",
            "Abstract: This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.\n",
            "----\n",
            "Paper 752:\n",
            "Title: Using AUC and accuracy in evaluating learning algorithms\n",
            "Abstract: The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.\n",
            "----\n",
            "Paper 753:\n",
            "Title: Transfer Learning for Reinforcement Learning Domains: A Survey\n",
            "Abstract: The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.\n",
            "----\n",
            "Paper 754:\n",
            "Title: Self-taught learning: transfer learning from unlabeled data\n",
            "Abstract: We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.\n",
            "----\n",
            "Paper 755:\n",
            "Title: Temporal difference learning and TD-Gammon\n",
            "Abstract: Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.\n",
            "----\n",
            "Paper 756:\n",
            "Title: Learning Question Classifiers\n",
            "Abstract: In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.\n",
            "----\n",
            "Paper 757:\n",
            "Title: Support Vector Machines for Multiple-Instance Learning\n",
            "Abstract: This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristic ally. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization.\n",
            "----\n",
            "Paper 758:\n",
            "Title: Deep Learning using Linear Support Vector Machines\n",
            "Abstract: Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these \"deep learning\" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.\n",
            "----\n",
            "Paper 759:\n",
            "Title: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data\n",
            "Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.\n",
            "----\n",
            "Paper 760:\n",
            "Title: Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning\n",
            "Abstract: One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.\n",
            "----\n",
            "Paper 761:\n",
            "Title: On the mathematical foundations of learning\n",
            "Abstract: (1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In\n",
            "----\n",
            "Paper 762:\n",
            "Title: Multiple kernel learning, conic duality, and the SMO algorithm\n",
            "Abstract: While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.\n",
            "----\n",
            "Paper 763:\n",
            "Title: Evolutionary extreme learning machine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 764:\n",
            "Title: Algorithmic Learning in a Random World\n",
            "Abstract: None\n",
            "----\n",
            "Paper 765:\n",
            "Title: Toward an Online Anomaly Intrusion Detection System Based on Deep Learning\n",
            "Abstract: In the past twenty years, progress in intrusion detection has been steady but slow. The biggest challenge is to detect new attacks in real time. In this work, a deep learning approach for anomaly detection using a Restricted Boltzmann Machine (RBM) and a deep belief network are implemented. Our method uses a one-hidden layer RBM to perform unsupervised feature reduction. The resultant weights from this RBM are passed to another RBM producing a deep belief network. The pre-trained weights are passed into a fine tuning layer consisting of a Logistic Regression (LR) classifier with multi-class soft-max. We have implemented the deep learning architecture in C++ in Microsoft Visual Studio 2013 and we use the DARPA KDDCUP'99 dataset to evaluate its performance. Our architecture outperforms previous deep learning methods implemented by Li and Salama in both detection speed and accuracy. We achieve a detection rate of 97.9% on the total 10% KDDCUP'99 test dataset. By improving the training process of the simulation, we are also able to produce a low false negative rate of 2.47%. Although the deficiencies in the KDDCUP'99 dataset are well understood, it still presents machine learning approaches for predicting attacks with a reasonable challenge. Our future work will include applying our machine learning strategy to larger and more challenging datasets, which include larger classes of attacks.\n",
            "----\n",
            "Paper 766:\n",
            "Title: Single-machine scheduling with learning considerations\n",
            "Abstract: None\n",
            "----\n",
            "Paper 767:\n",
            "Title: Imbalanced Learning: Foundations, Algorithms, and Applications\n",
            "Abstract: The first book of its kind to review the current status and future direction of the exciting new branch of machine learning/data mining called imbalanced learningImbalanced learning focuses on how an intelligent system can learn when it is provided with imbalanced data. Solving imbalanced learning problems is critical in numerous data-intensive networked systems, including surveillance, security, Internet, finance, biomedical, defense, and more. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. The first comprehensive look at this new branch of machine learning, this book offers a critical review of the problem of imbalanced learning, covering the state of the art in techniques, principles, and real-world applications. Featuring contributions from experts in both academia and industry, Imbalanced Learning: Foundations, Algorithms, and Applications provides chapter coverage on:Foundations of Imbalanced LearningImbalanced Datasets: From Sampling to ClassifiersEnsemble Methods for Class Imbalance LearningClass Imbalance Learning Methods for Support Vector MachinesClass Imbalance and Active LearningNonstationary Stream Data Learning with Imbalanced Class DistributionAssessment Metrics for Imbalanced LearningImbalanced Learning: Foundations, Algorithms, and Applications will help scientists and engineers learn how to tackle the problem of learning from imbalanced datasets, and gain insight into current developments in the field as well as future research directions.\n",
            "----\n",
            "Paper 768:\n",
            "Title: Sparse Bayesian learning for basis selection\n",
            "Abstract: Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.\n",
            "----\n",
            "Paper 769:\n",
            "Title: Deep learning with COTS HPC systems\n",
            "Abstract: Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.\n",
            "----\n",
            "Paper 770:\n",
            "Title: Regularized Extreme Learning Machine\n",
            "Abstract: Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.\n",
            "----\n",
            "Paper 771:\n",
            "Title: Large Scale Multiple Kernel Learning\n",
            "Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun .\n",
            "----\n",
            "Paper 772:\n",
            "Title: Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond\n",
            "Abstract: From the Publisher: \n",
            "In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. \n",
            "Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.\n",
            "----\n",
            "Paper 773:\n",
            "Title: Collaborative Learning: Cognitive and Computational Approaches\n",
            "Abstract: Acknowledgement. Contributors. Introduction: what do you mean by 'collaborative learning'? (P. Dillenbourg). Learning together: understanding the processes of computer-based collaborative learning (K. Littleton, P. Hakkinen). The role of grounding in collaborative learning tasks (M. Baker et al.). What is \"multi\" in multi-agent learning? (G. Weiss, P. Dillenbourg). Comparing human-human and robot-robot interactions (R. Joiner et al.). Learning by explaining to oneself and to others (R. Ploetzner et al.). Knowledge transformations in agents and interactions: a comparison of machine learning and dialogue operators (E. Mephu Nguifo et al.). Can analytic models support learning in groups? (H.U. Hoppe, R. Ploetzner). Using telematics for collaborative knowledge construction (T. Hansen et al.). The productive agency that drives collaborative learning (D. Schwatrtz). References. Index.\n",
            "----\n",
            "Paper 774:\n",
            "Title: Scaling learning algorithms towards AI\n",
            "Abstract: One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.\n",
            "----\n",
            "Paper 775:\n",
            "Title: A Survey on Metric Learning for Feature Vectors and Structured Data\n",
            "Abstract: The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.\n",
            "----\n",
            "Paper 776:\n",
            "Title: Evaluating Learning Algorithms: A Classification Perspective\n",
            "Abstract: The field of machine learning has matured to the point where many sophisticated learning approaches can be applied to practical applications. Thus it is of critical importance that researchers have the proper tools to evaluate learning approaches and understand the underlying issues. This book examines various aspects of the evaluation process with an emphasis on classification algorithms. The authors describe several techniques for classifier performance assessment, error estimation and resampling, obtaining statistical significance as well as selecting appropriate domains for evaluation. They also present a unified evaluation framework and highlight how different components of evaluation are both significantly interrelated and interdependent. The techniques presented in the book are illustrated using R and WEKA facilitating better practical insight as well as implementation.Aimed at researchers in the theory and applications of machine learning, this book offers a solid basis for conducting performance evaluations of algorithms in practical settings.\n",
            "----\n",
            "Paper 777:\n",
            "Title: The Cerebellum: A Neuronal Learning Machine?\n",
            "Abstract: Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning. Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibulo-ocular reflex suggest that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.\n",
            "----\n",
            "Paper 778:\n",
            "Title: Semi-Supervised Learning\n",
            "Abstract: In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series\n",
            "----\n",
            "Paper 779:\n",
            "Title: The Helmholtz Machine\n",
            "Abstract: Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.\n",
            "----\n",
            "Paper 780:\n",
            "Title: Learning to learn with the informative vector machine\n",
            "Abstract: This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.\n",
            "----\n",
            "Paper 781:\n",
            "Title: Active Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 782:\n",
            "Title: Learning Collaborative Information Filters\n",
            "Abstract: Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.\n",
            "----\n",
            "Paper 783:\n",
            "Title: UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\n",
            "Abstract: UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.\n",
            "----\n",
            "Paper 784:\n",
            "Title: Automatic differentiation in PyTorch\n",
            "Abstract: In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.\n",
            "----\n",
            "Paper 785:\n",
            "Title: Measuring the VC-Dimension of a Learning Machine\n",
            "Abstract: A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.\n",
            "----\n",
            "Paper 786:\n",
            "Title: XGBoost: A Scalable Tree Boosting System\n",
            "Abstract: Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.\n",
            "----\n",
            "Paper 787:\n",
            "Title: “Why Should I Trust You?”: Explaining the Predictions of Any Classifier\n",
            "Abstract: Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\n",
            "----\n",
            "Paper 788:\n",
            "Title: Overcoming catastrophic forgetting in neural networks\n",
            "Abstract: Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.\n",
            "----\n",
            "Paper 789:\n",
            "Title: A Perspective View and Survey of Meta-Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 790:\n",
            "Title: and Machine\n",
            "Abstract: This editorial introduces the first part of CEJEME’s Special Issue on Artificial Intelligence and Machine Learning in Educational Measurement. As AI and ML technologies revolutionize education, they offer new opportunities for personalized learning and innovative assessment practices. This issue highlights the transformative impact of AI and ML on educational measurement, addressing both their potential and the ethical challenges they pose. This issue includes four articles that explore the opportunities and ethical challenges of AI in educational measurement, automated text scoring in the age of Generative AI for the GPU-poor, a novel approach using autoencoders and BERT to detect compromised items in computerized testing, and the use of ML packages in R. The issue provides valuable insights into the future of educational measurement. A second part of this special issue will be available in spring 2025.\n",
            "----\n",
            "Paper 791:\n",
            "Title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\n",
            "Abstract: This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.\n",
            "----\n",
            "Paper 792:\n",
            "Title: Distributed Representations of Sentences and Documents\n",
            "Abstract: Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.\n",
            "----\n",
            "Paper 793:\n",
            "Title: Dropout: a simple way to prevent neural networks from overfitting\n",
            "Abstract: Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n",
            "----\n",
            "Paper 794:\n",
            "Title: Explaining and Harnessing Adversarial Examples\n",
            "Abstract: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\n",
            "----\n",
            "Paper 795:\n",
            "Title: A Learning Machine: Part I\n",
            "Abstract: Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.\n",
            "----\n",
            "Paper 796:\n",
            "Title: Adversarial examples in the physical world\n",
            "Abstract: Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.\n",
            "----\n",
            "Paper 797:\n",
            "Title: Learning and relearning in Boltzmann machines\n",
            "Abstract: This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References\n",
            "----\n",
            "Paper 798:\n",
            "Title: From local explanations to global understanding with explainable AI for trees\n",
            "Abstract: None\n",
            "----\n",
            "Paper 799:\n",
            "Title: Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data\n",
            "Abstract: Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.\n",
            "----\n",
            "Paper 800:\n",
            "Title: A Comprehensive Survey on Graph Neural Networks\n",
            "Abstract: Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.\n",
            "----\n",
            "Paper 801:\n",
            "Title: Density estimation using Real NVP\n",
            "Abstract: Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.\n",
            "----\n",
            "Paper 802:\n",
            "Title: LIBSVM: A library for support vector machines\n",
            "Abstract: LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.\n",
            "----\n",
            "Paper 803:\n",
            "Title: Siamese Neural Networks for One-Shot Image Recognition\n",
            "Abstract: The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.\n",
            "----\n",
            "Paper 804:\n",
            "Title: Random Forests\n",
            "Abstract: None\n",
            "----\n",
            "Paper 805:\n",
            "Title: The mythos of model interpretability\n",
            "Abstract: In machine learning, the concept of interpretability is both important and slippery.\n",
            "----\n",
            "Paper 806:\n",
            "Title: Competition-level code generation with AlphaCode\n",
            "Abstract: Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.\n",
            "----\n",
            "Paper 807:\n",
            "Title: Support vector machines\n",
            "Abstract: Support vector machines (SVMs) are a family of machine learning methods, originally introduced for the problem of classification and later generalized to various other situations. They are based on principles of statistical learning theory and convex optimization, and are currently used in various domains of application, including bioinformatics, text categorization, and computer vision. Copyright © 2009 John Wiley & Sons, Inc.\n",
            "----\n",
            "Paper 808:\n",
            "Title: Statistical Comparisons of Classifiers over Multiple Data Sets\n",
            "Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.\n",
            "----\n",
            "Paper 809:\n",
            "Title: Experiments with a New Boosting Algorithm\n",
            "Abstract: In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.\n",
            "----\n",
            "Paper 810:\n",
            "Title: Spark: Cluster Computing with Working Sets\n",
            "Abstract: MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.\n",
            "----\n",
            "Paper 811:\n",
            "Title: Understanding Black-box Predictions via Influence Functions\n",
            "Abstract: How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.\n",
            "----\n",
            "Paper 812:\n",
            "Title: Deep Sets\n",
            "Abstract: In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \\emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \\citep{Jung15Exploration}, to cosmology \\citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging.\n",
            "----\n",
            "Paper 813:\n",
            "Title: Descriptor : The HAM 10000 dataset , a large collection of multi-source dermatoscopic images of common pigmented skin lesions\n",
            "Abstract: Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.\n",
            "----\n",
            "Paper 814:\n",
            "Title: Understanding of a convolutional neural network\n",
            "Abstract: The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.\n",
            "----\n",
            "Paper 815:\n",
            "Title: Collective Classification in Network Data\n",
            "Abstract: Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.\n",
            "----\n",
            "Paper 816:\n",
            "Title: Learning Multiple Tasks with Kernel Methods\n",
            "Abstract: We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.\n",
            "----\n",
            "Paper 817:\n",
            "Title: Laplacian Eigenmaps for Dimensionality Reduction and Data Representation\n",
            "Abstract: One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.\n",
            "----\n",
            "Paper 818:\n",
            "Title: Random Features for Large-Scale Kernel Machines\n",
            "Abstract: To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.\n",
            "----\n",
            "Paper 819:\n",
            "Title: Introduction to Statistical Pattern Recognition\n",
            "Abstract: Annotation : Pattern recognition problem is briefly characterized as a process of machine learning. Its main stages (dimensionality reduction and classifier design) are stated. Statistical approach is given priority here. Two approaches to dimensionality reduction, namely feature selection (FS) and feature extraction (FE) are specified. Though FS is a special case of FE, they are very different from a practical viewpoint and thus must be considered separately.\n",
            "----\n",
            "Paper 820:\n",
            "Title: An Introduction to Genetic Algorithms.\n",
            "Abstract: An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a \"Brief History\" and \"Future Directions.\" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering.\n",
            "----\n",
            "Paper 821:\n",
            "Title: Genetic Algorithms + Data Structures = Evolution Programs\n",
            "Abstract: None\n",
            "----\n",
            "Paper 822:\n",
            "Title: ELLA: An Efficient Lifelong Learning Algorithm\n",
            "Abstract: The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines. In this paper, we develop a method for online multi-task learning in the lifelong learning setting. The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multitask learning methods, and provide robust theoretical performance guarantees. We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.\n",
            "----\n",
            "Paper 823:\n",
            "Title: A Practical Guide to Training Restricted Boltzmann Machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 824:\n",
            "Title: Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks\n",
            "Abstract: Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.\n",
            "----\n",
            "Paper 825:\n",
            "Title: From Data Mining to Knowledge Discovery in Databases\n",
            "Abstract: ■ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.\n",
            "----\n",
            "Paper 826:\n",
            "Title: The relationship between Precision-Recall and ROC curves\n",
            "Abstract: Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.\n",
            "----\n",
            "Paper 827:\n",
            "Title: A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts\n",
            "Abstract: Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.\n",
            "----\n",
            "Paper 828:\n",
            "Title: Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\n",
            "Abstract: Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.\n",
            "----\n",
            "Paper 829:\n",
            "Title: Theano: A Python framework for fast computation of mathematical expressions\n",
            "Abstract: Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. \n",
            "The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.\n",
            "----\n",
            "Paper 830:\n",
            "Title: Survey of clustering algorithms\n",
            "Abstract: Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.\n",
            "----\n",
            "Paper 831:\n",
            "Title: Neural Networks and the Bias/Variance Dilemma\n",
            "Abstract: Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.\n",
            "----\n",
            "Paper 832:\n",
            "Title: Full regularization path for sparse principal component analysis\n",
            "Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients, with complexity O(n3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n3). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.\n",
            "----\n",
            "Paper 833:\n",
            "Title: State of the Art\n",
            "Abstract: We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …\n",
            "----\n",
            "Paper 834:\n",
            "Title: A review of feature selection techniques in bioinformatics\n",
            "Abstract: Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.\n",
            "----\n",
            "Paper 835:\n",
            "Title: Concrete Problems in AI Safety\n",
            "Abstract: Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.\n",
            "----\n",
            "Paper 836:\n",
            "Title: Artificial intelligence in healthcare: past, present and future\n",
            "Abstract: Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.\n",
            "----\n",
            "Paper 837:\n",
            "Title: Sequence Transduction with Recurrent Neural Networks\n",
            "Abstract: Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.\n",
            "----\n",
            "Paper 838:\n",
            "Title: Orange: data mining toolbox in python\n",
            "Abstract: Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.\n",
            "----\n",
            "Paper 839:\n",
            "Title: Return of Frustratingly Easy Domain Adaptation\n",
            "Abstract: \n",
            " \n",
            " Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n",
            " \n",
            "\n",
            "----\n",
            "Paper 840:\n",
            "Title: Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\n",
            "Abstract: Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.\n",
            "----\n",
            "Paper 841:\n",
            "Title: An Introduction to Convolutional Neural Networks\n",
            "Abstract: The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. \n",
            "This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\n",
            "----\n",
            "Paper 842:\n",
            "Title: Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms\n",
            "Abstract: Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.\n",
            "----\n",
            "Paper 843:\n",
            "Title: Estimating Attributes: Analysis and Extensions of RELIEF\n",
            "Abstract: None\n",
            "----\n",
            "Paper 844:\n",
            "Title: Adaptive Federated Optimization\n",
            "Abstract: Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Due to the heterogeneity of the client datasets, standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.\n",
            "----\n",
            "Paper 845:\n",
            "Title: Deep One-Class Classification\n",
            "Abstract: Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.\n",
            "----\n",
            "Paper 846:\n",
            "Title: Counterfactual Fairness\n",
            "Abstract: Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.\n",
            "----\n",
            "Paper 847:\n",
            "Title: Large Margin Methods for Structured and Interdependent Output Variables\n",
            "Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.\n",
            "----\n",
            "Paper 848:\n",
            "Title: CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy\n",
            "Abstract: Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.\n",
            "----\n",
            "Paper 849:\n",
            "Title: A review of classification algorithms for EEG-based brain–computer interfaces: a 10 year update\n",
            "Abstract: Objective. Most current electroencephalography (EEG)-based brain–computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs. Approach. We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons. Main results. We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods. Significance. This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.\n",
            "----\n",
            "Paper 850:\n",
            "Title: Model Cards for Model Reporting\n",
            "Abstract: Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.\n",
            "----\n",
            "Paper 851:\n",
            "Title: Gradient boosting machines, a tutorial\n",
            "Abstract: Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.\n",
            "----\n",
            "Paper 852:\n",
            "Title: Solving the quantum many-body problem with artificial neural networks\n",
            "Abstract: Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.\n",
            "----\n",
            "Paper 853:\n",
            "Title: Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition\n",
            "Abstract: Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.\n",
            "----\n",
            "Paper 854:\n",
            "Title: Twitter Sentiment Classiﬁcation using Distant Supervision\n",
            "Abstract: We introduce a novel approach for automatically classifying the sentiment of Twitter messages. These messages are classiﬁed as either positive or negative with respect to a query term. This is useful for consumers who want to research the sentiment of products before purchase, or companies that want to monitor the public sentiment of their brands. There is no previous research on classifying sentiment of messages on microblogging services like Twitter. We present the results of machine learning algorithms for classifying the sentiment of Twitter messages using distant supervision. Our training data consists of Twitter messages with emoticons, which are used as noisy labels. This type of training data is abundantly available and can be obtained through automated means. We show that machine learning algorithms (Naive Bayes, Maximum Entropy, and SVM) have accuracy above 80% when trained with emoticon data. This paper also describes the preprocessing steps needed in order to achieve high accuracy. The main contribution of this paper is the idea of using tweets with emoticons for distant supervised learning.\n",
            "----\n",
            "Paper 855:\n",
            "Title: Learning and evaluating classifiers under sample selection bias\n",
            "Abstract: Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.\n",
            "----\n",
            "Paper 856:\n",
            "Title: An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks\n",
            "Abstract: Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.\n",
            "----\n",
            "Paper 857:\n",
            "Title: The class imbalance problem: A systematic study\n",
            "Abstract: In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.\n",
            "----\n",
            "Paper 858:\n",
            "Title: Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution\n",
            "Abstract: Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality\n",
            "----\n",
            "Paper 859:\n",
            "Title: A Tutorial on the Cross-Entropy Method\n",
            "Abstract: None\n",
            "----\n",
            "Paper 860:\n",
            "Title: Differentially Private Empirical Risk Minimization\n",
            "Abstract: Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.\n",
            "----\n",
            "Paper 861:\n",
            "Title: An Efficient Boosting Algorithm for Combining Preferences\n",
            "Abstract: We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the “collaborative-ﬁltering” problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efﬁcient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm’s behavior both on the training data, and on new test data not seen during training. We also describe an efﬁcient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the ﬁrst exper-iment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-ﬁltering task for making movie recommendations.\n",
            "----\n",
            "Paper 862:\n",
            "Title: Factors in automatic musical genre classification of audio signals\n",
            "Abstract: Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.\n",
            "----\n",
            "Paper 863:\n",
            "Title: Building Machines that Learn and Think Like People\n",
            "Abstract: Recent successes in artificial intelligence and machine learning have been largely driven by methods for sophisticated pattern recognition, including deep neural networks and other data-intensive methods. But human intelligence is more than just pattern recognition. And no machine system yet built has anything like the flexible, general-purpose commonsense grasp of the world that we can see in even a one-year-old human infant. I will consider how we might capture the basic learning and thinking abilities humans possess from early childhood, as one route to building more human-like forms of machine learning and thinking. At the heart of human common sense is our ability to model the physical and social environment around us: to explain and understand what we see, to imagine things we could see but haven't yet, to solve problems and plan actions to make these things real, and to build new models as we learn more about the world. I will focus on our recent work reverse-engineering these capacities using methods from probabilistic programming, program induction and program synthesis, which together with deep learning methods and video game simulation engines, provide a toolkit for the joint enterprise of modeling human intelligence and making AI systems smarter in more human-like ways.\n",
            "----\n",
            "Paper 864:\n",
            "Title: Sparse Feature Learning for Deep Belief Networks\n",
            "Abstract: Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.\n",
            "----\n",
            "Paper 865:\n",
            "Title: Quantile Regression Forests\n",
            "Abstract: Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.\n",
            "----\n",
            "Paper 866:\n",
            "Title: Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval\n",
            "Abstract: None\n",
            "----\n",
            "Paper 867:\n",
            "Title: PennyLane: Automatic differentiation of hybrid quantum-classical computations\n",
            "Abstract: PennyLane is a Python 3 software framework for optimization and machine learning of quantum and hybrid quantum-classical computations. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for Strawberry Fields, Rigetti Forest, Qiskit, Cirq, and ProjectQ, allowing PennyLane optimizations to be run on publicly accessible quantum devices provided by Rigetti and IBM Q. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, and autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.\n",
            "----\n",
            "Paper 868:\n",
            "Title: Text Classification Algorithms: A Survey\n",
            "Abstract: In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.\n",
            "----\n",
            "Paper 869:\n",
            "Title: Less is More: Active Learning with Support Vector Machines\n",
            "Abstract: We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classiﬁcation tasks. We observe a number of beneﬁts, the most surprising of which is that a SVM trained on a well-chosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data.\n",
            "----\n",
            "Paper 870:\n",
            "Title: 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text\n",
            "Abstract: The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.\n",
            "----\n",
            "Paper 871:\n",
            "Title: kernlab - An S4 Package for Kernel Methods in R\n",
            "Abstract: kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.\n",
            "----\n",
            "Paper 872:\n",
            "Title: POT: Python Optimal Transport\n",
            "Abstract: Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel eﬃcient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.\n",
            "----\n",
            "Paper 873:\n",
            "Title: Molecular graph convolutions: moving beyond fingerprints\n",
            "Abstract: None\n",
            "----\n",
            "Paper 874:\n",
            "Title: Generalizing from a Few Examples\n",
            "Abstract: Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1\n",
            "----\n",
            "Paper 875:\n",
            "Title: On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\n",
            "Abstract: Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.\n",
            "----\n",
            "Paper 876:\n",
            "Title: Support-Vector Networks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 877:\n",
            "Title: Quantum-chemical insights from deep tensor neural networks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 878:\n",
            "Title: ALVINN, an autonomous land vehicle in a neural network\n",
            "Abstract: The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.\n",
            "----\n",
            "Paper 879:\n",
            "Title: Ad click prediction: a view from the trenches\n",
            "Abstract: Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.\n",
            "----\n",
            "Paper 880:\n",
            "Title: Clipper: A Low-Latency Online Prediction Serving System\n",
            "Abstract: Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. \n",
            "In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.\n",
            "----\n",
            "Paper 881:\n",
            "Title: Rectified Linear Units Improve Restricted Boltzmann Machines\n",
            "Abstract: Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.\n",
            "----\n",
            "Paper 882:\n",
            "Title: Persistence Images: A Stable Vector Representation of Persistent Homology\n",
            "Abstract: Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.\n",
            "----\n",
            "Paper 883:\n",
            "Title: Very Simple Classification Rules Perform Well on Most Commonly Used Datasets\n",
            "Abstract: None\n",
            "----\n",
            "Paper 884:\n",
            "Title: An Introduction to Variational Methods for Graphical Models\n",
            "Abstract: None\n",
            "----\n",
            "Paper 885:\n",
            "Title: Images of Organization\n",
            "Abstract: Preface Part I. An Overview Introduction Part II. Some Images of Organization 2. Mechanization Takes Command: Organizations as Machines Machines, Mechanical Thinking, and the Rise of Bureaucratic Organization The Origins of Mechanistic Organization Classical Management Theory: Designing bureaucratic organizations Scientific Management Strengths and Limitations of the Machine Metaphor 3. Nature Intervenes: Organizations as Organisms Discovering Organizational Needs Recognizing the Importance of Environment: Organizations as Open Systems Contingency Theory: Adapting Organization to Environment The Variety of the Species Contingency Theory: Promoting Organizational Health and Development Natural Selection: The Population-Ecology View of Organizations Organizational Ecology: The Creation of Shared Futures Strengths and Limitations of the Organismic Metaphor 4. Learning and Self-Organization: Organizations as Brains Images of the Brain Organizations as Information Processing Brains Creating Learning Organizations Cybernetics, Learning, and Learning to Learn Can Organizations Learn to Learn? Guidelines for \"Learning Organizations\" Organizations as Holographic Brains Principles of Holographic Design Strengths and Limitations of the Brain Metaphors 5. Creating Social Realty: Organizations as Cultures Culture and Organization Organization as a Cultural Phenomenon Organization and Cultural Context Corporate Cultures and Subcultures Creating Organizational Reality Culture: Rule Following or Enactment? Organization: The enactment of a Shared Reality Strengths and Limitations of the Cultural Metaphor 6. Interests, Conflict, and Power: Organizations as Political Systems Organizations as Systems of Government Organizations as Systems of Political Activity Analyzing Interests Understanding Conflict Exploring Power Managing Pluralist Organizations Strengths and Limitations of the Political Metaphor 7. Exploring Plato's Cave: Organizations as Psychic Prisons The Trap of Favored Ways of Thinking Organization and the Unconscious Organization and Repressed Sexuality Organization and the Patriarchal Family Organization, Death, and Immortality Organization and Anxiety Organization, Dolls, and Teddy Bears Organization, Shadow, and Archetype The Unconscious: A Creative and Destructive Force Strengths and Limitations of the Psychic Prison Metaphor 8. Unfolding Logics of Change: Organization as Flux and Transformation Autopoiesis: Rethinking Relations With the Environment Enactment as a Form of Narcissism: Organizations Interact With Projections of Themselves Identity and Closure: Egocentrism Versus Systemic Wisdom Shifting \"Attractors\": The Logic of Chaos and Complexity Managing in the Midst of Complexity Loops, Not Lines: The Logic of Mutual Causality Contradiction and Crisis: The Logic of Dialectical Change Dialectical Analysis: How Opposing Forces Drive Change The Dialectics of Management Strengths and Limitations of the Flux and Transformation Metaphor 9. The Ugly Face: Organizations as Instruments of Domination Organization as Domination How Organizations Use and Exploit Their Employees Organization, Class, and Control Work Hazards, Occupational Disease, and Industrial Accidents Workaholism and Social and Mental Stress Organizational Politics and the Radicalized Organization Multinationals and the World Economy The Multinationals as World Powers Multinationals: A Record of Exploitation? Strengths and Limitations of the Domination Metaphor Part III. Implications For Practice 10. The Challenge of Metaphor Metaphors Create Ways of Seeing and Shaping Organizational Life Seeing, Thinking, and Acting in New Ways 11. Reading and Shaping Organizational Life The Multicom Case Interpreting Multicom Developing and Detailed Reading and \"Storyline\" Multicom From Another View \"Reading\" and Emergent Intelligence 12. Postscript Bibliographic Notes Introduction The Machine Metaphor The Organismic Metaphor The Brain Metaphor The Culture Metaphor The Political Metaphor The Psychic Prison Metaphor The Flux and Transformation Metaphor The Domination Metaphor The Challenge of Metaphor Reading and Shaping Organizational Life Postscript Bibliography\n",
            "----\n",
            "Paper 886:\n",
            "Title: Support-vector networks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 887:\n",
            "Title: Outlier Detection in High Dimensional Data\n",
            "Abstract: Artificial intelligence (AI) is the science that allows\n",
            "computers to replicate human intelligence in areas such as\n",
            "decision-making, text processing, visual perception. Artificial\n",
            "Intelligence is the broader field that contains several subfields\n",
            "such as machine learning, robotics, and computer vision.\n",
            "Machine Learning is a branch of Artificial Intelligence that\n",
            "allows a machine to learn and improve at a task over time. Deep\n",
            "Learning is a subset of machine learning that makes use of deep\n",
            "artificial neural networks for training. The paper proposed on\n",
            "outlier detection for multivariate high dimensional data for\n",
            "Autoencoder unsupervised model.\n",
            "----\n",
            "Paper 888:\n",
            "Title: Learning logical definitions from relations\n",
            "Abstract: None\n",
            "----\n",
            "Paper 889:\n",
            "Title: A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI\n",
            "Abstract: Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.\n",
            "----\n",
            "Paper 890:\n",
            "Title: Convolutional Pose Machines\n",
            "Abstract: Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.\n",
            "----\n",
            "Paper 891:\n",
            "Title:  Neural Network Methods for Natural Language Processing\n",
            "Abstract: Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.\n",
            "\n",
            "The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.\n",
            "----\n",
            "Paper 892:\n",
            "Title: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\n",
            "Abstract: In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\n",
            "----\n",
            "Paper 893:\n",
            "Title: Semantics derived automatically from language corpora contain human-like biases\n",
            "Abstract: Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.\n",
            "----\n",
            "Paper 894:\n",
            "Title: Poisoning Attacks against Support Vector Machines\n",
            "Abstract: We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. \n",
            " \n",
            "The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.\n",
            "----\n",
            "Paper 895:\n",
            "Title: Transductive Inference for Text Classification using Support Vector Machines\n",
            "Abstract: This paper introduces Transductive Support Vector Machines (TSVMs) for text classi(cid:12)-cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transduc-tive Support Vector Machines take into account a particular test set and try to minimize misclassi(cid:12)cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi(cid:12)cation. These theoretical (cid:12)ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, espe-ciallyfor smalltraining sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e(cid:14)-ciently, handling 10,000 examples and more.\n",
            "----\n",
            "Paper 896:\n",
            "Title: Solving large scale linear prediction problems using stochastic gradient descent algorithms\n",
            "Abstract: Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.\n",
            "----\n",
            "Paper 897:\n",
            "Title: An assessment of support vector machines for land cover classi(cid:142) cation\n",
            "Abstract: . The support vector machine (SVM) is a group of theoreticallysuperior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensionaldata sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classi(cid:142) cations from satellite images. The SVM was compared to three other popular classi(cid:142) ers, including the maximum likelihood classi(cid:142) er (MLC), neural network classi(cid:142) ers (NNC) and decision tree classi(cid:142) ers (DTC). The impacts of kernel con(cid:142)guration on the performance of the SVM and of the selection of training data and input variables on the four classi(cid:142) ers were also evaluated in this experiment.\n",
            "----\n",
            "Paper 898:\n",
            "Title: An empirical evaluation of deep architectures on problems with many factors of variation\n",
            "Abstract: Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.\n",
            "----\n",
            "Paper 899:\n",
            "Title: Pegasos: primal estimated sub-gradient solver for SVM\n",
            "Abstract: We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\\epsilon}$$ is $${\\tilde{O}(1 / \\epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\\Omega(1 / \\epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\\tilde{O}(d/(\\lambda \\epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.\n",
            "----\n",
            "Paper 900:\n",
            "Title: Understanding Data Augmentation for Classification: When to Warp?\n",
            "Abstract: In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.\n",
            "----\n",
            "Paper 901:\n",
            "Title: A Comparison of ARIMA and LSTM in Forecasting Time Series\n",
            "Abstract: Forecasting time series data is an important subject in economics, business, and finance. Traditionally, there are several techniques to effectively forecast the next lag of time series data such as univariate Autoregressive (AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and more notably Autoregressive Integrated Moving Average (ARIMA) with its many variations. In particular, ARIMA model has demonstrated its outperformance in precision and accuracy of predicting the next lags of time series. With the recent advancement in computational power of computers and more importantly development of more advanced machine learning algorithms and approaches such as deep learning, new algorithms are developed to analyze and forecast time series data. The research question investigated in this article is that whether and how the newly developed deep learning-based algorithms for forecasting time series data, such as \"Long Short-Term Memory (LSTM)\", are superior to the traditional algorithms. The empirical studies conducted and reported in this article show that deep learning-based algorithms such as LSTM outperform traditional-based algorithms such as ARIMA model. More specifically, the average reduction in error rates obtained by LSTM was between 84 - 87 percent when compared to ARIMA indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that the number of training times, known as \"epoch\" in deep learning, had no effect on the performance of the trained forecast model and it exhibited a truly random behavior.\n",
            "----\n",
            "Paper 902:\n",
            "Title: Fuzzy support vector machines\n",
            "Abstract: A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).\n",
            "----\n",
            "Paper 903:\n",
            "Title: Learning to Decode Cognitive States from Brain Images\n",
            "Abstract: None\n",
            "----\n",
            "Paper 904:\n",
            "Title: Stock market's price movement prediction with LSTM neural networks\n",
            "Abstract: Predictions on stock market prices are a great challenge due to the fact that it is an immensely complex, chaotic and dynamic environment. There are many studies from various areas aiming to take on that challenge and Machine Learning approaches have been the focus of many of them. There are many examples of Machine Learning algorithms been able to reach satisfactory results when doing that type of prediction. This article studies the usage of LSTM networks on that scenario, to predict future trends of stock prices based on the price history, alongside with technical analysis indicators. For that goal, a prediction model was built, and a series of experiments were executed and theirs results analyzed against a number of metrics to assess if this type of algorithm presents and improvements when compared to other Machine Learning methods and investment strategies. The results that were obtained are promising, getting up to an average of 55.9% of accuracy when predicting if the price of a particular stock is going to go up or not in the near future.\n",
            "----\n",
            "Paper 905:\n",
            "Title: Intrinsic Motivation and Reinforcement Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 906:\n",
            "Title: Unmasking Clever Hans predictors and assessing what machines really learn\n",
            "Abstract: None\n",
            "----\n",
            "Paper 907:\n",
            "Title: Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers\n",
            "Abstract: We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework uniﬁes some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classiﬁers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classiﬁcation learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.\n",
            "----\n",
            "Paper 908:\n",
            "Title: Who should fix this bug?\n",
            "Abstract: Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.\n",
            "----\n",
            "Paper 909:\n",
            "Title: Feature selection based on mutual information\n",
            "Abstract: The application of machine learning models such as support vector machine (SVM) and artificial neural networks (ANN) in predicting reservoir properties has been effective in the recent years when compared with the traditional empirical methods. Despite that the machine learning models suffer a lot in the faces of uncertain data which is common characteristics of well log dataset. The reason for uncertainty in well log dataset includes a missing scale, data interpretation and measurement error problems. Feature Selection aimed at selecting feature subset that is relevant to the predicting property. In this paper a feature selection based on mutual information criterion is proposed, the strong point of this method relies on the choice of threshold based on statistically sound criterion for the typical greedy feedforward method of feature selection. Experimental results indicate that the proposed method is capable of improving the performance of the machine learning models in terms of prediction accuracy and reduction in training time.\n",
            "----\n",
            "Paper 910:\n",
            "Title: Speech Recognition Using Deep Neural Networks: A Systematic Review\n",
            "Abstract: Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.\n",
            "----\n",
            "Paper 911:\n",
            "Title: Automated learning of decision rules for text categorization\n",
            "Abstract: We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.\n",
            "----\n",
            "Paper 912:\n",
            "Title: Transductive Learning via Spectral Graph Partitioning\n",
            "Abstract: We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case.\n",
            "----\n",
            "Paper 913:\n",
            "Title: Lecture Notes in Artificial Intelligence\n",
            "Abstract: LNAI was established in the mid-1980s as a topical subseries of LNCS focusing on artificial intelligence. This subseries is devoted to the publication of state-of-the-art research results in artificial intelligence, at a high level and in both printed and electronic versions making use of the well-established LNCS publication machinery. As with the LNCS mother series, proceedings and postproceedings are at the core of LNAI; however, all other sublines are available for LNAI as well. The topics in LNAI include automated reasoning, automated programming, algorithms, knowledge representation, agent-based systems, intelligent systems, expert systems, machine learning, natural-language processing, machine vision, robotics, search systems, knowledge discovery, data mining, and related programming languages.\n",
            "----\n",
            "Paper 914:\n",
            "Title: The Case against Accuracy Estimation for Comparing Induction Algorithms\n",
            "Abstract: We analyze critically the use of classi cation accuracy to compare classi ers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classi ers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scienti c conclusions.\n",
            "----\n",
            "Paper 915:\n",
            "Title: Linear Discriminant Analysis\n",
            "Abstract: Linear discriminant analysis (LDA) and the related Fisher's linear discriminant are methods used in statistics, pattern recognition and machine learning to find a linear combination of features which characterize or separate two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
            "----\n",
            "Paper 916:\n",
            "Title: Designing neural networks through neuroevolution\n",
            "Abstract: None\n",
            "----\n",
            "Paper 917:\n",
            "Title: Applying Support Vector Machines to Imbalanced Datasets\n",
            "Abstract: None\n",
            "----\n",
            "Paper 918:\n",
            "Title: Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.\n",
            "Abstract: Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.\n",
            "----\n",
            "Paper 919:\n",
            "Title: Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n",
            "Abstract: As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.\n",
            "----\n",
            "Paper 920:\n",
            "Title: Anomaly Detection\n",
            "Abstract: None\n",
            "----\n",
            "Paper 921:\n",
            "Title: The FLUXCOM ensemble of global land-atmosphere energy fluxes\n",
            "Abstract: None\n",
            "----\n",
            "Paper 922:\n",
            "Title: Kernel Methods for Relation Extraction\n",
            "Abstract: We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.\n",
            "----\n",
            "Paper 923:\n",
            "Title: Data mining in bioinformatics using Weka\n",
            "Abstract: UNLABELLED\n",
            "The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.\n",
            "\n",
            "\n",
            "AVAILABILITY\n",
            "http://www.cs.waikato.ac.nz/ml/weka.\n",
            "----\n",
            "Paper 924:\n",
            "Title: Link-Based Classification\n",
            "Abstract: None\n",
            "----\n",
            "Paper 925:\n",
            "Title: A reliable effective terascale linear learning system\n",
            "Abstract: We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.\n",
            "----\n",
            "Paper 926:\n",
            "Title: Identifying Sarcasm in Twitter: A Closer Look\n",
            "Abstract: Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.\n",
            "----\n",
            "Paper 927:\n",
            "Title: Kernel Methods in Computational Biology\n",
            "Abstract: Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology.Following three introductory chapters -- an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology -- the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.\n",
            "----\n",
            "Paper 928:\n",
            "Title: Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis\n",
            "Abstract: The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.\n",
            "----\n",
            "Paper 929:\n",
            "Title: A System for Massively Parallel Hyperparameter Tuning\n",
            "Abstract: Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.\n",
            "----\n",
            "Paper 930:\n",
            "Title: An Introduction to Restricted Boltzmann Machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 931:\n",
            "Title: A connectionist machine for genetic hillclimbing\n",
            "Abstract: None\n",
            "----\n",
            "Paper 932:\n",
            "Title: Network intrusion detection\n",
            "Abstract: None\n",
            "----\n",
            "Paper 933:\n",
            "Title: Use of the Zero-Norm with Linear Models and Kernel Methods\n",
            "Abstract: We explore the use of the so-called zero-norm of the parameters of linear models in learning. Minimization of such a quantity has many uses in a machine learning context: for variable or feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero-norm. The method boils down to implementing a simple modification of vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we investigate which aid our discussion include variable and feature selection on biological microarray data, and multicategory classification.\n",
            "----\n",
            "Paper 934:\n",
            "Title: Learning with Support Vector Machines\n",
            "Abstract: Support Vectors Machines have become a well established tool within machine learning. They work well in practice and have now been used across a wide range of applications from recognizing hand-written digits, to face identification, text categorisation, bioinformatics, and database marketing. In this book we give an introductory overview of this subject. We start with a simple Support Vector Machine for performing binary classification before considering multi-class classification and learning in the presence of noise. We show that this framework can be extended to many other scenarios such as prediction with real-valued outputs, novelty detection and the handling of complex output structures such as parse trees. Finally, we give an overview of the main types of kernels which are used in practice and how to learn and make predictions from multiple types of input data. Table of Contents: Support Vector Machines for Classification / Kernel-based Models / Learning with Kernels\n",
            "----\n",
            "Paper 935:\n",
            "Title: Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data\n",
            "Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent \"popular\" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.\n",
            "----\n",
            "Paper 936:\n",
            "Title: Classification using discriminative restricted Boltzmann machines\n",
            "Abstract: Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.\n",
            "----\n",
            "Paper 937:\n",
            "Title: Semi-Supervised Support Vector Machines\n",
            "Abstract: We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach.\n",
            "----\n",
            "Paper 938:\n",
            "Title: sbi: A toolkit for simulation-based inference\n",
            "Abstract: e Equally contributing authors 1 Computational Neuroengineering, Department of Electrical and Computer Engineering, Technical University of Munich 2 School of Informatics, University of Edinburgh 3 Neural Systems Analysis, Center of Advanced European Studies and Research (caesar), Bonn 4 Model-Driven Machine Learning, Centre for Materials and Coastal Research, Helmholtz-Zentrum Geesthacht 5 Machine Learning in Science, University of Tübingen 6 Empirical Inference, Max Planck Institute for Intelligent Systems, Tübingen DOI: 10.21105/joss.02505\n",
            "----\n",
            "Paper 939:\n",
            "Title: Logical and relational learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 940:\n",
            "Title: Why Are We Using Black Box Models in AI When We Don’t Need To? A Lesson From An Explainable AI Competition\n",
            "Abstract: In 2018, a landmark challenge in artificial intelligence (AI) took place, namely, the Explainable Machine Learning Challenge. The goal of the competition was to create a complicated black box model for the dataset and explain how it worked. One team did not follow the rules. Instead of sending in a black box, they created a model that was fully interpretable. This leads to the question of whether the real world of machine learning is similar to the Explainable Machine Learning Challenge, where black box models are used even when they are not needed. We discuss this team’s thought processes during the competition and their implications, which reach far beyond the competition itself.Keywords: interpretability, explainability, machine learning, finance\n",
            "----\n",
            "Paper 941:\n",
            "Title: Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)\n",
            "Abstract: Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.\n",
            "----\n",
            "Paper 942:\n",
            "Title: The responsibility gap: Ascribing responsibility for the actions of learning automata\n",
            "Abstract: None\n",
            "----\n",
            "Paper 943:\n",
            "Title: Beyond the point cloud: from transductive to semi-supervised learning\n",
            "Abstract: Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.\n",
            "----\n",
            "Paper 944:\n",
            "Title: Incremental learning with support vector machines\n",
            "Abstract: Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.\n",
            "----\n",
            "Paper 945:\n",
            "Title: Enhancing Supervised Learning with Unlabeled Data\n",
            "Abstract: In a wide variety of supervised learning scenarios, there is a small set of labeled data, along with a large pool of unlabeled data. In this thesis, we present a new semi-supervised learning method called co-learning that is designed to use unlabeled data to enhance standard supervised learning algorithms. The idea is that two or more standard supervised learning algorithms can leverage off the fact that they have different representations of the hypotheses and they are likely to detect different patterns in labeled data. We also design an active co-learning strategy to bootstrap our co-leaning procedure when the originally labeled data set is too small to provide accurate confidence estimate for the learned hypotheses. We provide a priority sampling technique as the selection component in our active co-learning method. We evaluate our co-learning algorithms on several datasets from a commonly used data repository in the machine learning community. We also test our co-learning method on text categorization. The contribution of this research is to put forward a new semi-supervised learning approach for learning with a small number of labeled examples, and explore the applicability of our co-learning strategy in real world applications.\n",
            "----\n",
            "Paper 946:\n",
            "Title: Incorporating Diversity in Active Learning with Support Vector Machines\n",
            "Abstract: In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.\n",
            "----\n",
            "Paper 947:\n",
            "Title: Learning Kernel Classifiers: Theory and Algorithms\n",
            "Abstract: From the Publisher: \n",
            "Linear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library.\n",
            "----\n",
            "Paper 948:\n",
            "Title: Support vector learning\n",
            "Abstract: Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology.\n",
            "----\n",
            "Paper 949:\n",
            "Title: Learning to Learn: Introduction and Overview\n",
            "Abstract: None\n",
            "----\n",
            "Paper 950:\n",
            "Title: K-Nearest Neighbors\n",
            "Abstract: None\n",
            "----\n",
            "Paper 951:\n",
            "Title: PRoNTo: Pattern Recognition for Neuroimaging Toolbox\n",
            "Abstract: None\n",
            "----\n",
            "Paper 952:\n",
            "Title: Lifelong Learning Algorithms\n",
            "Abstract: None\n",
            "----\n",
            "Paper 953:\n",
            "Title: A Simple Approach to Ordinal Classification\n",
            "Abstract: None\n",
            "----\n",
            "Paper 954:\n",
            "Title: Incremental Support Vector Learning: Analysis, Implementation and Applications\n",
            "Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network traffic, can be foreseen.\n",
            "----\n",
            "Paper 955:\n",
            "Title: Learning the Kernel with Hyperkernels\n",
            "Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional.We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.\n",
            "----\n",
            "Paper 956:\n",
            "Title: Support vector machines : theory and applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 957:\n",
            "Title: Learning by Transduction\n",
            "Abstract: We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.\n",
            "----\n",
            "Paper 958:\n",
            "Title: Hidden Markov Support Vector Machines\n",
            "Abstract: This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach.\n",
            "----\n",
            "Paper 959:\n",
            "Title: INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES\n",
            "Abstract: Data based machine learning covers a wide range of topics from pattern recognition to function regression and density estimation. Most of the existing methods are based on traditional statistics, which provides conclusion only for the situation where sample size is tending to infinity. So they may not work in practical cases of limited samples. Statistical Learning Theory or SLT is a small sample statistics by Vapnik et al., which concerns mainly the statistic principles when samples are limited, especially the properties of learning procedure in such cases. SLT provides us a new framework for the general learning problem, and a novel powerful learning method called Support Vector Machine or SVM, which can solve small sample learning problems better. It is believed that the study of SLT and SVM is becoming a new hot area in the field of machine learning. This review introduces the basic ideas of SLT and SVM, their major characteristics and some current research trends.\n",
            "----\n",
            "Paper 960:\n",
            "Title: Support Vector Machines Under Adversarial Label Noise\n",
            "Abstract: In adversarial classication tasks like spam ltering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classication performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classication problems, their eectiveness in adversarial classication tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.\n",
            "----\n",
            "Paper 961:\n",
            "Title: Learning in Humans and Machines: Towards an Interdisciplinary Learning Science\n",
            "Abstract: Chapter headings: Towards an Interdisciplinary Learning Science (P. Reimann, H. Spada). A Cognitive Psychological Approach to Learning (S. Vosniadou). Learning to Do and Learning to Understand: A Lesson and a Challenge for Cognitive Modeling (S. Ohlsson). Machine Learning: Case Studies of an Interdisciplinary Approach (W. Emde). Mental and Physical Artifacts in Cognitive Practices (R. Saljo). Learning Theory and Instructional Science (E. De Corte). Knowledge Representation Changes in Humans and Machines (L. Saitta and Task Force 1). Multi-Objective Learning with Multiple Representations (M. Van Someren, P. Reimann). Order Effects in Incremental Learning (P. Langley). Situated Learning and Transfer (H. Gruber et al.). The Evolution of Research on Collaborative Learning (P. Dillenbourg et al.). A Developmental Case Study on Sequential Learning: The Day-Night Cycle (K. Morik, S. Vosniadou). Subject index. Author index.\n",
            "----\n",
            "Paper 962:\n",
            "Title: WEKA: The Waikato Environment for Knowledge Analysis\n",
            "Abstract: WEKA is a workbench designed to aid in the application of machine learning technology to real world data sets, in particular, data sets from New Zealand’s agricultural sector. In order to do this a range of machine learning techniques are presented to the user in such a way as to hide the idiosyncrasies of input and output formats, as well as allow an exploratory approach in applying the technology. The system presented is a component based one that also has application in machine learning research and education.\n",
            "----\n",
            "Paper 963:\n",
            "Title: Optimization Techniques for Semi-Supervised Support Vector Machines\n",
            "Abstract: Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S3VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3VMs algorithms is studied together, under a common experimental setting.\n",
            "----\n",
            "Paper 964:\n",
            "Title: Ensembles of Learning Machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 965:\n",
            "Title: The Bayesian backfitting relevance vector machine\n",
            "Abstract: Traditional non-parametric statistical learning techniques are often computationally attractive, but lack the same generalization and model selection abilities as state-of-the-art Bayesian algorithms which, however, are usually computationally prohibitive. This paper makes several important contributions that allow Bayesian learning to scale to more complex, real-world learning scenarios. Firstly, we show that backfitting --- a traditional non-parametric, yet highly efficient regression tool --- can be derived in a novel formulation within an expectation maximization (EM) framework and thus can finally be given a probabilistic interpretation. Secondly, we show that the general framework of sparse Bayesian learning and in particular the relevance vector machine (RVM), can be derived as a highly efficient algorithm using a Bayesian version of backfitting at its core. As we demonstrate on several regression and classification benchmarks, Bayesian backfitting offers a compelling alternative to current regression methods, especially when the size and dimensionality of the data challenge computational resources.\n",
            "----\n",
            "Paper 966:\n",
            "Title: The Set Covering Machine\n",
            "Abstract: We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a general-purpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection.\n",
            "----\n",
            "Paper 967:\n",
            "Title: Toward Practical Smile Detection\n",
            "Abstract: Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.\n",
            "----\n",
            "Paper 968:\n",
            "Title: Explanation-Based Learning: An Alternative View\n",
            "Abstract: None\n",
            "----\n",
            "Paper 969:\n",
            "Title: On the relation between multi-instance learning and semi-supervised learning\n",
            "Abstract: Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.\n",
            "----\n",
            "Paper 970:\n",
            "Title: Knowledge Discovery with Support Vector Machines\n",
            "Abstract: An easy-to-follow introduction to support vector machines This book provides an in-depth, easy-to-follow introduction to support vector machines drawing only from minimal, carefully motivated technical and mathematical background material. It begins with a cohesive discussion of machine learning and goes on to cover: Knowledge discovery environments Describing data mathematically Linear decision surfaces and functions Perceptron learning Maximum margin classifiers Support vector machines Elements of statistical learning theory Multi-class classification Regression with support vector machines Novelty detection Complemented with hands-on exercises, algorithm descriptions, and data sets, Knowledge Discovery with Support Vector Machines is an invaluable textbook for advanced undergraduate and graduate courses. It is also an excellent tutorial on support vector machines for professionals who are pursuing research in machine learning and related areas.\n",
            "----\n",
            "Paper 971:\n",
            "Title: ISPRS Journal of Photogrammetry and Remote Sensing\n",
            "Abstract: A wide range of methods for analysis of airborne-and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement. © 2010 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by\n",
            "----\n",
            "Paper 972:\n",
            "Title: PyBrain\n",
            "Abstract: PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easyto-use yet still powerful algorithms for machine learning t asks, including a variety of predefined environments and benchmarks to test and compare algorithms . I plemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (m ultidimensional) recurrent neural networks and deep belief networks.\n",
            "----\n",
            "Paper 973:\n",
            "Title: On Robustness Properties of Convex Risk Minimization Methods for Pattern Recognition\n",
            "Abstract: The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have - besides other good properties - also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the influence function of the classifiers and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.\n",
            "----\n",
            "Paper 974:\n",
            "Title: Leave One Out Error, Stability, and Generalization of Voting Combinations of Classifiers\n",
            "Abstract: None\n",
            "----\n",
            "Paper 975:\n",
            "Title: An Introduction to Statistical Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 976:\n",
            "Title: Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data\n",
            "Abstract: The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.\n",
            "----\n",
            "Paper 977:\n",
            "Title: Trends in extreme learning machines: A review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 978:\n",
            "Title: Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond\n",
            "Abstract: Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.\n",
            "----\n",
            "Paper 979:\n",
            "Title: Learning Multiple Layers of Features from Tiny Images\n",
            "Abstract: Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.\n",
            "----\n",
            "Paper 980:\n",
            "Title: What video games have to teach us about learning and literacy\n",
            "Abstract: Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.\n",
            "----\n",
            "Paper 981:\n",
            "Title: Learning Deep Architectures for AI\n",
            "Abstract: Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.\n",
            "----\n",
            "Paper 982:\n",
            "Title: An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods\n",
            "Abstract: This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.\n",
            "----\n",
            "Paper 983:\n",
            "Title: An overview of statistical learning theory\n",
            "Abstract: Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).\n",
            "----\n",
            "Paper 984:\n",
            "Title: An introduction to kernel-based learning algorithms\n",
            "Abstract: This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.\n",
            "----\n",
            "Paper 985:\n",
            "Title: A Learning Algorithm for Boltzmann Machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 986:\n",
            "Title: Semi-Supervised and Unsupervised Extreme Learning Machines\n",
            "Abstract: Extreme learning machines (ELMs) have proven to be efficient and effective learning mechanisms for pattern classification and regression. However, ELMs are primarily applied to supervised learning problems. Only a few existing research papers have used ELMs to explore unlabeled data. In this paper, we extend ELMs for both semi-supervised and unsupervised tasks based on the manifold regularization, thus greatly expanding the applicability of ELMs. The key advantages of the proposed algorithms are as follows: 1) both the semi-supervised ELM (SS-ELM) and the unsupervised ELM (US-ELM) exhibit learning capability and computational efficiency of ELMs; 2) both algorithms naturally handle multiclass classification or multicluster clustering; and 3) both algorithms are inductive and can handle unseen data at test time directly. Moreover, it is shown in this paper that all the supervised, semi-supervised, and unsupervised ELMs can actually be put into a unified framework. This provides new perspectives for understanding the mechanism of random feature mapping, which is the key concept in ELM theory. Empirical study on a wide range of data sets demonstrates that the proposed algorithms are competitive with the state-of-the-art semi-supervised or unsupervised learning algorithms in terms of accuracy and efficiency.\n",
            "----\n",
            "Paper 987:\n",
            "Title: Induction: Processes of Inference, Learning, and Discovery\n",
            "Abstract: Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.\n",
            "----\n",
            "Paper 988:\n",
            "Title: Regularized multi--task learning\n",
            "Abstract: Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.\n",
            "----\n",
            "Paper 989:\n",
            "Title: Learning to classify text using support vector machines - methods, theory and algorithms\n",
            "Abstract: None\n",
            "----\n",
            "Paper 990:\n",
            "Title: Extreme Learning Machines [Trends & Controversies]\n",
            "Abstract: This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In \"Representational Learning with ELMs for Big Data,\" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In \"A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,\" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In \"ELM-Guided Memetic Computation for Vehicle Routing,\" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In \"ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,\" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In \"Combining ELMs with Random Projections,\" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In \"Reduced ELMs for Causal Relation Extraction from Unstructured Text,\" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In \"A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,\" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry for touchless applications without the need for handheld devices. Finally, in \"An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,\" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.\n",
            "----\n",
            "Paper 991:\n",
            "Title: Online learning with kernels\n",
            "Abstract: Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.\n",
            "----\n",
            "Paper 992:\n",
            "Title: Kernel Methods for Deep Learning\n",
            "Abstract: We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.\n",
            "----\n",
            "Paper 993:\n",
            "Title: Learning by Design: Good Video Games as Learning Machines\n",
            "Abstract: This article asks how good video and computer game designers manage to get new players to learn long, complex and difficult games. The short answer is that designers of good games have hit on excellent methods for getting people to learn and to enjoy learning. The longer answer is more complex. Integral to this answer are the good principles of learning built into successful games. The author discusses 13 such principles under the headings of ‘Empowered Learners’, ‘Problem Solving’ and ‘Understanding’ and concludes that the main impediment to implementing these principles in formal education is cost. This, however, is not only (or even so much) monetary cost. It is, importantly, the cost of changing minds about how and where learning is done and of changing one of our most profoundly change-resistant institutions: the school.\n",
            "----\n",
            "Paper 994:\n",
            "Title: Reinforcement Learning with Hierarchies of Machines\n",
            "Abstract: We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and \"behavior-based\" or \"teleo-reactive\" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.\n",
            "----\n",
            "Paper 995:\n",
            "Title: Self-Normalizing Neural Networks\n",
            "Abstract: Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.\n",
            "----\n",
            "Paper 996:\n",
            "Title: Caffe: Convolutional Architecture for Fast Feature Embedding\n",
            "Abstract: Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.\n",
            "----\n",
            "Paper 997:\n",
            "Title: Large Scale Distributed Deep Networks\n",
            "Abstract: Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.\n",
            "----\n",
            "Paper 998:\n",
            "Title: Factorization Machines with libFM\n",
            "Abstract: Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.\n",
            " Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.\n",
            "----\n",
            "Paper 999:\n",
            "Title: Deep Boltzmann Machines\n",
            "Abstract: We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n",
            "----\n",
            "Paper 1000:\n",
            "Title: Support vector machines for classification and regression.\n",
            "Abstract: The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.\n",
            "----\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Rate limit exceeded. Retrying in 40 seconds... (Attempt 1/5)\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Request failed with status code 400: {\"error\":\"Relevance search offset + limit must be < 1000. Consider '/paper/search/bulk' or the Datasets API to retrieve more papers.\"}\n",
            "\n",
            "Rate limit exceeded. Retrying in 80 seconds... (Attempt 1/5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d1286fc09649>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_delay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait before retrying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mretry_delay\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Exponential backoff: increase the wait time after each retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# Define the search query (e.g., \"machine learning\")\n",
        "query = \"machine learning, data science, artificial intelligence, information extraction\"\n",
        "\n",
        "# Define the API endpoint URL\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Set query parameters\n",
        "query_request = {\n",
        "    \"query\": query,  # Search term\n",
        "    \"offset\": 0,  # Starting point for pagination\n",
        "    \"limit\": 100,  # Number of papers to return per request (maximum 100)\n",
        "    \"fields\": \"paperId,title,abstract\"  # Fields to include in the response\n",
        "}\n",
        "\n",
        "headers = {}\n",
        "\n",
        "# List to store the papers\n",
        "papers = []\n",
        "\n",
        "# Counter to track the number of abstracts retrieved\n",
        "total_abstracts = 0\n",
        "max_abstracts = 10000\n",
        "\n",
        "# Retry parameters\n",
        "max_retries = 5  # Maximum number of retries\n",
        "retry_delay = 5  # Initial delay before retrying (in seconds)\n",
        "\n",
        "# Start making requests to the API\n",
        "while total_abstracts < max_abstracts:\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        # Ensure the combined value of offset + limit is within 1000\n",
        "        if query_request[\"offset\"] + query_request[\"limit\"] > 1000:\n",
        "            print(f\"Offset value exceeds the limit of 1000. Stopping at offset {query_request['offset']}.\")\n",
        "            break  # Break the loop if we exceed the limit\n",
        "\n",
        "        response = requests.get(url, params=query_request, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            # Process and collect the papers\n",
        "            for idx, paper in enumerate(response_data['data']):\n",
        "                if total_abstracts < max_abstracts:\n",
        "                    papers.append(paper)\n",
        "                    total_abstracts += 1\n",
        "\n",
        "                    # Print paper title and abstract for each\n",
        "                    print(f\"Paper {total_abstracts}:\")\n",
        "                    print(f\"Title: {paper['title']}\")\n",
        "                    print(f\"Abstract: {paper['abstract']}\")\n",
        "                    print(\"----\")\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # If there are more pages of results, update the offset and continue\n",
        "            if 'next' in response_data and response_data['next']:\n",
        "                query_request['offset'] = response_data['next']  # Update offset for next batch\n",
        "            else:\n",
        "                break  # Exit loop if there are no more results\n",
        "\n",
        "            break  # Successfully retrieved the data, break the retry loop\n",
        "\n",
        "        elif response.status_code == 429:  # Rate limit exceeded\n",
        "            retries += 1\n",
        "            print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
        "            time.sleep(retry_delay)  # Wait before retrying\n",
        "            retry_delay *= 2  # Exponential backoff: increase the wait time after each retry\n",
        "        else:\n",
        "            print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "            break\n",
        "\n",
        "    if retries >= max_retries:\n",
        "        print(\"Max retries reached. Exiting.\")\n",
        "        break\n",
        "\n",
        "# Check if we've collected the desired number of abstracts\n",
        "print(f\"Total papers fetched: {total_abstracts}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlcNpaCNq2Zy",
        "outputId": "4a3db799-99d5-4523-9101-5e3552e8993b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper 1:\n",
            "Title: Human Activities Recognition Using Machine Learning and Artificial Initialization\n",
            "Abstract: Human Activity Recognition (HAR)is an important challenge for applications in various areas such as healthcare, smart environments, and surveillance. In this paper, we propose a machine learning and artificial intelligence-based approach for HAR using wearable sensor data. The proliferation of wearable devices has made it possible to collect a wide range of sensor data, including accelerometer and gyroscope readings, providing valuable insights into human activity. Our proposed approach uses machine learning algorithms, including support vector machines (SVMs), random forests, and artificial neural networks (ANNs), to classify human activities based on sensor data. We explore feature extraction methods that transform raw sensor readings into meaningful representations including time- and frequency-domain features. We also explore the effectiveness of feature selection methods to identify the most discriminatory features for activity recognition. We also use deeplearning techniques suchas Convolutional Neural Network (CNN)and Recurrent Neural Network (RNN) to automatically learn hierarchical representations from sensor for HAR.We are developing a deeplearning architecture tailored to sequential sensor data that captures both the spatial and temporal dependencies inherent in human activity. We evaluate the proposed approach on publicly available datasets covering a variety of human activities, including walking, running, sitting, standing, and other common daily activities. Experimental results demonstrate the effectiveness of our method in accurately recognizing human activities, outperforming baseline approaches, and achieving state-of-the-art performance on HAR tasks. We also compare and analyses various machine learning and deep learning models to review the pros and cons of HAR applications. We also discuss practical considerations such as computational complexity, scalability, and real-time performance, highlighting challenges and opportunities for future research.\n",
            "----\n",
            "Paper 2:\n",
            "Title: Empowering Patient Care: A Novel Patient Drug Recommendation System Using Artificial Intelligence with Modified Learning Strategy\n",
            "Abstract: In the era of personalized medicine, the demand for efficient and accurate drug recommendation systems has escalated. This paper introduces a pioneering approach to enhance patient care through a novel Patient Drug Recommendation System (PDRS). Leveraging the power of machine learning (ML), specifically Cascaded Multi-Layer Perceptrons (MLP), our system aims to provide precise and personalized drug recommendations based on an extensive Drug Review Dataset. The proposed PDRS capitalizes on the inherent complexities of drug reviews, extracting valuable insights and patterns from patient feedback. The cascaded MLP architecture is designed to sequentially process and analyze diverse aspects of drug reviews, capturing nuanced information and correlations that may be overlooked by traditional recommendation systems. Our methodology involves preprocessing the drug review dataset to address noise and ensure data quality. Feature extraction techniques are applied to distill meaningful information, and the cascaded MLP model is trained using a robust set of labeled data. The system's performance is evaluated on various metrics, with a remarkable accuracy of 97%, signifying its efficacy in predicting suitable drug recommendations for patients. The cascaded MLP architecture demonstrates superior adaptability, allowing the system to comprehend intricate relationships between patients, drugs, and their respective effects. The achieved accuracy underscores the potential of our PDRS to revolutionize clinical decision-making, fostering a more personalized and efficient approach to prescribing medications.\n",
            "----\n",
            "Paper 3:\n",
            "Title: Machine learning based techniques for ECG noise removal and feature extraction\n",
            "Abstract: Machine learning (ML) is being applied to all aspects of life with the development of artificial intelligence (AI). This paper explores the application of machine learning technology in electrocardiogram (ECG) analysis to diagnose and classify a patient's current cardiac disease, predict possible future diseases, and provide a personalised treatment plan. However, several challenges have been highlighted. First, individual ECG signals display variability, causing concern about effective diagnosis based on changing ECG data. Second, different diseases can produce similar ECG results, requiring powerful classification algorithms to accurately classify diseases. Finally, using patient information to predict the probability of future heart attacks is critical to developing appropriate prevention and treatment strategies. Overcoming these challenges could revolutionise the field of cardiology. It could enable precise and proactive medical intervention. The study highlights the potential of machine learning to improve cardiovascular care and personalised medicine and emphasises the importance of addressing key challenges to maximise its impact in clinical practice.\n",
            "----\n",
            "Paper 4:\n",
            "Title: Data-Driven Decision Making: Maximizing Insights Through Business Intelligence, Artificial Intelligence and Big Data Analytics\n",
            "Abstract: In today's rapidly evolving digital landscape, the proliferation of data has become ubiquitous, with organizations facing an unprecedented influx of information. As a result, the ability to effectively utilize this wealth of data has emerged as a critical determinant of success. This research paper embarks on a comprehensive exploration of the intricate interplay between Business Intelligence (BI), Artificial Intelligence (AI), and Big Data Analytics, elucidating their synergistic potential in driving data-driven decision-making strategies. BI serves as the cornerstone of data-driven endeavors, facilitating the extraction of historical insights through structured data analysis. Conversely, AI augments decision-making capabilities by leveraging advanced techniques such as machine learning and natural language processing to provide predictive and prescriptive analytics. Moreover, Big Data Analytics emerges as a pivotal enabler, addressing the challenges posed by the voluminous and diverse datasets characteristic of the contemporary data landscape. The integration of BI and AI within the framework of Big Data Analytics represents a paradigm shift, enabling organizations to transcend traditional analytical boundaries and embrace a holistic approach to data-driven decision-making. By amalgamating retrospective analysis with predictive and prescriptive capabilities, this integration empowers organizations to glean insights into not only ’what happened’ but also ’why it happened’ and ’what will happen,’ thereby fostering agility and responsiveness in decision-making processes. Furthermore, the paper delves into the requisite hardware and software resources essential for the successful implementation of BI, AI, and Big Data Analytics initiatives. By delineating the infrastructure prerequisites and elucidating the significance of robust technological frameworks, organizations can navigate the complexities of the data landscape with confidence, fostering a culture of data-driven innovation and steering towards transformative outcomes in today's data-centric milieu.\n",
            "----\n",
            "Paper 5:\n",
            "Title: Human Movement Recognition with Machine Learning Techniques\n",
            "Abstract: Human MR (HMR) use of IAs has generated much attention on their applications on various campuses, including health services, deportations, security, and security situations. This research focuses on creating powerful computations and models for data mining and collecting accurate human information as a source of information obtained from sensors, accelerometers, sensors, and wearable devices. Artificial intelligence techniques including deep learning, support vector machines (SVMs), and irregular plots are used to process and analyze sensor information, extraction, and meaningful aspects to separate the results, such as a fireplace, adjustment, senders, parse, and movements compels. The goal is to create HMR brands trusted for preparation to realize accurate information about constant and precise actions, carry out mass medical surveillance, provide Bienstar personalized customization, and create the highest association between people and computers in an intelligent environment\n",
            "----\n",
            "Paper 6:\n",
            "Title: Study of Machine Learning Techniques in Autism Spectrum Disorder Detection\n",
            "Abstract: ASD is a neurodevelopmental disorder that affects how people communicate, interact, and behave. It is not a single condition, but a spectrum of different symptoms and challenges that can vary from person to person. To diagnose ASD, clinicians and experts use various tests and observations, but these methods can have some drawbacks. They can be subjective, expensive, and time-consuming, and they may not work well for different cultures and regions. There is also no clear biological or genetic marker for ASD. ML is a type of artificial intelligence that can learn from data and make predictions or decisions without being programmed. ML can help with ASD diagnosis in different ways, such as screening, feature extraction, classification, and outcome prediction. Screening is the process of finding out who may have ASD and need more tests. Feature extraction is the process of getting useful information from the data, such as brain scans, speech patterns, or eye movements. Classification is the process of putting people into different groups or categories, such as ASD or non-ASD. Outcome prediction is the process of estimating the future situation or performance of people, such as their thinking skills or well-being. In this paper, we review the latest ML methods for ASD diagnosis and the challenges and limitations of these methods. We also talk about the ethical and social issues of using ML for ASD diagnosis and suggest some ideas for future research.\n",
            "----\n",
            "Paper 7:\n",
            "Title: Evaluation of Machine Learning Performance Based on BERT Data Representation with LSTM Model to Conduct Sentiment Analysis in Indonesian for Predicting Voices of Social Media Users in the 2024 Indonesia Presidential Election\n",
            "Abstract: In this era, the innovation of science and technology has changed rapidly, such as Artificial Intelligence (A.I.) which has helped a lot in human life. Deep learning (DL) as part of A.I. is the development of one of the machine learning models, namely the neural network. With many neural network layers, deep learning models can perform feature extraction and classification processes in a single architecture. This model has proven to perform state-of-the-art machine learning techniques in text classification, pattern recognition, speech, and imagery. Various text classification tasks, including sentiment analysis, have gone beyond AI-based approaches in Deep Learning models. Text data can come from multiple sources, including social media, such as comments on videos on YouTube. Sentiment analysis, one of the opinion mining, is a computational study that analyzes people's opinions from texts. In this research, machine learning performance analysis is carried out on a deep learning method based on BERT data representation with the LSTM method. The implementation of the model uses youtube commentary data on political videos related to the 2024 presidential election in Indonesia; performance analysis is carried out using accuracy, precision, and recall metrics. In this study, the accuracy of the BERT-LSTM model outperformed the BERT model with an accuracy of 0.8783.\n",
            "----\n",
            "Paper 8:\n",
            "Title: RESNET-53 for Extraction of Alzheimer’s Features Using Enhanced Learning Models\n",
            "Abstract: Detecting Alzheimer's disease typically involves a combination of medical and cognitive assessments, neuro imaging, and sometimes genetic testing. Machine learning and artificial intelligence (AI) techniques are being applied to analyze neuro imaging data, genetic information, and clinical records to develop predictive models for Alzheimer's disease risk and early detection. Many AI models, particularly deep learning models, lack interpretability. Understanding how a model reaches a particular diagnosis or prediction can be challenging, which is a concern in the medical field where interpretability and transparency are crucial. CNNs typically learn features directly from data without prior feature engineering. While this is an advantage, it may also limit the exploration of specific features or biomarkers known to be associated with Alzheimer's disease. Medical images often require pre-processing steps, such as normalization, registration, and segmentation, before feeding them into CNNs. The effectiveness of CNNs may depend on the quality and accuracy of these pre-processing steps. The proposed methodology combines both CNN-based feature extraction and integrates adaptive filtering techniques to leverage the strengths of each method. This hybrid approach can lead to improved Alzheimer's disease detection by enhancing image quality and extracting relevant features for diagnosis. The combination of filtering techniques and CNNs allows the network to focus on relevant features while filtering out noise and irrelevant information. The proposed methodology integrates Gaussian filter with bilateral filter to produce an adaptive filter. Bilateral filtering adapts to the local image structure and content. By using it in combination with Gaussian filtering, the model can adaptively filter different regions of the image, optimizing the smoothing and enhancement process based on local features. This can lead to more effective and discriminative feature learning. Using the traditional CNN approaches the feature extraction has got nearly 57.78% accuracy but with the proposed model the accuracy has improved to 94.24%.\n",
            "----\n",
            "Paper 9:\n",
            "Title: A Blackbox Fuzzing Based on Automated State Machine Extraction\n",
            "Abstract: Currently, protocol fuzzing techniques mainly employ two approaches: greybox fuzzing based on mutation and blackbox fuzzing based on generation. Greybox fuzzing techniques use message exchanges between the protocol server and actual clients as seeds, and generate test cases through mutation. Although this approach can provide coverage information of the SUT's code and state space through instrumentation and feedback, its drawback lies in the relatively random mutation strategy, which makes it challenging to validate the SUT's message verification process. This paper addresses this limitation by utilizing artificial intelligence techniques to extract protocol state machines. It aims to overcome the reliance on manual work in blackbox fuzzing based on generation and leverage its advantages to generate more effective fuzzing test cases. The study utilizes Prompt-Learning technology to analyze the semantic information in protocol RFC documents, obtain corresponding intermediate representations, and extract protocol state machines from these representations. Taking the BGP protocol as an experimental subject, the experimental results demonstrate a certain level of accuracy in the obtained protocol state machines and the ability to generate test cases from these state machines, thereby enhancing the automation level of protocol fuzzing.\n",
            "----\n",
            "Paper 10:\n",
            "Title: Measuring shallow-water bathymetric signal strength in lidar point attribute data using machine learning\n",
            "Abstract: ABSTRACT The goal of this work was to evaluate if routinely collected but seldom used airborne lidar metadata – ‘point attribute data’ (PAD) – analyzed using machine learning/artificial intelligence can improve extraction of shallow-water (less than 20 m) bathymetry from lidar point clouds. Extreme gradient boosting (XGB) models relating PAD to an existing bathymetry/not bathymetry classification were fitted and evaluated for four areas near the Florida Keys. The PAD examined include ‘pulse specific’ information such as the return intensity and PAD describing flight path consistency. The R2 values for the XGB models were between 0.34 and 0.74. Global classification accuracies were above 80% although this reflected a sometimes extreme Bathy/NotBathy imbalance that inflated global accuracy. This imbalance was mitigated by employing a probability decision threshold (PDT) that equalizes the true positive (Bathy) and true negative (NotBathy) rates. It was concluded that 1) the strength of the bathymetric signal in the PAD should be sufficient to increase accuracy of density-based lidar point cloud bathymetry extraction methods and 2) ML can successfully model the relationship between the PAD and the Bathy/NotBathy classification. A method is also presented to examine the spatial and feature-space distribution of errors that will facilitate quality assurance and continuous improvement.\n",
            "----\n",
            "Paper 11:\n",
            "Title: Analysis of Various Diabetic Prediction Methods of Machine Learning\n",
            "Abstract: The method that can derive valuable information from rough data is data mining. Prediction analysis is a data mining technique that predicts future possibilities from current knowledge. A range of statistical techniques (including machine learning, predictive modelling and data mining) are included in predictive analytics and statistics (both historical and current) to estimate, predict, and predict future results. There are different steps in the prediction analysis that include pre-processing, extraction of features and classification. This paper is focused on the use of machine learning techniques for diabetic prediction. In this paper, different techniques for diabetic prediction are reviewed based on machine learning.\n",
            "----\n",
            "Paper 12:\n",
            "Title: Use Machine Learning to Predict the Running Time of the Program\n",
            "Abstract: The prediction of program running time can be used to improve scheduling performance of distributed systems. In 2011, Google released a data set documenting the vast amount of information in the Google cluster. However, most of the existing running time prediction models only consider the coarse-grained characteristics of the running environment without considering the influence of the time series data of the running environment on the prediction results. Based on this, this paper innovatively proposes a model to predict the running time of the program, which predicts the future running time through historical information. At the same time, we also propose a new data processing and feature extraction scheme for Google cluster data sets. The results show that our model greatly outperforms the classical model on the Google cluster data set, and the root-mean-square error index of running time under different prediction modes is reduced by more than 60% and 40%, respectively. We hope that the model proposed in this paper can provide new research ideas for cloud computing system design.\n",
            "----\n",
            "Paper 13:\n",
            "Title: Robotic Process Automation of Unstructured Data with Machine Learning\n",
            "Abstract: —In this paper we present our work in progress on building an artiﬁcial intelligence system dedicated to tasks regarding the processing of formal documents used in various kinds of business procedures. The main challenge is to build machine learning (ML) models to improve the quality and efﬁciency of business processes involving image processing, optical character recognition (OCR), text mining and information extraction. In the paper we introduce the research and application ﬁeld, some common techniques used in this area and our preliminary results and conclusions.\n",
            "----\n",
            "Paper 14:\n",
            "Title: Research on Speech Emotion Recognition Technology Based on Machine Learning\n",
            "Abstract: In recent years, with the continuous improvement of computer computing power and the emergence of big data processing, artificial intelligence has become an important research field for researchers. The current research on emotion and consciousness is an important part of artificial intelligence research, and the research on speech emotion recognition technology is receiving more and more attention from researchers. Speech emotion recognition has a very broad application prospects. For example, it can be used for student education to identify students’ emotions in time and make appropriate processing,for intelligent human-computer interaction to discover the speaker’s emotional changes in time. It can also be used for criminal investigation to detect the psychological state of the suspect and assist in polygraph detection.This paper mainly studies the MFCC parameters and KNN algorithm of speech signals, and realizes MFCC feature extraction based on MATLAB, and realizes emotion classification through KNN algorithm. The CASIA corpus was used for training and testing, and finally reached an accuracy of 78.00%.\n",
            "----\n",
            "Paper 15:\n",
            "Title: Application of data-driven feature extraction methods in biometrics\n",
            "Abstract: With the continuous development of research in the field of machine learning, especially the progress in deep learning and the continuous improvement of arithmetic power such as image processors, the recognition technology using biometric big data has gained wide attention and has been well applied in many fields such as human-witness matching, intelligent monitoring and epidemic prevention and control. The development trend of big data biometric identification technology is analyzed, the types of biometric features and the development and application of big data-driven biometric identification technology are summarized, and the future development trend of big data biometric identification technology is discussed.\n",
            "----\n",
            "Paper 16:\n",
            "Title: Soil Pollution Prediction Based on KPCA and Improved Deep Sparse Extreme Learning Machine\n",
            "Abstract: A soil prediction model combining kernel principal component analysis (KPCA) features extraction and optimized deep extreme learning machine(DELM) is proposed to solve the problems of traditional methods for predicting heavy metal content in soil are of low accuracy and soil testing is expensive. First of all, KPCA method is used to extract the effective parameters of soil heavy metals to realize data dimension reduction. Secondly, the input weights and biases of deep extreme learning machine (DELM) with kernel mapping theory are optimized by an improved Mayfly algorithm (MA) based on adaptive weights. Finally, MA-DKELM model is constructed; the data after dimension reduction are used as input to predict the heavy metal content in soil. The results show that the MA-DKELM model has strong prediction stability and can be applied to the prediction of heavy metal content in soil. Compared with PSO-DELM and DELM prediction models, MA-DKELM model has the highest accuracy and the best comprehensive performance in predicting soil heavy metals. MA-DKELM model is practical in predicting heavy metal content in soil.\n",
            "----\n",
            "Paper 17:\n",
            "Title: Optimal Feature Extraction Technique for Sentiment Analysis of Product Reviews for Product Development\n",
            "Abstract: Consumer review sites, social media and micro-blogs carry a wealth of information on the general perspective, experience and feedback that consumers have on products. When there is a high volume of product reviews, it can be challenging to product developers to sift through and make a decision based on consumers’ sentiments. Sentiment Analysis, a branch of Artificial Intelligence, assists in providing data to help businesses understand customers’ desire and track how brands and goods are perceived. When performing Sentiment Analysis, feature extraction, converts raw text input into a machine learning compatible format. A strong feature set is necessary in order to achieve high prediction and object classification accuracy. Identifying an optimal feature set combination is critical for increasing the overall performance of data classification. In this research, we tackle this problem by identifying an optimal feature extraction technique for product review Sentiment Analysis using a feature-level analysis. N-gram, POS and techniques based on the lexicons Stanford CoreNLP, TextBlob, and SentiWordNet in different combinations are examined. Multinomial Naïve Bayes, Lexicon and Multinomial Naïve Bayes + Unsupervised Lexicon ensemble classifiers were modeled for classification of the reviews into positive, neutral and negative classes thereby identifying the optimal feature combination. We explored optimal feature extraction technique based on real product reviews datasets for two products; a car make and model known as “Nissan Sentra” and a mobile phone product known as “Samsung Galaxy A12”. The optimal feature extraction technique for MNB and MNB + Lexicon ensemble classifications was provided by a combination of N-Gram, Part of Speech and TextBlob features while the optimal technique for unsupervised Lexicon was provided by a combination of N-Gram, Part of Speech and VADER.\n",
            "----\n",
            "Paper 18:\n",
            "Title: Introduction to Information Extraction Technology\n",
            "Abstract: introduction to pinch technology linhoffmarch, introduction to information extraction itl nist gov, introduction to sentiment analysis lct master org, geographic information system gis, introduction to sustainable energy nuclear science and, an introduction to data and information openlearn open, international journal of computer science amp information, introduction to modern technology media essay, natural language processing and information extraction, an introduction to petroleum refining and the production, history of information technology introduction to, introduction to mining cincia viva, introduction to pattern recognition and machine learning, basic introduction to information technology tutorial, text mining and analysis sas support, university of jyvskyl department of mathematical, information and communications technology wikipedia, introduction to information extraction technology a, mining equipment technology and services, air sparging technology overview report clu in, openclinical information extraction, information retrieval wikipedia, text mining department of computer science, an introduction to recombinant dna, rule based information extraction is dead long live rule, automate contract analysis in auditing rutgers university, introduction to information extraction dl acm org, introduction to information and communication technology, a gentle introduction to blockchain technology web, partially and sat 14 apr 2018 01 14 00 gmt alveolar ridge,\n",
            "----\n",
            "Paper 19:\n",
            "Title: Transformative Automation: AI in Scientific Literature Reviews\n",
            "Abstract: —This paper investigates the integration of Artificial Intelligence (AI) into systematic literature reviews (SLRs), aiming to address the challenges associated with the manual review process. SLRs, a crucial aspect of scholarly research, often prove time-consuming and prone to errors. In response, this work explores the application of AI techniques, including Natural Language Processing (NLP), machine learning, data mining, and text analytics, to automate various stages of the SLR process. Specifically, we focus on paper identification, information extraction, and data synthesis. The study delves into the roles of NLP and machine learning algorithms in automating the identification of relevant papers based on defined criteria. Researchers now have access to a diverse set of AI-based tools and platforms designed to streamline SLRs, offering automated search, retrieval, text mining, and analysis of relevant publications. The dynamic field of AI-driven SLR automation continues to evolve, with ongoing exploration of new techniques and enhancements to existing algorithms. This shift from manual efforts to automation not only enhances the efficiency and effectiveness of SLRs but also marks a significant advancement in the broader research process.\n",
            "----\n",
            "Paper 20:\n",
            "Title: Optimizing classification models for medical image diagnosis: a comparative analysis on multi-class datasets\n",
            "Abstract: The surge in machine learning (ML) and artificial intelligence has revolutionized medical diagnosis, utilizing data from chest ct-scans, COVID-19, lung cancer, brain tumor, and alzheimer parkinson diseases. However, the intricate nature of medical data necessitates robust classification models. This study compares support vector machine (SVM), naïve Bayes, k-nearest neighbors (K-NN), artificial neural networks (ANN), and stochastic gradient descent on multi-class medical datasets, employing data collection, Canny image segmentation, hu moment feature extraction, and oversampling/under-sampling for data balancing. Classification algorithms are assessed via 5-fold cross-validation for accuracy, precision, recall, and F-measure. Results indicate variable model performance depending on datasets and sampling strategies. SVM, K-NN, ANN, and SGD demonstrate superior performance on specific datasets, achieving accuracies between 0.49 to 0.57. Conversely, naïve Bayes exhibits limitations, achieving precision levels of 0.46 to 0.47 on certain datasets. The efficacy of oversampling and under-sampling techniques in improving classification accuracy varies inconsistently. These findings aid medical practitioners and researchers in selecting suitable models for diagnostic applications.\n",
            "----\n",
            "Paper 21:\n",
            "Title: Fine-Tuning-Powered Plant Disease Detection Based on Cloud-Edge Computing in Smart Farms\n",
            "Abstract: Plants are an indispensable part of agricultural production, and the study of plant disease classification and recognition has an important impact on garden horticulture, environmental protection, crop growth and the continuous development of agricultural economy. Moreover, it plays a very important role in crop production intelligence and crop big data management applications. Traditional machine learning relies too much on expert knowledge, and the form of artificial extraction of plant leaf information cannot meet the recognition of complex diseases. In this paper, we come up with an edge-cloud plant disease classification approach by using fine-tuning to improve the performance continuously. Methods of crop disease identification leverages convolutional neural networks (CNN) that achieves successful performance in various computer vision tasks. The improved leaf recognition model performs the collection, analysis, processing and prediction of heterogeneous data in the process of crop planting. The proposed fine-tunning-improved CNN model is used for searching three common potato diseases in real time. In the cloud, the dataset is deployed to derive the prediction model using CNN for performing fine-tuning in the edge device. Based on the presented approach, the performance of the model is improved and the time consuming decreased by fine-tuning method in the edge environment.\n",
            "----\n",
            "Paper 22:\n",
            "Title: Transfer Learning for Encrypted Malicious Traffic Detection Based on Efficientnet\n",
            "Abstract: With the development of data encryption technology, encrypted traffic has shown an explosive growth trend. More and more malicious network services rely on encryption to evade detection, which brings huge challenges to traditional rule-based traffic classification methods. In recent years, artificial intelligence technology provides feasible ideas to solve this problem. However, conventional machine learning depends on expert experience for manual feature extraction, and the training of deep learning models requires large number of high-quality labeled data, which makes the research of encrypted traffic detection hard to conduct. To address these issues, we propose a transfer learning method based on Efficientnet to detect the encrypted malicious traffic. Efficientnet-B0, a pre-trained model based on the Imagenet dataset, is transferred to an encrypted traffic dataset, extracting features automatically from the original traffic data without expert experience. The experimental results show that our method can achieve 100% detection accuracy and recall rate. Furthermore, our method can also achieve high detection performance with a small amount of training samples.\n",
            "----\n",
            "Paper 23:\n",
            "Title: Hybrid Decision Fusion based Multimodal Ensemble Framework for Cervical Cancer Detection\n",
            "Abstract: Cervical cancer is fourth in the list of cancers that affect women. It has remained the main cause of death for women in developing nations. The cancer is spread through human papilloma virus (HPV), which is sexually transmitted. Pap smear and colposcopy image analysis remain prominent methods of diagnosis. These screening tests require skilled diagnostic experts, a scarce resource in developing countries thus restricting the effectiveness of the cancer detection process in large scale. Machine learning and deep learning are branches of artificial intelligence that are being used increasingly in cancer diagnosis. This study proposes a novel hybrid intelligent system for cervical cancer detection. A hybrid model of feature extraction and feature fusion is proposed for merging the two-state image and clinical data. Subsequently a machine learning ensemble learner is assembled to classify the features. The model performed with a satisfactory accuracy of 96.16%. Our results show that our method outperforms state of the art approaches and archives better, dependable accuracy.\n",
            "----\n",
            "Paper 24:\n",
            "Title: Autonomous driving based on deep neural network\n",
            "Abstract: Deep learning, the critical part of machine learning, has become influential in different fields, including natural language processing, computational biology, and computer vision. In the last decade, there has been a massive surge in computer vision research since its related application is so promising. Many have proposed various methods to fulfill the automation of driving based on deep learning, but, up until now, there is still a gap between the virtual and reality. This paper focuses on its application in autonomous driving. A new framework is proposed to fill that gap using a deep neural network. Specifically, instead of using the raw images captured by cameras to make decisions, semantic segmentation is applied first to get intermediate products that can better connect virtual and reality. Considering the road landscape needs to be mainly treated, the pre-trained model PSPNet is used to process the original image data. Then this data is provided as input to a deep CNN model for feature extraction and prediction. Compared to the traditional method, a semantic segmentation process is added to help extract useful information within an image and is expected to bring some positive effects.\n",
            "----\n",
            "Paper 25:\n",
            "Title: Roadmap on signal processing for next generation measurement systems\n",
            "Abstract: Signal processing is a fundamental component of almost any sensor-enabled system, with a wide range of applications across different scientific disciplines. Time series data, images, and video sequences comprise representative forms of signals that can be enhanced and analysed for information extraction and quantification. The recent advances in artificial intelligence and machine learning are shifting the research attention towards intelligent, data-driven, signal processing. This roadmap presents a critical overview of the state-of-the-art methods and applications aiming to highlight future challenges and research opportunities towards next generation measurement systems. It covers a broad spectrum of topics ranging from basic to industrial research, organized in concise thematic sections that reflect the trends and the impacts of current and future developments per research field. Furthermore, it offers guidance to researchers and funding agencies in identifying new prospects.\n",
            "----\n",
            "Paper 26:\n",
            "Title: Classification of specialities in textual medical reports based on natural language processing and feature selection\n",
            "Abstract: Nowadays, a great deal of detailed information about patients, including disease status, medication history, and side effects, is collected in an electronic format; called an electronic medical record (EMR), and the data serves as a valuable resource for further analysis, diagnosis, and treatment. The huge q uantity of detailed patient information in these medical texts produces a huge challenge in terms of processing this data efficiently, however. Machine learning (ML) algorithms, artificial intelligence techniques, and natural language processing tools can have the potential effect of simplifying unstructured data, which could positively affect medical report analysis. Natural language processing (NLP) has recently made huge advances on a variety of tasks. In this paper, an automatic system was thus produced to classify specialist consultant interactions based on patients’ medical reports. NLP was used as a pre-processing step on a dataset formed of unstructured medical reports. Feature extraction and selection methods were used to convert the textual reports into sets of features and to extract the most effective features to increase classification accuracy and reduce execution time. Various classification methods were then applied (ML perceptron, logistic regression random forest (RF), and linear support vec tor classifier (LSVC)). The highest accuracy (99.39%) was achieved in ML-perceptron classification techniques .\n",
            "----\n",
            "Paper 27:\n",
            "Title: Effect of Feature Engineering Technique for Determining Vegetation Density\n",
            "Abstract: —Vegetation density is one type of information collected from vegetation cover. Vegetation density influences evapotranspiration in terrain, which is essential in assessing how vulnerable peatlands are to fire. The Keetch and Byram Drought Index model, which evaluates peatland fire vulnerability, divides vegetation density into heavily grazed, softly grazed, and un-grazed. Manual approaches for analyzing vegetation density in the field, on the other hand, need a significant amount of resources. Image data acquisition, pre-processing, feature extraction, classification, feature selection, classification, and validation are all computer vision approaches used to solve these problems. Artificial intelligence algorithms and machine learning approaches promise outstanding accuracy in modern computer vision research. However, in the classification process, the impact of feature extraction is critical. Pattern identification at Back Propagation Neural Network (BPNN) is problematic because the feature extraction dimension is excessively complicated. The solution to this problem is using the feature engineering technique to choose the characteristics. This research aims to explore how feature engineering influences the accuracy of results. According to the statistics, implementing the recommended strategy can increase accuracy by 1% and increase kappa by 1.5%. This increase in vegetation density classification accuracy might help detect peatland vulnerability sooner. The novel aspect of this paper is that, after feature extraction, a feature engineering strategy is used in the machine learning classification stage to reduce the number of complex dimensions.\n",
            "----\n",
            "Paper 28:\n",
            "Title: A blind steganalysis-based predictive analytics of numeric image descriptors for digital forensics with Random Forest & SqueezeNet\n",
            "Abstract: Image steganalysis have been a prominent study in digital forensics and the data science use case of artificial intelligence has been widely adopted in conceptual frameworks. In existing studies, deep learners gain prominence for intrusion detection systems while other dissimilar modules are used for feature extraction. Hence, this study rather employs deep learners as image embedding networks aimed at feature extraction for a predictive analytics of image steganalysis. The extracted numeric image descriptors trains three learner algorithms for pattern recognition using a 10 fold cross-validation system. Experimental result indicates the ensemble of Random forest algorithm and SqueezeNet image embedder as the best for steganalysis in digital forensics while the size of the training set turns out to be insignificant for the supervised machine learning study.\n",
            "----\n",
            "Paper 29:\n",
            "Title: Voting Classification Method with PCA and K-Means for Diabetic Prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 30:\n",
            "Title: A DNN-Based Method for Sea Clutter Doppler Parameters Prediction\n",
            "Abstract: With the dramatic development of information technology and rapid growth of computation performances, artificial intelligent techniques have been gradually applied in all aspects of industrial research, especially in radar signal processing. However, deep learning methods utilized in radar sea clutter are just beginning, and related researches on Doppler characteristics of sea clutter remain sparse. In this paper, artificial intelligent research on sea clutter Doppler parameters prediction is developed based on real data. Firstly, classical signal processing methods for sea clutter spectral parameters extraction are introduced. Secondly, a deep neural network model is built to predict sea clutter Doppler parameters. Finally, the raised DNN model is compared to three other classical machine learning models which are widely used in regression prediction. After comprehensive comparisons with other models in different metrics, it can be concluded that DNN model built in this paper achieves better prediction results.\n",
            "----\n",
            "Paper 31:\n",
            "Title: Feature Selection: Filter Methods for Classification of Indian Musical Instruments\n",
            "Abstract: From the last few years research in Musical Information Retrieval (MIR) is increased. Enormous availability of audio content on the internet demands audio information retrieval in less time. Many have explored music genre, singer identification, audio scene analysis, and instrument classification. It is technically challenging to search similar music pieces, singers, or instruments from available audio files as unstructured data. The classifier's performance depends on the features used to train and test the classifier. Hence, it is important not only to feature extraction from audio signal but also to select relevant features from a large set of extracted features, increasing classification accuracy. This empirical study focuses on the filter approach of feature selection using the Correlation Pearson coefficient, InfoGain, Gain Ration and ReliefF methods to classify Indian musical instruments. It checks the effectiveness of selection methods on classification. The performance of selected features is evaluated using a Random Forest classifier. From result, highest classification accuracy 93% has been achieved by the Information Gain method for top selected features i.e. Flatness, Mean, MFCC 4th Coefficient, Spectral Rolloff(85%) and Zero Crossing Rate.\n",
            "----\n",
            "Paper 32:\n",
            "Title: Multi - Class Document Classification: Effective and Systematized\n",
            "Method to Categorize Documents\n",
            "Abstract: A large section of World Wide Web is full of Documents, content; Data, Big data, unformatted data, formatted data, unstructured and unorganized data and we need information infrastructure, which is useful and easily accessible as an when required. This research work is combining approach of Natural Language Processing and Machine Learning for content-based classification of documents. Natural Language Processing is used which will divide the problem of understanding entire document at once into smaller chucks and give us only with useful tokens responsible for Feature Extraction, which is machine learning technique to create Feature Set which helps to train classifier to predict label for new document and place it at appropriate location. Machine Learning subset of Artificial Intelligence is enriched with sophisticated algorithms like Support Vector Machine, K – Nearest Neighbor, Naïve Bayes, which works well with many Indian Languages and Foreign Language content’s for classification. This Model is successful in classifying documents with more than 70% of accuracy for major Indian Languages and more than 80% accuracy for English Language.\n",
            "----\n",
            "Paper 33:\n",
            "Title: Key Success Factors for Successful Implementation of AI Based Segmentation Algorithms in Clinical Radiology Practice\n",
            "Abstract: With the rise of large amounts of patient’s clinical and imaging data, the development of artificial intelligence tools based on machine learning and deep learning capable of performing several tasks such as image classification or regression, organ segmentation or feature extraction, has soared over the past few years [1]. These developments create many opportunities for radiologists and are likely to impact their routine practice in the long run by providing tools that will improve the accuracy and efficiency of diagnosis and prognosis. It will arguably allow radiologists to spend more time on complex problem solving by rebarbative tasks and help grasping more useful information from medical images [2]. Despite a great research interest, many challenges are getting in the way of an efficient, safe and ethical implementation of those tools in radiologists’ daily practice [3]. Nevertheless, not all tasks and modalities of medical AI have reached the same level of maturity nor are they developing at the same pace.\n",
            "----\n",
            "Paper 34:\n",
            "Title: MetaSeer.STEM: Towards Automating Meta-Analyses\n",
            "Abstract: Meta-analysis is a principled statistical approach for summarizing quantitative information reported across studies within a research domain of interest. Although the results of metaanalyses can be highly informative, the process of collecting and coding the data for a meta-analysis is often a laborintensive effort fraught with the potential for human error and idiosyncrasy. This is due to the fact that researchers typically spend weeks poring over published journal articles, technical reports, book chapters and other materials in order to retrieve key data elements that are then manually coded for subsequent analyses (e.g., descriptive statistics, effect sizes, reliability estimates, demographics, and study conditions). In this paper, we propose a machine learning based system developed to support automated extraction of data pertinent to STEM education meta-analyses, including educational and human resource initiatives aimed at improving achievement, literacy and interest in the fields of science, technology, engineering, and mathematics.\n",
            "----\n",
            "Paper 35:\n",
            "Title: Machine learning for cognitive behavioral analysis: datasets, methods, paradigms, and research directions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 36:\n",
            "Title: The Data Science Revolution - How Learning Machines Changed the Way We Work and Do Business\n",
            "Abstract: None\n",
            "----\n",
            "Paper 37:\n",
            "Title: CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions\n",
            "Abstract: Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.\n",
            "----\n",
            "Paper 38:\n",
            "Title: A Topical Survey: Applications of Machine Learning in Medical Issues\n",
            "Abstract: Computational Intelligence methods have replaced almost all real world applications with high accuracy within the given time period. Machine Learning approaches like classification, feature selection, feature extraction have solved many problems of different domain. They use different\n",
            " ML models implemented with suitable ML tool or combination of tools from NN (Neural Network), SVM (Support Vector Machine), DL (Deep Learning), ELM (Extreme Learning Machine). The model is used for training with known data along with ML algorithms (fuzzy logic, genetic algorithm) to optimize\n",
            " the accuracy for different medical issues for example gene expression and image segmentation for information extraction and disease diagnosis, health monitoring, disease treatment. Most of the medical problems are solved using recent advances in AI (Artificial Intelligence) technologies with\n",
            " the biomedical systems development (e.g., Knowledge based Decision Support Systems) and AI technologies with medical informatics science. AI based methods like machine learning algorithms implemented models are increasingly found in real life applications ex. healthcare, natural calamity detection\n",
            " and forecasting. There are the expert systems handled by experts for knowledge gain which is used in decision making applications. The ML models are found in different medical applications like disease diagnosis (ex. cancer prediction, diabetics disease prediction) and for treatment of diseases\n",
            " (ex. in diabetics disease the reduction in mean glucose concentration following intermittent gastric feeds). The feature selection ML method is used for EEG classification for detection of the severity of the disease in heart related diseases and for identification of genes in different disorder\n",
            " like autism disorder. The ML models are found in health record systems. There are other applications of ML approaches found in image segmentation, tissue extraction, image fragmentation for disease diagnosis (ex. lesion detection in breast cancer for malignancy) and then treatment of those\n",
            " diseases. ML models are found in mobile health treatment, treatment of psychology patients, treatment of dumb patients etc. Medical data handling is the vital part of health care systems for the development of AI systems which can again be solved by machine learning approaches. The ML approaches\n",
            " for medical issues have used ensemble methods or combinations of machine learning tools and machine learning algorithms to optimize the result with good accuracy value at a faster rate.\n",
            "----\n",
            "Paper 39:\n",
            "Title: THE MULTIFACETED IMPACT OF STATISTICAL METHODOLOGY AND THEORY IN DATA SCIENCE\n",
            "Abstract: The vast amount of recorded data and the exponential growth of computational power in the last two decades have enabled the extraction and processing of information in unprecedented ways. This has led to the emergence of data science as a bridge between data mining, algorithm design, modeling, machine learning, visualization, and artificial intelligence, in an effort to understand and gain deeper insights from data in various forms. In turn, this has caused a resurgence of statistical science, due to the prominent role of statistical methodology and statistical theory in data science. This special issue of Mathematics for Applications features a compilation of contributions dealing with the multifaceted impact of statistical methodology and theory in data science. The works featured in this special issue have initially been reported at the conferences UP-STAT 2016 “Data Science, Statistical Practice, and Education” and UP-STAT 2017 “Data Science, Statistics, and the Environment.” After a careful selection and a rigorous review process, seven substantially extended papers out of over 100 works presented at both conferences have been selected for publication in this issue. These works contribute to the theoretical foundations of data science and explore mathematical models and methods for solving problems in applied fields. One of the most prominent ways in which statistical science has enriched data science is through the introduction of regularization methods, such as ridge regression and LASSO, which are used to handle ultra-high dimensional or multicollinear datasets. In the first paper of this special issue, the authors Y. Zhang, J. Thakar, D.J. Topham, A.R. Falsey, D. Zeng and X. Qiu construct two useful equivalence relationships for regularized regression – one for efficiently fitting the concurrent functional regression model, and the second for efficiently solving weighted principal component regression. The development of ensemble learning methods has complemented traditional model selection and regularization approaches, giving experimenters and end-users a rich arsenal of powerful statistical learning methods. Spearheaded by techniques such as random forest, bagging, adaptive boosting, gradient boosting, and random subspace learning, ensemble learning methods have proven valuable in the study and analysis of increasingly larger and more complex data sets. In the second paper, M. Elshrif and E. Fokoué present a novel adaptation of the random subspace learning approach to regression analysis and classification of high-dimension, lowsample-size data.\n",
            "----\n",
            "Paper 40:\n",
            "Title: Amazon Employees Resources Access Data Extraction via Clonal Selection Algorithm and Logic Mining Approach\n",
            "Abstract: Amazon.com Inc. seeks alternative ways to improve manual transactions system of granting employees resources access in the field of data science. The work constructs a modified Artificial Neural Network (ANN) by incorporating a Discrete Hopfield Neural Network (DHNN) and Clonal Selection Algorithm (CSA) with 3-Satisfiability (3-SAT) logic to initiate an Artificial Intelligence (AI) model that executes optimization tasks for industrial data. The selection of 3-SAT logic is vital in data mining to represent entries of Amazon Employees Resources Access (AERA) via information theory. The proposed model employs CSA to improve the learning phase of DHNN by capitalizing features of CSA such as hypermutation and cloning process. This resulting the formation of the proposed model, as an alternative machine learning model to identify factors that should be prioritized in the approval of employees resources applications. Subsequently, reverse analysis method (SATRA) is integrated into our proposed model to extract the relationship of AERA entries based on logical representation. The study will be presented by implementing simulated, benchmark and AERA data sets with multiple performance evaluation metrics. Based on the findings, the proposed model outperformed the other existing methods in AERA data extraction.\n",
            "----\n",
            "Paper 41:\n",
            "Title: Reports of the Workshops of the 32nd AAAI Conference on Artificial Intelligence\n",
            "Abstract: The AAAI-18 workshop program included 15 workshops covering a wide range of topics in AI. Workshops were held Sunday and Monday, February 2–7, 2018, at the Hilton New Orleans Riverside in New Orleans, Louisiana, USA. This report contains summaries of the Affective Content Analysis workshop; the Artificial Intelligence Applied to Assistive Technologies and Smart Environments; the AI and Marketing Science workshop; the Artificial Intelligence for Cyber Security workshop; the AI for Imperfect-Information Games; the Declarative Learning Based Programming workshop; the Engineering Dependable and Secure Machine Learning Systems workshop; the Health Intelligence workshop; the Knowledge Extraction from Games workshop; the Plan, Activity, and Intent Recognition workshop; the Planning and Inference workshop; the Preference Handling workshop; the Reasoning and Learning for Human-Machine Dialogues workshop; and the the AI Enhanced Internet of Things Data Processing for Intelligent Applications workshop.\n",
            "----\n",
            "Paper 42:\n",
            "Title: Exploiting Network Science for Feature Extraction and Representation Learning\n",
            "Abstract: Networks are ubiquitous for many real-world problems such as modeling information diffusion over social networks, transportation systems, understanding protein-proteininteractions, human mobility, computational sustainability, among many others. Recently, due to the ongoing Big Data revolution, the fields of machine learning and Artificial Intelligence (AI) have also become extremely important, with AI mostly being dominated by representation learning techniques such as deep learning. However, research at the intersection of network science, machine learning and AI has been mostly unexplored. Specifically, most of the prior research focuses on how machine learning techniques can be used to solve “network” problems such as predicting information diffusion on social networks or classifying blogger interests in a blog network, etc. On the contrary, in this thesis, we answer the following key question: How canwe exploit network science to improve machine learning and representation learning models when addressing general problems? To answer the above question, we address several problems at the intersection of network science, machine learning, and AI. Specifically, we address four fundamental research challenges: (i) Network Science for Traditional Machine Learning, (ii) Representation Learning for Small-Sample Datasets, (iii) Network Science-BasedDeep Learning Model Compression, and (iv) Network Science for Neural Architecture Space Exploration. In other words, we show that many problems are governed by latent network dynamics which must be incorporated into the machine learning or representation learning models.To this end, we first demonstrate how network science can be used for traditional machine learning problems such as spatiotemporal timeseries prediction and application-specific feature extraction. More precisely, we propose a new framework called Network-of-Dynamic Bayesian Networks (NDBN) to address a complex probabilistic learning problem over networks with known but rapidly changing structure. We also propose a new domain-specific network inference approach when the network structure is unknown and only the high-dimensional data is available. We further introducea new network science-based, application-specific feature extraction method called K-Hop Learning. As concrete case studies, we show that both NDBN framework and K-Hop Learning significantly outperform traditional machine learning techniques for computational sustainability problems such as short-term solar energy and river flow prediction, respectively. We then discuss how network science can be used to address general representationlearning problems with high-dimensional and small-sample datasets. Here, we propose a new network community-based dimensionality reduction framework calledFeatureNet. Our approach is based on a new correlations-based network construction technique that explicitly discovers hidden communities in high-dimensional raw data.We show the effectiveness of FeatureNet on many diverse small-sample problems as deep learning typically overfits for such problems. We demonstrate that our techniqueachieves significantly higher accuracy than ten state-of-the-art dimensionality reduction methods (up to 40% improvement) for the small-sample problems. Since a simple correlations-based network alone cannot capture meaningful features for problems like image classification, we focus on deep learning models like Convolutional Neural Networks (CNN). Indeed, in the era of Internet-of-Things (IoT),computational costs of deep networks have become a critical challenge for deploying such models on resource-constrained edge devices. Towards this, model compressionhas emerged as an important area of research. However, when a computationally expensive CNN (or even a compressed model) cannot fit within the memory-budgetof a single IoT-device, it must be distributed across multiple devices which leads to significant inter-device communication. To alleviate the above problem, we propose a new model compression framework called the Network-of-Neural Networks (NoNN) which first exploits network science to partition a large “teacher” model’s knowledge into disjoint groups and then trains individual “student” models for each group. This results in a set of student moduleswhich satisfy the strict resource-constraints of individual IoT-devices. Extensive experiments on five well-known image classification tasks show that NoNN achieves similar accuracy as the teacher model and significantly outperforms the prior art. We also deploy our proposed framework on real hardware such as Raspberry Pi’s and Odroids to demonstrate that NoNN results in up to 12? reduction in latency, and up to 14? reduction in energy per device with negligible loss of accuracy. Finally, since deep networks are essentially a network of (artificial) neurons, networkscience is a perfect candidate to study their architectural characteristics. Hence, we model deep networks from a network science perspective to identify which architecture level characteristics enable models with different number of parameters and layers to achieve comparable accuracy. To this end, we propose new metrics called NN-Massand NN-Density to study the architecture design space of deep networks. We further theoretically demonstrate that (i) For a given depth and width, CNN architectures withhigher NN-Mass achieve lower generalization error, and (ii) Irrespective of number of parameters and layers (but same width), models with similar NN-Mass yield similar test accuracy. We then present extensive empirical evidence towards the above two theoretical insights by conducting experiments on real image classification tasks suchas CIFAR-10 and CIFAR-100. Lastly, we exploit the latter insight to directly design efficient architectures which achieve comparable accuracy to large models (? 97%on CIFAR-10 dataset) with up to 3? reduction in total parameters. This ultimately reveals how model sizes can be reduced directly from the architecture perspective.In summary, in this thesis, we address several problems at the intersection of network science, machine learning, and representation learning. Our research comprehensivelydemonstrates that network science can not only play a significant role but also lead to excellent results in both machine learning and representation learning.\n",
            "----\n",
            "Paper 43:\n",
            "Title: Reports of the Workshops of the Thirty-First AAAI Conference on Artificial Intelligence\n",
            "Abstract: The AAAI-18 workshop program included 15 workshops covering a wide range of topics in AI. Workshops were held Sunday and Monday, February 2–7, 2018, at the Hilton New Orleans Riverside in New Orleans, Louisiana, USA. This report contains summaries of the Affective Content Analysis workshop; the Artificial Intelligence Applied to Assistive Technologies and Smart Environments; the AI and Marketing Science workshop; the Artificial Intelligence for Cyber Security workshop; the AI for Imperfect-Information Games; the Declarative Learning Based Programming workshop; the Engineering Dependable and Secure Machine Learning Systems workshop; the Health Intelligence workshop; the Knowledge Extraction from Games workshop; the Plan, Activity, and Intent Recognition workshop; the Planning and Inference workshop; the Preference Handling workshop; the Reasoning and Learning for Human-Machine Dialogues workshop; and the the AI Enhanced Internet of Things Data Processing for Intelligent Applications workshop.\n",
            "----\n",
            "Paper 44:\n",
            "Title: A Comparative Study of Motion Feature Recognition under Different Learning Methods\n",
            "Abstract: Abstract The accurate analysis and prediction of human motion posture can provide effective data support for sports training. With the continuous progress of computer science and technology and the gradual maturity of artificial intelligence technology, there is still some controversy about the recognition effect of motion features under different algorithms. Therefore, this paper compares the recognition effects of different learning methods, including two machine learning methods by using k-means classification, such as SVM and a deep learning method CNN. The motion data of three basic movements, such as running, jumping and turning, are collected and their feature information is extracted by sensor equipment. The recognition effect is compared with the overall accuracy rate as the evaluation index. Among them, when k-means classification is used, through feature extraction and selection, the final combination of mean and variance features is obtained and applied to SVM. When CNN is adopted, its performance can be optimised by adjusting its network structure and some super parameters in the convolution layer and pool layer.\n",
            "----\n",
            "Paper 45:\n",
            "Title: The Use of Hyperparameter Tuning in Model Classification: A Scientific Work Area Identification\n",
            "Abstract: This research aims to investigate the effectiveness of hyperparameter tuning, particularly using Optuna, in enhancing the classification performance of machine learning models on scientific work reviews. The study focuses on automating the classification of academic papers into eight distinct fields: decision support systems, information technology, data science, technology education, artificial intelligence, expert systems, image processing, and information systems. The research dataset comprises reviews of scientific papers ranging from 150 to 500 words, collected from the repository of Universitas Putra Indonesia YPTK Padang. The classification process involved the application of the TF-IDF method for feature extraction, followed using various machine learning algorithms including SVM, MNB, KNN, and RF, with and without the integration of SMOTE for data balancing and Optuna for hyperparameter optimization. The results show that combining SMOTE with Optuna significantly improves the accuracy, precision, recall, and F1-score of the models, with the SVM algorithm achieving the highest accuracy at 90%. Additionally, the research explored the effectiveness of ensemble methods, revealing that hard voting combined with SMOTE and Optuna provided substantial improvements in classification performance. These findings underscore the importance of hyperparameter tuning and data balancing in optimizing machine learning models for text classification tasks. The implications of this research are broad, suggesting that the methodologies developed can be applied to various text classification tasks in different domains. Future research should consider exploring other hyperparameter tuning techniques and ensemble methods to further enhance model performance across diverse datasets.\n",
            "----\n",
            "Paper 46:\n",
            "Title: Application of classification trees technique in optimization of parameters of production of film-coated tablets\n",
            "Abstract: It is relatively easy to analyze data obtained in a design of experiments and when it is possible, to describe evaluated system by using the first or second order model. When higher order models are required to describe nonlinearity of the system, or when data are not gathered in a statistical design of experiments it can be practical to apply one of data mining techniques to develop models (Kovačević et al., 2022, Mihajlović et al., 2011). Data mining is a branch of computer science that is involved in untrivial extraction of implicit, previously unknown and potentially useful information from data bases. It combines techniques of machine learning, artificial intelligence, pattern recognition and is also known as data driven discovery. Data mining techniques that can be employed are: regression and classification trees, artificial neural networks, genetic algorithms and fuzzy systems (Maimon and Rokach, 2009). The aim of this work was to apply data mining technique in evaluating effects of input factors on the dissolution profile of film-coated tablets. During the production of development batches, it was observed that after film-coating, dissolution rate increases for some batches and for the others it decreases.\n",
            "----\n",
            "Paper 47:\n",
            "Title: Research on Bioengineering Algorithm Based on Deep Learning Neural Network\n",
            "Abstract: Deep learning (DL) is a fresh study orientation in the field of machine learning in computer science. It is recommend into machine learning to make it nearer to the customary artificial target intelligence. DL is the inherent law and express level of learning sample data, and the information get in the learning procedure is of mighty help to the explain of data such as words, images and sounds. CNN (Convolutional Neural Network) combines feature extraction with itemize process to train neural network, which has acquire mighty successful in the field of image classification. This paper focuses on the automatic classification of fetal facial ultrasound images. A 19-layer convolution network is proposed and improved. By using data enhancement, adding global mean pooling layer, reducing the number of channels in the full connection layer of the model, and optimizing learning based on parameter transfer learning of fine-tuning training, the automatic classification of fetal facial ultrasound images with limited data volume can be realized. Match with the present solutions, the depth network proposed in this paper can effectively avoid ultrasonic noise interference and learn deep features more effectively. A heavy quantity of specific analysis test have proved its effectiveness.\n",
            "----\n",
            "Paper 48:\n",
            "Title: Technological Effectiveness, Clinical Credibility, Data Sources, and WBMS Behavioural Intention\n",
            "Abstract: Patients' treatments are becoming more personalized as healthcare becomes more commodified. Meeting this need requires not just a large allocation of capital, but also a comprehensive application of information, resulting in efforts like electronic health record standards. The quantity of medical data accessible for analytics and data extraction will grow rapidly as these become more mainstream. This is accompanied by an increase in new methods for non-invasive assessment and collection of medically important data in different forms, such as signals and pictures. Despite problems with standardisation and availability, the enormous quantity of data that results is a significant tool for the machine learning industry. Biomedical Computational Intelligence (CI) technologies are already flourishing as a result of getting into this data stream. The legislative session \"Computer science and information Intelligence in Biology and medicine\" at European Symposium on Artificial Neural Networks (ESANN) addresses some of the field's most pressing issues. This paper introduces the theme session by highlighting a few of the submissions and pointing out possibilities and difficulties for CI in biomedicine.\n",
            "----\n",
            "Paper 49:\n",
            "Title: PySio: A New Python Toolbox for Physiological Signal Visualization and Feature Analysis\n",
            "Abstract: In physiological signal analysis, identifying meaningful relationships and inherent patterns in signals can provide valuable information regarding subjects' physiological state and changes. Although MATLAB has been widely used in signal processing and feature analysis, Python has recently dethroned MATLAB with the rise of data science, machine learning and artificial intelligence. Hence, there is a compelling need for a Python package for physiological feature analysis and extraction to achieve compatibility with downstream models often trained in Python. Thus, we present a novel visualization and feature analysis Python toolbox, PySio, to enable rapid, efficient and user-friendly analysis of physiological signals. First, the user should import the signal-of-interest with the corresponding sampling rate. After importing, the user can either analyze the signal as it is, or can choose a specific region for more detailed analysis. PySio enables the user to (i) visualize and analyze the physiological signals (or user-selected segments of the signals) in time domain, (ii) study the signals (or user-selected segments of the signals) in frequency domain through discrete Fourier transform and spectrogram representations, and (iii) investigate and extract the most common time (energy, entropy, zero crossing rate and peaks) and frequency (spectral entropy, rolloff, centroid, spread, peaks and bandpower) domain features, all with one click. Clinical relevance— As the physiological signals originate directly from the underlying physiological events, proper analysis of the signal patterns can provide valuable information in personalized treatment and wearable technology applications.\n",
            "----\n",
            "Paper 50:\n",
            "Title: Learning from medical data streams: an introduction\n",
            "Abstract: Clinical practice and research are facing a new challenge created by the rapid growth of health information science and technology, and the complexity and volume of biomedical data. Machine learning from medical data streams is a recent area of research that aims to provide better knowledge extraction and evidence-based clinical decision support in scenarios where data are produced as a continuous flow. This year's edition of AIME, the Conference on Artificial Intelligence in Medicine, enabled the sound discussion of this area of research, mainly by the inclusion of a dedicated workshop. This paper is an introduction to LEMEDS, the Learning from Medical Data Streams workshop, which highlights the contributed papers, the invited talk and expert panel discussion, as well as related papers accepted to the main conference.\n",
            "----\n",
            "Paper 51:\n",
            "Title: Unimagined Futures – ICT Opportunities and Challenges\n",
            "Abstract: None\n",
            "----\n",
            "Paper 52:\n",
            "Title: Affective Computing and Sentiment Analysis: Emotion, Metaphor and Terminology\n",
            "Abstract: This volume maps the watershed areas between two 'holy grails' of computer science: the identification and interpretation of affect including sentiment and mood. The expression of sentiment and mood involves the use of metaphors, especially in emotive situations. Affect computing is rooted in hermeneutics, philosophy, political science and sociology, and is now a key area of research in computer science. The 24/7 news sites and blogs facilitate the expression and shaping of opinion locally and globally. Sentiment analysis, based on text and data mining, is being used in the looking at news and blogs for purposes as diverse as: brand management, film reviews, financial market analysis and prediction, homeland security. There are systems that learn how sentiments are articulated. This work draws on, and informs, research in fields as varied as artificial intelligence, especially reasoning and machine learning, corpus-based information extraction, linguistics, and psychology.\n",
            "----\n",
            "Paper 53:\n",
            "Title: GeoTerrace-2023-064 The research of the configurations in some locating acoustic system for geospatial modeling in GIS to increase the coordinate accuracy\n",
            "Abstract: SUMMARY The paper is dedicated to the integration of artificial intelligence, machine learning, and deep learning with geospatial science and technology as a key part of spatial analysis, called GeoAI. The embedded applications of GeoAI will allow us to solve the tasks related to classification, clustering and pattern detection, prediction and forecasting, and information extraction not only from sound and meteorological datasets but also from imagery, radar, videos, and unstructured text data. Nowadays it is a very important task for defence sphere in Ukraine. Hostile Artillery Locating Acoustic System has a low target location accuracy in warfare and low ability to identify the targets (among classes or types). To improve the accuracy of the system when processing the sound waves from combat system fire activity is possible using datasets from different functional systems. We analyzed the HALAS configurations for the sound source position prediction.\n",
            "----\n",
            "Paper 54:\n",
            "Title: Affective Computing and Sentiment Analysis: Emotion, Metaphor and Terminology - Volume 45\n",
            "Abstract: This volume maps the watershed areas between two 'holy grails' of computer science: the identification and interpretation of affect including sentiment and mood. The expression of sentiment and mood involves the use of metaphors, especially in emotive situations. Affect computing is rooted in hermeneutics, philosophy, political science and sociology, and is now a key area of research in computer science. The 24/7 news sites and blogs facilitate the expression and shaping of opinion locally and globally. Sentiment analysis, based on text and data mining, is being used in the looking at news and blogs for purposes as diverse as: brand management, film reviews, financial market analysis and prediction, homeland security. There are systems that learn how sentiments are articulated. This work draws on, and informs, research in fields as varied as artificial intelligence, especially reasoning and machine learning, corpus-based information extraction, linguistics, and psychology.\n",
            "----\n",
            "Paper 55:\n",
            "Title: A short review on emotional recognition based on biosignal pattern analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 56:\n",
            "Title: A New Framework for Vehicle Number Plate Recognition using Data Mining Techniques\n",
            "Abstract: Data mining is a field at the intersection of computer science is the process that attempts to discover the patterns in large data sets. One of the key steps in Knowledge Discovery in Databases is to create a suitable target data set for the data mining tasks. It utilizes methods at the intersection of artificial intelligence, machine learning, statistics, networks and database systems. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. The K-Nearest Neighbour algorithm is amongst the simplest of all machine learning algorithms is proposed in this paper. In this proposed approach, the data mining technique is used for edge detection, extraction of plate region, segmentation of plate characters and recognition of characters. Edge is a basic feature of image. The image edges include rich information that is very significant for obtaining the image characteristic by object recognition. In this paper the Modified Sobel edge detection technique is used to detect the edges of the image. With the help of presented technique in this thesis, can detect the number of any plate just by giving as input the image of the plate and number gets extracted and recognized. Here present simplest of all and with lesser complexity to detect the numbers. The image is stored in the form of a matrix and the output is displayed in the form of detected numbers. Experimental Results are carried out in MATLAB and it has been proven that the data mining technique is more efficient and accurate one compared with other techniques.\n",
            "----\n",
            "Paper 57:\n",
            "Title: Geospatial Intelligence Enhancement using Advanced Data Science and Machine Learning: A Systematic Literature Review\n",
            "Abstract: In the era of rapid advancements in artificial intelligence, the geospatial field is experiencing transformative changes. Traditional methods for land cover classification and anomaly detection have often been inconsistent and inaccurate, leading to significant real-world issues such as resource misallocation, unnoticed illegal activities like deforestation, unmonitored topographical changes such as unauthorized constructions, unattended forest fires, and border fence crises, all of which exacerbate climate change and urbanization challenges. This study systematically explores various machine learning (ML) techniques and their application to publicly available geospatial datasets. Specifically, it compares selected Convolutional Neural Networks (CNNs) and other ML models on these datasets to evaluate multiple performance metrics and conduct a comparative analysis. While numerous ML models have been previously employed for land cover classification and anomaly detection, this review seeks to enhance performance metrics and improve classification accuracy. Prior studies have employed techniques such as Random Forest on Sentinel-2 data (Gromny et al., 2019), multiple regression approaches on Landsat data (Wu et al., 2016), and Principal Component Analysis (PCA) on OpenStreetMap data (Feldmeyer et al., 2020). Our study introduces the application of advanced models like VGG16, UNet, and Isolation Forest to geospatial datasets, assessing their impact on enhancing land cover classification and anomaly detection. This research not only aims to achieve higher classification accuracy but also contributes to the field by providing insights into the effectiveness of these models and proposing future directions and opportunities.\n",
            "----\n",
            "Paper 58:\n",
            "Title: Artificial Intelligence for road quality assessment in smart cities: a machine learning approach to acoustic data analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 59:\n",
            "Title: Special Issue on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research\n",
            "Abstract: This Special Issue of the IEEE Transactions on Plasma Science (TPS) follows the first American Physical Society Division of Plasma Physics (APS-DPP) mini-conference on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research held during the 60th APS-DPP Meeting in Portland, OR, USA (November 5–9, 2018). It contains selected highlights from not only the mini-conference but also the broader plasma physics community. Although data science has a long and rich history in plasma physics, dating back at least three decades, it is experiencing a renaissance, thanks in large part to the advances outside of plasma physics. Novel algorithms, hardware, and analytic techniques (buoyed by the open source software ecosystem) have led plasma scientists to explore ways in which the data revolution could accelerate and inform scientific discovery. Emerging data-driven methods could have a transformative effect across the full spectrum of plasma research. For fusion energy research, some areas of opportunities [item 1) in the Appendix] include using machine learning (ML) or data methods for scientific discoveries, augmented instrumentation, accelerated model development and simulations, data-informed intelligent controls of the experiment, and data-enhanced predictions. The DPP mini-conference and the articles herein represent only a tiny cross section of contemporary research on data-driven plasma science. The 3rd International Conference on Data-Driven Plasma Science (ICDDPS-3) will be held in Okinawa, Japan, in April 2020 [item 2) in the Appendix], with expected presentations on fusion plasmas and low-temperature plasmas and beyond. Furthermore, Plasma Science is not unique in its exploration of Scientific Machine Learning: the Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019, Vancouver, BC, Canada, December 2019) and it illustrates a trend in cross disciplinary collaboration with contributions from plasma research.\n",
            "----\n",
            "Paper 60:\n",
            "Title: Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence\n",
            "Abstract: Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.\n",
            "----\n",
            "Paper 61:\n",
            "Title: Data science, artificial intelligence, and machine learning: Opportunities for laboratory medicine and the value of positive regulation.\n",
            "Abstract: None\n",
            "----\n",
            "Paper 62:\n",
            "Title: Machine learning, artificial intelligence, and data science breaking into drug design and neglected diseases\n",
            "Abstract: Machine learning (ML) is becoming capable of transforming biomolecular interaction description and calculation, promising an impact on molecular and drug design, chemical biology, toxicology, among others. The first improvements can be seen from biomolecule structure prediction to chemical synthesis, molecular generation, mechanism of action elucidation, inverse design, polypharmacology, organ or issue targeting of compounds, property and multiobjective optimization. Chemical design proposals from an algorithm may be inventive and feasible. Challenges remain, with the availability, diversity, and quality of data being critical for developing useful ML models; marginal improvement seen in some cases, as well as in the interpretability, validation, and reuse of models. The ultimate aim of ML should be to facilitate options for the scientist to propose and undertake ideas and for these to proceed faster. Applications are ripe for transformative results in understudied, neglected, and rare diseases, where new data and therapies are strongly required. Progress and outlook on these themes are provided in this study.\n",
            "----\n",
            "Paper 63:\n",
            "Title: Data Science and Data Analytics: Artificial Intelligence and Machine Learning Integrated Based Approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 64:\n",
            "Title: Pathways to Artificial General Intelligence: A Brief Overview of Developments and Ethical Issues via Artificial Intelligence, Machine Learning, Deep Learning, and Data Science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 65:\n",
            "Title: Utilizing the Internet of Things (IoT), Artificial Intelligence, Machine Learning, and Vehicle Telematics for Sustainable Growth in Small and Medium Firms (SMEs)\n",
            "Abstract: New technologies like the Internet of Things (IoT), artificial intelligence, machine learning, and vehicle telematics have tremendous potential to improve SMEs business processes, increase efficiency, and reduce costs to obtain a competitive advantage. However, the application of these technologies is also associated with certain difficulties for SMEs to adopt and incorporate them in their business processes due to limited resources, knowledge, and funds. The advancement in technologies such as IoT and the digitization and datafication of physical infrastructure and processes are causing massive shifts across fields. While an increasing number of devices are being connected to the internet and are capturing large volumes of information about operations, users, and the physical environment, new opportunities are arising to leverage that big data for better analytics and automation. The purpose of this paper is to assess how SMEs can apply IoT, AI, machine learning, and vehicle telematics for sustainable development by enhancing business processes, data analysis, predictive maintenance, and efficient supply chain and transportation.\n",
            "----\n",
            "Paper 66:\n",
            "Title: Artificial Intelligence, Machine Learning and Data Science as Iterations of Business Automation for Small Businesses\n",
            "Abstract: Artificial intelligence and machine learning, the two iterations of automation are based on the data, small or large. The larger the data, the more effective an AI or machine learning tool will be. The opposite holds the opposite iteration. With a larger pool of data, the large businesses and multinational corporations have effectively been building, developing and adopting refined AI and machine learning based decision systems. The contention of this chapter is to explore if the small businesses with small data in hands are well-off to use and adopt AI and machine learning based tools for their day to day business operations.\n",
            "----\n",
            "Paper 67:\n",
            "Title: Data-centric artificial intelligence in oncology: a systematic review assessing data quality in machine learning models for head and neck cancer\n",
            "Abstract: None\n",
            "----\n",
            "Paper 68:\n",
            "Title: Artificial Intelligence and Neutrosophic Machine learning in the Diagnosis and Detection of COVID 19\n",
            "Abstract: The world has always suffered and from diseases and epidemics, and the coronavirus is one of the most dangerous viruses that threatened human life that requires the use of all scientific methods and means to respond to it and reduce its spread by early detection of infections and taking necessary measures In view of the significant role that artificial intelligence plays in most fields of science, it has become one of the most important scientific methods used to resolve complex issues and has been harnessed in medical diagnosis, one of the most complex areas. Many AI and machine learning algorithms have been used to diagnose and detect diseases in general and coronavirus in particular. The support vector machine (svm) machine algorithm was one of the most important algorithms in this area and is one of the most effective compilations used in the knowledge extraction process In spite of all this, the results they present remain incomplete because classification issues do not deal with cognitive uncertainties such as ambiguity, neutrality and inconsistency associated with perception of human thinking, This adversely affects the work of a classic support vector machine and affects the accurate diagnosis of the disease To solve this problem, we have done this research using a Neutrosophic Support Vector Machine because it takes into account all possible cases during the study of the sample and it reduces the impact of extreme values. This increases the accuracy of the results when diagnosing coronavirus symptoms. The study was conducted according to the following steps: 1. We extract features from chest radiographs based on GLCM 2. We form a neutrosophic dataset. 3. We train Neutrosophic Support Machine N-SVM on new data. 4. We record the results. Comparing the results, we got using the upgraded N-SVM algorithm with the classic SVM algorithm results we found that it gives a more accurate diagnosis of the disease.\n",
            "----\n",
            "Paper 69:\n",
            "Title: Trends in artificial intelligence, machine learning, and chemometrics applied to chemical data\n",
            "Abstract: Abstract Artificial intelligence‐based methods such as chemometrics, machine learning, and deep learning are promising tools that lead to a clearer and better understanding of data. Only with these tools, data can be used to its full extent, and the gained knowledge on processes, interactions, and characteristics of the sample is maximized. Therefore, scientists are developing data science tools mentioned above to automatically and accurately extract information from data and increase the application possibilities of the respective data in various fields. Accordingly, AI‐based techniques were utilized for chemical data since the 1970s and this review paper focuses on the recent trends of chemometrics, machine learning, and deep learning for chemical and spectroscopic data in 2020. In this regard, inverse modeling, preprocessing methods, and data modeling applied to spectra and image data for various measurement techniques are discussed.\n",
            "----\n",
            "Paper 70:\n",
            "Title: Artificial Intelligence-Driven Corporate Finance: Enhancing Efficiency and Decision-Making Through Machine Learning, Natural Language Processing, and Robotic Process Automation in Corporate Governance and Sustainability\n",
            "Abstract: This research paper delves into the transformative possibilities of Artificial Intelligence (AI) within corporate finance, specifically focusing on its role in improving efficiency and decision-making processes. Through the utilization of machine learning, natural language processing (NLP), and robotic process automation (RPA), AI introduces innovative methods for enhancing corporate governance and sustainability practices. In the contemporary business landscape, corporations encounter mounting pressure to streamline operations while simultaneously addressing concerns regarding environmental, social, and governance (ESG) issues. Conventional finance methodologies often struggle to efficiently handle large volumes of data and extract actionable insights promptly. However, AI presents a shift in paradigm by enabling automated data analysis, recognizing patterns, and conducting predictive modeling, thus enabling finance professionals to make data-informed decisions swiftly and accurately. Machine learning algorithms play a pivotal role in detecting patterns and correlations within financial data, facilitating proactive risk management and strategic planning. Additionally, NLP technologies facilitate the extraction of valuable insights from unstructured data sources like regulatory filings, news articles, and social media, thereby enabling informed decision-making in corporate governance and sustainability endeavors. Moreover, RPA simplifies repetitive tasks and workflows, thereby reducing operational expenses and freeing up human resources for more strategic pursuits. Through the automation of routine processes such as data entry, reconciliation, and reporting, RPA enhances operational efficiency and ensures adherence to regulatory standards. Through the adoption of AI technologies, corporations can unlock novel avenues for innovation, optimize resource allocation, and promote sustainable growth within today's dynamic business milieu.\n",
            "----\n",
            "Paper 71:\n",
            "Title: Beyond the Horizon: The Fusion of Data Science and Artificial Intelligence for Unprecedented Insights\n",
            "Abstract: This research explores the transformative synergy arising from the convergence of data science and artificial intelligence (AI), propelling the boundaries of insights to unprecedented levels. As data science and AI realms interlace, a paradigm shift unfolds, transcending conventional analytical approaches. This study delves into the novel methodologies and applications stemming from this fusion, shedding light on how it reshapes the landscape of information discovery. The investigation scrutinizes advanced machine learning algorithms, deep neural networks, and cognitive computing models within the context of data science applications. By intertwining these technologies, the research unveils the emergence of intelligent systems capable of deciphering intricate patterns in vast datasets and generating anticipatory insights that transcend traditional analytical boundaries. The synergy goes beyond predictive prowess, delving into prescriptive analytics, enabling proactive decision-making and strategic foresight. Ethical considerations and interpretability of AI-driven data science models are addressed, emphasizing the need for responsible and transparent deployment in diverse fields. Through case studies and practical examples, this research demonstrates the tangible impact of this fusion on industries such as healthcare, finance, and beyond. Ultimately, \"Beyond the Horizon\" presents a compelling narrative of how data science and AI fusion propels us into an era of unparalleled insights, reshaping our approach to knowledge extraction and decision-making\n",
            "----\n",
            "Paper 72:\n",
            "Title: Explainable Artificial Intelligence and Interpretable Machine Learning for Agricultural Data Analysis\n",
            "Abstract: Arti ﬁ cial intelligence and machine learning have been increasingly applied for prediction in agricultural science. However, many models are typicallyblack boxes,meaning wecannot explainwhat the modelslearnedfrom the data and the reasons behind predictions. To address this issue, I introduce an emerging subdomain of arti ﬁ cial intelligence, explainable arti ﬁ cial intelligence (XAI), and associated toolkits, interpretable machine learning. This study demonstrates the usefulness of several methods by applying them to an openly available dataset. Thedatasetincludes the no-tillage effect oncropyield relative to conventional tillageandsoil, climate,andman-agement variables. Data analysis discovered that no-tillage management can increase maize crop yield where yield in conventional tillage is <5000 kg/ha and the maximum temperature is higher than 32°. These methods are useful to answer (i) which variables are important for prediction in regression/classi ﬁ cation, (ii) which var-iableinteractionsareimportantforprediction,(iii)howimportantvariablesandtheirinteractionsareassociated with the response variable, (iv) what are the reasons underlying a predicted value for a certain instance, and (v) whether different machine learning algorithms offer the same answer to these questions. I argue that the goodness of model ﬁ t is overly evaluated with model performance measures in the current practice, while these questions are unanswered. XAI and interpretable machine learning can enhance trust and explainability in AI. © 2022 The Author. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is\n",
            "----\n",
            "Paper 73:\n",
            "Title: Mathematics in Data Science and Artificial Intelligence\n",
            "Abstract: Mathematics is a discipline that focuses on structure, order, and relation, derived from counting, measuring, and characterizing object shapes. Mathematics is necessary for professions in data science since machine learning algorithms, conducting analyses, and drawing conclusions from data all require it. A key component of data science is math. It can support problem-solving, model performance optimization, and the interpretation of complex data to address business-related queries. The technology known as artificial intelligence (AI) has come to revolutionize many facets of our existence. Mathematics plays a fundamental part in the astounding advances and capabilities of artificial intelligence. Mathematics contains various branches like algebra, geometry, Trigonometry, Calculus, Statistics and Probability. The foundation of mathematics gives artificial intelligence (AI) systems the ability to reason, learn, and make wise judgments. This article examines the relevance and use of mathematics in artificial intelligence. Large-scale data processing, analysis, and interpretation are made possible by machines thanks to mathematics, which forms the foundation of AI models and algorithms. Developing machine learning algorithms requires an understanding of concepts from statistics, probability theory, calculus, and linear algebra. These algorithms recognize patterns, forecast outcomes, and categorize data using mathematical equations and functions.\n",
            "----\n",
            "Paper 74:\n",
            "Title: Big SAR Data Science: Physics based Machine Learning and Artificial Intelligence\n",
            "Abstract: Radar imaging, particularly Synthetic Aperture Radar (SAR) are pioneer technologies in the field of Computational Sensing and Imaging. The challenges of the image formation principles, high data volume and very high acquisition rate stimulated the elaborations of techniques witch today are ubiquitous. SAR technologies have immensely evolved, the state of the art sensors deliver widely different images, and have made considerable progress in spatial and radiometric resolution, target acquisition strategies, imaging modes, or geographical coverage and data rates. Generally imaging sensors generate an isomorphic representation of the observed scene. This is not the case for SAR, the observations are a doppelganger of the scattered field, an indirect signature of the imaged object. This positions the load of SAR image understanding, and the outmost challenge of Big SAR Data Science, as new and particular challenge of Machine Learning (ML) and Artificial Intelligence (AI). The presentation reviews and analyses the new approaches of SAR imaging leveraging the recent advances in physical process based ML and AI methods and signal processing, and leading to Computational Imaging paradigms where intelligence is the analytical component of the end-to-end sensor and Data Science chain design. A particular focus is on the scientific methods of Deep Learning and an information theoretical model of the SAR information extraction process.\n",
            "----\n",
            "Paper 75:\n",
            "Title: Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning\n",
            "Abstract: In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.\n",
            "----\n",
            "Paper 76:\n",
            "Title: Big Data, Artificial Intelligence and Machine Learning: A Transformative Symbiosis in Favour of Financial Technology\n",
            "Abstract: The financial technology revolution is a reality, as the financial world is gradually transforming into a digital domain of high-volume information and high-speed data transformation and processing. The more this transformation takes place, the more consumer and investor behaviour shifts towards a pro-technology attitude of financial services offered by market participants, financial institutions and financial technology companies. This new norm is confirming that information technology is driving innovation for financial technology. In this framework, the value of big data, artificial intelligence and machine learning techniques becomes apparent. The aim of this chapter is multi-fold. Firstly, a multidimensional descriptive analysis is shown to familiarise the reader with the extent of penetration of the above in the financial technology road-map. A short non-technical overview of the methods is then presented. Next, the impact of data analytics and relevant techniques on the evolution of financial technology is explained and discussed along with their applications’ landscape. The chapter also presents a glimpse of the shifting paradigm these techniques bring forward for several fintech related professions, while artificial intelligence and machine learning techniques are tied with the future challenges of AI ethics, regulation technology and the smart data utilisation.\n",
            "----\n",
            "Paper 77:\n",
            "Title: Innovative Changes in Quantity Surveying Practice through BIM, Big Data, Artificial Intelligence and Machine Learning\n",
            "Abstract: Like many construction industry professions, quantity surveying (QS) has been around for many years and has undergone many changes to reflect developments within the wider industry and society. The proliferation of computers into the design process starting from the 1980’s leading up to the rise of Building Information Modelling has particularly led to significant changes in the design and construction landscape. In the UK for instance, the proliferation of BIM and possible demise of traditional Bill of Quantities, with the concurrent rise of smart buildings/cities with exploitation of Big Data (BD), artificial intelligence (AI) and Machine Learning (ML). It implies that QS practices need to reflect on emerging products and services that can promote construction quality and productivity as well as their own professional development. With the decline of traditional QS roles and increased focus on speed of construction, there may be opportunities for different roles for quantity surveyors when dealing with data-driven needs of advanced clients such as BIM managers and Project managers. Additionally, there is need to improve the market value for traditional QS practices when dealing with less innovative clients with less time constraints, which inadvertently contributes to a skills gap which will allow practices to charge more for the traditional services. This study is an exploratory research based on secondary data, which is aim at understanding BIM adoption and related technical advancements that represent innovative and emerging roles for QS professionals to meet the growing demands in the industry. The findings will ignite and support the need for changes in practice, professional education as well as attitudinal behaviour required toward the UK’s Construction 2025 goals.\n",
            "----\n",
            "Paper 78:\n",
            "Title: AI Renaissance, artificial intelligence, information overload, human-computer interaction, decision-making\n",
            "Abstract: Objective: This paper aims to explore the concept of AI as a modern-day Renaissance movement, triggered by the proliferation of the internet and advancements in artificial intelligence technologies. It delves into the transformative impact of AI on human-computer interactions and decision-making processes. \n",
            "Results: O’Leary's (1997) early notion of a Renaissance movement sparked by the internet's ubiquity finds resonance in the emergence of the AI renaissance. AI technologies such as natural language processing, machine learning, heuristic language processing, and neural networks have integrated into intricate networked computing environments. These technologies facilitate the handling, retrieval, and analysis of vast amounts of data available on the World Wide Web. Given the overwhelming volume of data, direct human analysis has become impractical, necessitating AI-driven support for efficient data utilization. In today's competitive and tech-driven landscape, the time available for decision-making has diminished, prompting reliance on intelligent agents and delegating decision-making tasks to these digital surrogates. \n",
            "Conclusions: The contemporary AI renaissance signifies a paradigm shift in human-computer dynamics. The convergence of AI technologies with the internet's vast information landscape has created a symbiotic relationship, redefining traditional computer roles. AI-enabled tools not only manage the deluge of data but also extend decision-making capabilities, optimizing efficiency in an increasingly fast-paced world. This transformative movement transcends conventional computing boundaries and has paved the way for a new era of human-machine interaction.\n",
            "----\n",
            "Paper 79:\n",
            "Title: Cyberattack detection in wireless sensor networks using a hybrid feature reduction technique with AI and machine learning methods\n",
            "Abstract: None\n",
            "----\n",
            "Paper 80:\n",
            "Title: Exploring the Knowledge Attained by Machine Learning on Ion Transport across Polyamide Membranes Using Explainable Artificial Intelligence.\n",
            "Abstract: Recent studies have increasingly applied machine learning (ML) to aid in performance and material design associated with membrane separation. However, whether the knowledge attained by ML with a limited number of available data is enough to capture and validate the fundamental principles of membrane science remains elusive. Herein, we applied explainable artificial intelligence (XAI) to thoroughly investigate the knowledge learned by ML on the mechanisms of ion transport across polyamide reverse osmosis (RO) and nanofiltration (NF) membranes by leveraging 1,585 data from 26 membrane types. The Shapley additive explanation method based on cooperative game theory was used to unveil the influences of various ion and membrane properties on the model predictions. XAI shows that the ML can capture the important roles of size exclusion and electrostatic interaction in regulating membrane separation properly. XAI also identifies that the mechanisms governing ion transport possess different relative importance to cation and anion rejections during RO and NF filtration. Overall, we provide a framework to evaluate the knowledge underlying the ML model prediction and demonstrate that ML is able to learn fundamental mechanisms of ion transport across polyamide membranes, highlighting the importance of elucidating model interpretability for more reliable and explainable ML applications to membrane selection and design.\n",
            "----\n",
            "Paper 81:\n",
            "Title: A Novel Analysis and Detection of Autism Spectrum Disorder in Artificial Intelligence Using Hybrid Machine Learning\n",
            "Abstract: Heart Disease or Cardiovascular Disease refers to the range of heart conditions like cardiac arrest, coronary artery disease. Heart disease can be very well hindered through certain lifestyle changes. There is a significant increase in the mortality rate recently due to the distinctive heart diseases. Machine learning uses mathematical models to work efficiently with the enormous amount of data. It plays a crucial role in medical science in the prediction of distinctive diseases. Cardiologists inspects the heart functionality using electrocardiography, computed tomography. These tests are quite expensive for a common man. Recent times, the life span of a human is guaranteed only with the support of medications. As prevention is better than the cure, machine learning helps to predict the vulnerability of a heart disease with few elemental symptoms and health factors. It is been fed by the basic data of the patients like age and sex. Machine learning helps to predict the vulnerability in advance which provides the cardiologists with great acumen for the adaption of the treatment. Machine learning algorithms have proven to produce reliable and accurate output with the help of the inputs. The algorithms used in the article include K-Nearest Neighbour (KNN) and decision tree classifier which is compared to yield the desired and efficient output.\n",
            "----\n",
            "Paper 82:\n",
            "Title: From Machine Learning to Artificial General Intelligence: A Roadmap and Implications\n",
            "Abstract: The prospect of developing artificial general intelligence (AGI) with the same comprehensive capabilities as the human mind presents humanity both tremendous opportunities and dire risks. This paper explores the potential applications and implications of AGI across diverse domains including science, healthcare, education, security, and the economy. However, realizing AGI's benefits requires proactive alignment of its goals and values with those of humanity through responsible governance. As AGI approaches and possibly surpasses human-level intellectual abilities, we must grapple with complex ethical issues surrounding autonomy, consciousness, and disruptive societal impacts. The exact timeline for achieving AGI remains uncertain, but its emergence will likely stem from the convergence of advanced technologies like big data, neural networks, and quantum computing. Ultimately, the creation of AGI represents humanity's greatest opportunity to profoundly enhance flourishing, as well as our greatest challenge to steer its development toward benevolence rather than catastrophe. With sage preparation and foresight, AGI could usher in an unparalleled era of insight and invention for the betterment of all people. But without adequate safeguards and alignment, its disruptive potential could prove catastrophically destabilizing. This paper argues prudently governing the transition to AGI is essential for harnessing its transformative power to elevate rather than endanger our collective future.\n",
            "----\n",
            "Paper 83:\n",
            "Title: Regulation and ethics in artificial intelligence and machine learning technologies: Where are we now? Who is responsible? Can the information professional play a role?\n",
            "Abstract: Artificial intelligence (AI) and machine learning (ML) technologies are rapidly maturing and proliferating through all public and private sectors. The potential for these technologies to do good and to help us in our everyday lives is immense. But there is a risk that unless managed and controlled AI can also cause us harm. Questions about regulation, what form it takes and who is responsible for governance are only just beginning to be answered. In May 2019, 42 countries came together to support a global governance framework for AI. The Organisation for Economic Co-operation and Development (OECD) Principles on Artificial Intelligence (OECD (2019) OECD principles on AI. Available at: https://www.oecd.org/going-digital/ai/principles/ (accessed 2 March 2020)) saw like-minded democracies of the world commit to common AI values of trust and respect. In Europe, the European Commission’s (EC) new president, Ursula von der Leyen has made calls for a General Data Protection Regulation style. As a first step the EC has published a white paper: ‘On Artificial Intelligence – A European Approach to Excellence and Trust’ (European Commission (2020) Report, Europa, February). In February 2020, the UK government has published a report on ‘Artificial Intelligence in the Public Sector’ (The Committee on Standards in Public Life (2020) Artificial intelligence and public standards. Report, UK Government, February). This article discusses some of the potential threats AI may hold if left unregulated. It provides a brief overview of the regulatory activities for AI worldwide, and in more detail the current UK AI regulatory landscape. Finally, the article looks at the role that the information professional might play in AI and ML.\n",
            "----\n",
            "Paper 84:\n",
            "Title: Self-Building Artificial Intelligence and Machine Learning to Empower Big Data Analytics in Smart Cities\n",
            "Abstract: None\n",
            "----\n",
            "Paper 85:\n",
            "Title: Machine learning in drug design: Use of artificial intelligence to explore the chemical structure–biological activity relationship\n",
            "Abstract: The paper presents a comprehensive overview of the use of artificial intelligence (AI) systems in drug design. Neural networks, which are one of the systems employed in AI, are used to identify chemical structures that can have medical relevance. Successful training of neural networks must be preceded by the acquisition of relevant information about chemical compounds, functional groups, and their possible biological activity. In general, a neural network requires a large set of training data, which must contain information about the chemical structure–biological activity relationship. The data can come from experimental measurements, but can also be generated using appropriate quantum models. In many of the studies presented below, authors showed a significant potential of neural networks to produce generalizations based on even relatively narrow training data. Despite the fact that neural network systems have been known for more than 40 years, it is only recently that they have seen rapid development due to the wider availability of computing power. In recent years, there has been a growing interest in deep learning techniques, bringing network modeling to a new level of abstraction. Deep learning allows combining what seems to be causally distant phenomena and effects, and to associate facts in a way resembling the human mind.\n",
            "----\n",
            "Paper 86:\n",
            "Title: A COMPREHENSIVE REVIEW OF THE IMPACT OF ARTIFICIAL INTELLIGENCE ON MODERN ACCOUNTING PRACTICES AND FINANCIAL REPORTING\n",
            "Abstract: The rapid integration of artificial intelligence (AI) into various industries has catalyzed transformative changes in accounting practices and financial reporting. This comprehensive review explores the multifaceted impact of AI on modern accounting, shedding light on the ways in which advanced technologies are reshaping traditional financial processes. The implementation of AI in accounting has led to increased efficiency and accuracy in routine tasks. Automation of data entry, reconciliation, and routine bookkeeping activities has not only reduced the risk of human errors but has also allowed accountants to redirect their focus towards more strategic and value-added activities. Machine learning algorithms are adept at analyzing vast datasets, identifying patterns, and predicting financial trends, enabling accountants to make more informed decisions. Furthermore, AI has revolutionized the auditing process, enhancing the detection of anomalies and fraudulent activities. Through continuous monitoring and analysis of financial data, AI-powered systems can quickly identify discrepancies, mitigating risks and ensuring the integrity of financial reports. This has profound implications for regulatory compliance and corporate governance, fostering greater transparency and accountability. In the realm of financial reporting, AI has played a pivotal role in improving the quality and timeliness of information. Natural Language Processing (NLP) technologies enable the extraction of valuable insights from unstructured data sources, facilitating the generation of comprehensive and insightful financial reports. This not only accelerates the reporting process but also enhances the communicative value of financial information to stakeholders. Despite the evident benefits, the widespread adoption of AI in accounting brings forth challenges such as ethical considerations, data security, and the need for upskilling the workforce. Ethical concerns regarding bias in AI algorithms and the responsible use of automation in decision-making processes necessitate a thoughtful approach towards AI integration in accounting practices. In conclusion, this review underscores the transformative impact of AI on modern accounting practices and financial reporting. As organizations navigate this technological revolution, a balanced approach that addresses ethical concerns while maximizing the benefits of AI will be crucial for the continued evolution of the accounting profession. \n",
            "Keywords: Impact, Artificial Intelligence, Modern, Accounting, Practices.\n",
            "----\n",
            "Paper 87:\n",
            "Title: Automation and machine learning augmented by large language models in a catalysis study\n",
            "Abstract: Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies. This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge. These innovations have given rise to the development of self-driving labs and significantly accelerated materials research. Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers. This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.\n",
            "----\n",
            "Paper 88:\n",
            "Title: Copyright Law and the Lifecycle of Machine Learning Models\n",
            "Abstract: Machine learning, a subfield of artificial intelligence (AI), relies on large corpora of data as input for learning algorithms, resulting in trained models that can perform a variety of tasks. While data or information are not subject matter within copyright law, almost all materials used to construct corpora for machine learning are protected by copyright law: texts, images, videos, and so on. There are global policy moves to address the copyright implications of machine learning, in particular in the context of so-called “foundation models” that underpin generative AI. This paper takes a step back, exploring empirically three technological settings through detailed case studies. We set out the established industry methodology of a lifecycle of AI (collecting data, organising data, model training, model operation) to arrive at descriptions suitable for legal analysis. This will allow an assessment of the challenges for a harmonisation of rights, exceptions and disclosure under EU copyright law. The three case studies are: 1. Machine learning for scientific purposes, in the context of a study of regional short-term letting markets; 2. Natural Language Processing (NLP), in the context of large language models; 3. Computer vision, in the context of content moderation of images. We find that the nature and quality of data corpora at the input stage is central to the lifecycle of machine learning. Because of the uncertain legal status of data collection and processing, combined with the competitive advantage gained by firms not disclosing technological advances, the inputs of the models deployed are often unknown. Moreover, the “lawful access” requirement of the EU exception for text and data mining may turn the exception into a decision by rightholders to allow machine learning in the context of their decision to allow access. We assess policy interventions at EU level, seeking to clarify the legal status of input data via copyright exceptions, opt-outs or the forced disclosure of copyright materials. We find that the likely result is a fully copyright-licensed environment of machine learning that may have problematic effects for the structure of industry, innovation and scientific research.\n",
            "----\n",
            "Paper 89:\n",
            "Title: Two Proposed Models for Face Recognition: Achieving High Accuracy and Speed with Artificial Intelligence\n",
            "Abstract: In light of the development in computer science and modern technologies, the impersonation crime rate has increased. Consequently, face recognition technology and biometric systems have been employed for security purposes in a variety of applications including human-computer interaction, surveillance systems, etc. Building an advanced sophisticated model to tackle impersonation-related crimes is essential. This study proposes classification Machine Learning (ML) and Deep Learning (DL) models, utilizing Viola-Jones, Linear Discriminant Analysis (LDA), Mutual Information (MI), and Analysis of Variance (ANOVA) techniques. The two proposed facial classification systems are J48 with LDA feature extraction method as input, and a one-dimensional Convolutional Neural Network Hybrid Model (1D-CNNHM). The MUCT database was considered for training and evaluation. The performance, in terms of classification, of the J48 model reached 96.01% accuracy whereas the DL model that merged LDA with MI and ANOVA reached 100% accuracy. Comparing the proposed models with other works reflects that they are performing very well, with high accuracy and low processing time.\n",
            "----\n",
            "Paper 90:\n",
            "Title: Data Science with Semantic Technologies: Application to Information Systems Development\n",
            "Abstract: ABSTRACT Various semantic technologies such as ontologies, machine learning, or artificial intelligence-based are being used today with data science for the purpose of explaining the meaning of data, and making this explanation exploitable by computer processing. Although some quick and brief reports do exist, to the best of our knowledge, the literature lacks a detailed study reporting why, when and how semantic technologies are used with data science. This paper is a theoretical review aiming at providing an insight into data science with semantic technologies. We characterize this research topic through a framework called DS2T helping to understand data science with semantic technologies and giving a comprehensive overview of the field through different, but complementary views. The proposed framework may be used to position research studies integrating semantic technologies with data science, compare them, understand new trends, and identify opportunities and open issues related to a given application domain. Software development processes are used as illustration domain.\n",
            "----\n",
            "Paper 91:\n",
            "Title: Alzheimer Disease Detection using AI with Deep Learning based Features with Development and Validation based on Data Science\n",
            "Abstract: Alzheimer's disease (AD), a neurological condition that worsens over time, affects millions of individuals worldwide. Because of this, effective intervention and therapy depend on early and precise detection. In recent years, encouraging findings have been obtained using data science and artificial intelligence (AI) techniques in the field of medical diagnostics, particularly AD diagnosis. This work seeks to develop an accurate algorithm for diagnosing AD by identifying AI-based traits from neuroimaging and clinical data.The three key steps of the proposed methodology are data preprocessing, feature extraction, and model development and validation. To offer neuroimaging data, such as MRI and PET scans, as well as essential clinical information, a cohort of persons made up of AD patients and healthy controls is obtained. Throughout the preparation stage, the data are normalised, standardised, and quality-checked to ensure accuracy and consistency.The critical role of feature extraction in locating critical patterns and features potentially indicative of AD is critical. Advanced AI techniques like Convolutional Neural Networks and Recurrent Neural Networks are utilised to extract discriminative features from neuroimaging data after subjecting it to feature engineering methods.The retrieved features are then utilised to build a prediction model using state-of-the-art machine learning techniques such as Support Vector Machines (SVM), Random Forests, or Deep Learning architectures. Strict validation methods, such cross-validation and test datasets, are used to evaluate the model's performance in order to ensure generalizability and minimise overfitting.The project's objective is to identify AD with high specificity, sensitivity, and accuracy to support early diagnosis and tailored treatment planning. The results of this research contribute to the body of knowledge on AI-based diagnostics for neurodegenerative diseases and have the potential to significantly impact clinical practises by facilitating early interventions and improving patient outcomes. It is important to take into account the size and heterogeneity of the dataset as well as any prospective improvements and future expansions to the usage of AI in AD detection.\n",
            "----\n",
            "Paper 92:\n",
            "Title: Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network\n",
            "Abstract: None\n",
            "----\n",
            "Paper 93:\n",
            "Title: A Fuzzy Logic Based Machine Learning Tool for Supporting Big Data Business Analytics in Complex Artificial Intelligence Environments\n",
            "Abstract: Business analytics use techniques from data science, data mining, artificial intelligence (especially, machine learning), mathematics and statistics to gain insights and understanding on the performance of business processes. The gained insights and knowledge help driving the business planning. As employees play important roles in the business process, having a tool to classify and predict their wage levels is desirable. Such classification and prediction enables the public or private sector to offer competitive wages for recruiting and retaining employees. In this paper, we present a tool for classifying and predicting wage levels. It incorporates fuzzy logic into a machine-learning tool to support business analytics on big data. Evaluation results show the applicability of our tool for classification and prediction of wages levels in the business world, which in turn supports business analytics in complex artificial intelligence environments.\n",
            "----\n",
            "Paper 94:\n",
            "Title: Importance of Artificial Intelligence and Machine Learning in fighting with COVID-19 Epidemic\n",
            "Abstract: Novel coronavirus known as COVID-19 is spreading continuously with exponential rate in the world and till date we have no any proper treatment to fight and treat corona positive patients. The economy and employment of entire world lay down and collapsed due to COVID-19. As per WHO guidelines the entire world followed some precautionary measures only and therefore there is no cure mechanism and treatment to treat the COVID-19 patients. Entire health community treats the COVID-19 patient symptotically. The spreading momentum of COVID-19 is exponentially but fortunately the death rate of COVID-19 patient is very low. In such situation, Deep Learning (DL), Data Science, Machine Learning (ML) and Artificial Intelligence (AI) play vital role in cope up and deal with the COVID-19 patients. This paper focuses on predictions, challenges and dealing methods to fight COVID-19 patents with AI, ML and data science for lay hold of precautions and discover the vaccine and treatment. This paper also shows that how AI/ML should be engaged researchers, and governments to ensure the most effective responses and actions are taken.\n",
            "----\n",
            "Paper 95:\n",
            "Title: Artificial intelligence and machine learning research: towards digital transformation at a global scale\n",
            "Abstract: None\n",
            "----\n",
            "Paper 96:\n",
            "Title: The Innovation of Ideological and Political Education Integrating Artificial Intelligence Big Data with the Support of Wireless Network\n",
            "Abstract: ABSTRACT Artificial intelligence (AI) and big data profoundly impact people’s way of life and way of thinking, and college ideological and political education (IPE) has gradually entered the era of online education. On account of this, this study designs an online education algorithm based on AI technology to help teachers better understand the situation of students’ online IPE teaching and improve the management of IPE in universities. Firstly, the learning features of students are extracted through the Back Propagation Neural Network (BP) model. This model summarizes the shortcomings of feature extraction in machine learning, and can simultaneously obtain depth information from the signals of multiple sensors, thus increasing the overall algorithm classification accuracy. Secondly, combined with the human behavior recognition model, the status and behavior of students’ IPE teaching can be obtained in real-time from students’ listening devices. Finally, the algorithm’s classification performance is evaluated by experiments and compared with the designed model. The results reveal that the recognition accuracy of the designed classification algorithms for the sample students is 98.59%, 98.99%, 99.21%, 100%, 97.10%, 95.61%, and 100%, respectively. In addition, comparing the algorithm with similar recognition algorithms, its index values of accuracy and precision are 97.83% and 97.82%, respectively, which are better than similar classification algorithms. Finally, through the experimental samples, the accuracy of the human recognition model is tested and compared with other recognition models. The results reveal that the designed model has high recognition accuracy. This study is of great significance for improving teachers’ innovative IPE methods and optimizing the management level of online IPE teaching.\n",
            "----\n",
            "Paper 97:\n",
            "Title: The Significance of Machine Learning and Deep Learning Techniques in Cybersecurity: A Comprehensive Review\n",
            "Abstract: People in the modern era spend most of their lives in virtual environments that offer a range of public and private services and social platforms. Therefore, these environments need to be protected from cyber attackers that can steal data or disrupt systems. Cybersecurity refers to a collection of technical, organizational, and executive means for preventing the unauthorized use or misuse of electronic information and communication systems to ensure the continuity of their work, guarantee the confidentiality and privacy of personal data, and protect consumers from threats and intrusions. Accordingly, this article explores the cybersecurity practices that protect computer systems from attacks, hacking, and data thefts and investigates the role of artificial intelligence in this domain. This article also summarizes the most significant literature that explore the roles and effects of machine learning and deep learning techniques in cybersecurity. Results show that machine learning and deep learning techniques play significant roles in protecting computer systems from unauthorized entry and in controlling system penetration by predicting and understanding the behaviour and traffic of malicious software.\n",
            "----\n",
            "Paper 98:\n",
            "Title: Machine Learning Predictive Analysis of Liquefaction Resistance for Sandy Soils Enhanced by Chemical Injection\n",
            "Abstract: The objective of this study was to investigate the liquefaction resistance of chemically improved sandy soils in a straightforward and accurate manner. Using only the existing experimental databases and artificial intelligence, the goal was to predict the experimental results as supporting information before performing the physical experiments. Emphasis was placed on the significance of data from 20 loading cycles of cyclic undrained triaxial tests to determine the liquefaction resistance and the contribution of each explanatory variable. Different combinations of explanatory variables were considered. Regarding the predictive model, it was observed that a case with the liquefaction resistance ratio as the dependent variable and other parameters as explanatory variables yielded favorable results. In terms of exploring combinations of explanatory variables, it was found advantageous to include all the variables, as doing so consistently resulted in a high coefficient of determination. The inclusion of the liquefaction resistance ratio in the training data was found to improve the predictive accuracy. In addition, the results obtained when using a linear model for the prediction suggested the potential to accurately predict the liquefaction resistance using historical data.\n",
            "----\n",
            "Paper 99:\n",
            "Title: Data preprocessing impact on machine learning algorithm performance\n",
            "Abstract: Abstract The popularity of artificial intelligence applications is on the rise, and they are producing better outcomes in numerous fields of research. However, the effectiveness of these applications relies heavily on the quantity and quality of data used. While the volume of data available has increased significantly in recent years, this does not always lead to better results, as the information content of the data is also important. This study aims to evaluate a new data preprocessing technique called semi-pivoted QR (SPQR) approximation for machine learning. This technique is designed for approximating sparse matrices and acts as a feature selection algorithm. To the best of our knowledge, it has not been previously applied to data preprocessing in machine learning algorithms. The study aims to evaluate the impact of SPQR on the performance of an unsupervised clustering algorithm and compare its results to those obtained using principal component analysis (PCA) as the preprocessing algorithm. The evaluation is conducted on various publicly available datasets. The findings suggest that the SPQR algorithm can produce outcomes comparable to those achieved using PCA without altering the original dataset.\n",
            "----\n",
            "Paper 100:\n",
            "Title: Crop yield forecasting with climate data using PCA and Machine Learning\n",
            "Abstract: Accurately forecasting annual crop production is crucial for countries as it enables them to formulate import and export policies and estimate the economic benefits of their agricultural planning. The crop growth is significantly influenced by weather conditions throughout the year, and climate conditions during different stages of plant development can greatly affect crop yield. The availability of historical climate data has greatly benefited the agricultural sciences and food sector, particularly with the application of Artificial Intelligence methods in big data analysis, enabling the extraction of practical information and actions. The objective of this research is to develop a predictive Machine Learning (ML) model that utilizes climate data from a specific time frame to forecast the wheat yield in the Pelagonia valley, a crucial region for wheat cultivation in North Macedonia. Principal Component Analysis (PCA) was employed as a dimensionality-reduction method to reduce the input data set's dimensionality. A least-squares boosting regression model was implemented as the ML method to estimate wheat yield from climate data. The results indicate a high accuracy of wheat yield prediction, even with limited dataset, on both the training and testing datasets. The study demonstrates the feasibility of utilizing ML methods as complementary to existing models for accurate wheat yield forecasting, offering significant advantages due to the ease of calibrating the ML model parameters.\n",
            "----\n",
            "Paper 101:\n",
            "Title: Studying high-energy nuclear physics with machine learning\n",
            "Abstract: The research paradigm in physics has evolved through three distinct phases: empirical observation and induction, theoretical modeling and deduction and computational numerical analysis and simulation. We are now situated within a novel epoch wherein the scientific research paradigm is increasingly shaped by the preeminence of large-scale data and artificial intelligence, particularly within the realm of AI for science applications. The advent of high-energy colliders coupled with Monte Carlo simulations has given rise to an unprecedented accumulation of data. Nested within this transformative research paradigm, machine learning and artificial intelligence technologies have been extensively harnessed for the analysis of these vast data sets. Within the domain of high-energy nuclear physics, two prevalent machine learning techniques have emerged: Bayesian analysis and deep learning. The former employs comprehensive fitting methodologies that compare extensive data sets against theoretical models, enabling the extraction of critical information pertaining to the initial nuclear structure, parton distributions, the equation of state governing hot and dense nuclear matter, and the transport coefficients of the quark–gluon plasma, among other parameters. Conversely, the latter capitalizes on the unparalleled pattern recognition capabilities of deep learning to discern robust features from high-dimensional raw data, specifically targeting individual physical parameters. This paper elucidates the fundamental principles of machine learning and delineates its potential to augment high-energy nuclear physics research endeavors.\n",
            "----\n",
            "Paper 102:\n",
            "Title: The role of artificial neural network and machine learning in utilizing spatial information\n",
            "Abstract: None\n",
            "----\n",
            "Paper 103:\n",
            "Title: Implementation of computer vision technology based on artificial intelligence for medical image analysis\n",
            "Abstract: As one of the branches of machine learning, the deep learning model combined with artificial intelligence is widely used in the field of computer vision technology, and the image recognition field represented by medical image analysis is also developing. Its advantage is that it does not rely on human annotation, and the computer can recognize and process the feature information omitted by human beings during the model training process, so as to achieve or even exceed the accuracy of human processing. Based on the general lack of explain ability caused by the unknown data processing process in the deep model, the existing solutions mainly include the establishment of internal explain ability, attention mechanism interpretation of specific models, and the interpretation of unknowable models represented by LIME. The way to quantitatively assess interpretability is still being explored, especially in the interpretative assessment of both doctors and patients in medical decision-related models, several scales have been proposed for reference. The current research on the application of artificial intelligence deep learning models in medical imaging generally pays more attention to accuracy rather than explain ability, resulting in the lack of explain ability, and thus hindering the practical clinical application of deep learning models. Therefore, the need to analyze the development of medical image analysis in the field of artificial intelligence and computer vision technology, and how to balance accuracy and interpretability to develop deep learning models that both doctors and patients can trust will become the research focus of the industry in the future.\n",
            "----\n",
            "Paper 104:\n",
            "Title: A Review on Blood Disease Detection using Artificial Intelligence Techniques\n",
            "Abstract: Patient’s data is gathered during medical procedures in order to assist the doctor in making an accurate diagnosis of the patient's health. This information may consist of straightforward symptoms that the subject has noticed, a doctor's preliminary findings, or a thorough test result from a laboratory. Therefore, a doctor is the only one who uses this data for examination, the doctor then determines the ailment using his or her own medical skills. Assessing if a patient has heart illness, diabetes, cancer or any other blood related disease the artificial intelligence has classified numerous disease datasets using various classification algorithms like Machine learning, Deep learning, Data Mining, Automation and so on. The field of computer science known as artificial intelligence focuses on the intelligence of machine and generates the highest possibilities for achievement and accuracy. The primary goal of this article is a survey of AI methods used in medical research to identify blood disease or disorders and find out which artificial intelligence technique is best suited for highest accuracy in blood disease or disorders diagnostics.\n",
            "----\n",
            "Paper 105:\n",
            "Title: A Review on Background and Applications of Machine Learning in Materials Research\n",
            "Abstract: In recent decades, Artificial Intelligence (AI) has garnered considerable interest owing to its potential to facilitate greater levels of automation and speed up overall output. There has been a significant increase in the quantity of training data sets, processing capacity, and deep learning techniques that are all favorable to the widespread use of AI in fields like material science. Attempting to learn anything new by trial and error is a slow and ineffective approach. Therefore, AI, and particularly machine learning, may hasten the process by gleaning rules from information and constructing predictive models. In traditional computational chemistry, human scientists give the formulae, and the computer just crunches the numbers. In this article, we take a look back at the ways in which artificial intelligence has been put to use in the creation of new materials, such as in their design, performance prediction, and synthesis. In these programs, an emphasis is placed on the specifics of AI methodology implementation and the benefits gained over more traditional approaches. The last section elaborates, from both an algorithmic and an infrastructural perspective, where AI is headed in the future.\n",
            "----\n",
            "Paper 106:\n",
            "Title: Data Driven Analysis of Insurance Claims Using Machine Learning Algorithm\n",
            "Abstract: The insurance industry is undergoing major changes with the integration of big data and artificial intelligence technologies. The design and research of an insurance survey claims system based on big data analysis aims to improve the efficiency and accuracy of insurance claims processing. The system uses image recognition, computer vision systems, language recognition, and other AI technologies to analyze case information and accelerate the speed of insurance claims settlement. The system also includes an intelligent customer service feature that uses AI algorithms such as language processing and big data statistical analysis to provide processing suggestions to policyholders. The system implements an individualized insurance service that collects, stores, and analyzes data on policyholders to create personalized insurance products and perform precise marketing. Big data analysis in the insurance personalized service primarily uses association rule analysis, classification and clustering analysis, and change and deviation analysis to improve the service. The one-click reporting function simplifies the reporting process for policyholders, allowing them to report a case from anywhere at any time. The intelligent claims processing feature separates liability in claims cases and deals with non-controversial cases through the use of AI, shortening the processing time and reducing manpower costs. The insurance survey and claim system has undergone five iterations under an agile development model and has achieved the goals of personalized insurance services, one-click reporting, intelligent claims processing, and intelligent customer service. The practical application results demonstrate that the system can improve the efficiency and accuracy of insurance claims processing while also providing policyholders with a more convenient and personalized experience. In conclusion, the design and research of an insurance survey claims system based on big data analysis has the potential to revolutionize the insurance industry and greatly benefit both insurance companies and policyholders\n",
            "----\n",
            "Paper 107:\n",
            "Title: ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling\n",
            "Abstract: Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as inform technology and policymaking for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open-source efforts for reproducibility, resulting in the use of inconsistent or underspecified datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning models for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementation of state-of-the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supplement these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.\n",
            "----\n",
            "Paper 108:\n",
            "Title: Data-Driven Sustainability: Leveraging Big Data and Machine Learning to Build a Greener Future\n",
            "Abstract: Environmental challenges like climate change and resource depletion necessitate sustainable solutions that balance present and future needs. Advanced information technologies offer immense potential for confronting these issues via data-driven intelligence. This paper explores frameworks harnessing big data and machine learning (ML) to promote ecological sustainability across contexts like energy, agriculture, conservation and resilience. First, we review existing literature establishing this domain as an emerging transdisciplinary field. Next, we propose an architectural pipeline encompassing: (i) multi-modal data acquisition from sensors, surveys and satellites; (ii) preprocessing via cleaning, integration and transformation; (iii) application of supervised algorithms for prediction and unsupervised techniques for pattern discovery tailored to sustainability objectives; (iv) cloud-based model operationalization. Through sample use cases on optimizing renewables forecasting, boosting efficiency of infrastructure systems and monitoring ecosystems, we demonstrate analytical versatility. However, challenges around bias, transparency and scale necessitate ethical governance. Nonetheless, prudent development of specialized ML solutions offers sociotechnical instruments for evidence-driven sustainability planning and impactful interventions promoting resilience and welfare globally. This research aims to advance computational sustainability by outlining conceptual foundations, architectures and directions for real-world deployments of artificial intelligence that align with ecological priorities for current and upcoming generations worldwide.\n",
            "----\n",
            "Paper 109:\n",
            "Title: Effective Surrogate Gradient Learning With High-Order Information Bottleneck for Spike-Based Machine Intelligence\n",
            "Abstract: Brain-inspired computing technique presents a promising approach to prompt the rapid development of artificial general intelligence (AGI). As one of the most critical aspects, spiking neural networks (SNNs) have demonstrated superiority for AGI, such as low power consumption. Effective training of SNNs with high generalization ability, high robustness, and low power consumption simultaneously is a significantly challenging problem for the development and success of applications of spike-based machine intelligence. In this research, we present a novel and flexible learning framework termed high-order spike-based information bottleneck (HOSIB) leveraging the surrogate gradient technique. The presented HOSIB framework, including second-order and third-order formation, i.e., second-order information bottleneck (SOIB) and third-order information bottleneck (TOIB), comprehensively explores the common latent architecture and the spike-based intrinsic information and discards the superfluous information in the data, which improves the generalization capability and robustness of SNN models. Specifically, HOSIB relies on the information bottleneck (IB) principle to prompt the sparse spike-based information representation and flexibly balance its exploitation and loss. Extensive classification experiments are conducted to empirically show the promising generalization ability of HOSIB. Furthermore, we apply the SOIB and TOIB algorithms in deep spiking convolutional networks to demonstrate their improvement in robustness with various categories of noise. The experimental results prove the HOSIB framework, especially TOIB, can achieve better generalization ability, robustness and power efficiency in comparison with the current representative studies.\n",
            "----\n",
            "Paper 110:\n",
            "Title: Opportunities of Artificial Intelligence and Machine Learning in the Food Industry\n",
            "Abstract: The food processing and handling industry is the most significant business among the various manufacturing industries in the entire world that subsidize the highest employability. The human workforce plays an essential role in the smooth execution of the production and packaging of food products. Due to the involvement of humans, the food industries are failing to maintain the demand-supply chain and also lacking in food safety. To overcome these issues of food industries, industrial automation is the best possible solution. Automation is completely based on artificial intelligence (AI) or machine learning (ML) or deep learning (DL) algorithms. By using the AI-based system, food production and delivery processes can be efficiently handled and also enhance the operational competence. This article is going to explain the AI applications in the food industry which recommends a huge amount of capital saving with maximizing resource utilization by reducing human error. Artificial intelligence with data science can improve the quality of restaurants, cafes, online delivery food chains, hotels, and food outlets by increasing production utilizing different fitting algorithms for sales prediction. AI could significantly improve packaging, increasing shelf life, a combination of the menu by using AI algorithms, and food safety by making a more transparent supply chain management system. With the help of AI and ML, the future of food industries is completely based on smart farming, robotic farming, and drones.\n",
            "----\n",
            "Paper 111:\n",
            "Title: Radiomics and artificial intelligence in breast imaging: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 112:\n",
            "Title: Addressing the Challenge of Biomedical Data Inequality: An Artificial Intelligence Perspective.\n",
            "Abstract: Artificial intelligence (AI) and other data-driven technologies hold great promise to transform healthcare and confer the predictive power essential to precision medicine. However, the existing biomedical data, which are a vital resource and foundation for developing medical AI models, do not reflect the diversity of the human population. The low representation in biomedical data has become a significant health risk for non-European populations, and the growing application of AI opens a new pathway for this health risk to manifest and amplify. Here we review the current status of biomedical data inequality and present a conceptual framework for understanding its impacts on machine learning. We also discuss the recent advances in algorithmic interventions for mitigating health disparities arising from biomedical data inequality. Finally, we briefly discuss the newly identified disparity in data quality among ethnic groups and its potential impacts on machine learning. Expected final online publication date for the Annual Review of Biomedical Data Science, Volume 6 is August 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\n",
            "----\n",
            "Paper 113:\n",
            "Title: Artificial intelligence in medical science: a review.\n",
            "Abstract: None\n",
            "----\n",
            "Paper 114:\n",
            "Title: Analysis of Medical Image Processing using Machine Learning Applications - A Review\n",
            "Abstract: Intelligent health systems and a range of patient care can\n",
            "benefit from the help of artificial intelligence. In the medical\n",
            "field, artificial intelligence methods from machine learning to\n",
            "deep learning are widely used for patient risk assessment,\n",
            "medication development, and illness diagnosis. To accurately\n",
            "detect illnesses using artificial intelligence approaches, a\n",
            "variety of medical data sources are needed, including\n",
            "computed tomography scans, genomes, mammograms,\n",
            "ultrasound, magnetic resonance imaging, and more.\n",
            "Additionally, artificial intelligence mainly improved the\n",
            "experience of patients in the hospital and expedited the\n",
            "process of getting them ready to continue their recovery at\n",
            "home. This article discusses a thorough analysis of artificial\n",
            "intelligence-based methods for diagnosing a wide range of\n",
            "illnesses, including cancer, diabetes, Alzheimer's disease,\n",
            "chronic heart disease, stroke, cerebrovascular, hypertension,\n",
            "skin, and liver disease. We carried out a thorough analysis that\n",
            "included the medical imaging dataset that was utilized, as well\n",
            "as the feature extraction and classification procedure for\n",
            "making predictions. For the purpose of early prediction of\n",
            "various disease types using artificial intelligence-based\n",
            "methods, articles published up until October 2020 on the Web\n",
            "of Science, Scopus, Google Scholar, PubMed, Excerpta\n",
            "Medical Database, and Psychology Information are chosen\n",
            "based on preferred reporting items for systematic reviews and\n",
            "Meta-Analysis guidelines.\n",
            "\n",
            "----\n",
            "Paper 115:\n",
            "Title: Explainable Artificial Intelligence for Data Science on Customer Churn\n",
            "Abstract: Machine learning, as a tool, has become critical for decision-making mechanisms in the modern world. It has applications in a wide range of areas, including finance, healthcare, justice, and transportation. Unfortunately, machine learning is often considered as a “black box”. As such, recommendations made by machine learning techniques, as well as the reasoning behind those recommendations, are not easily understood by humans. In this paper, we present an explainable artificial intelligence (XAI) solution that integrates and enhances state-of-the-art techniques to produce understandable and practical explanations to end-users. To evaluate the effectiveness of our XAI solution for data science, we conduct a case study on applying our solution to explaining a random forest-based predictive model on customer churn. Results show the practicality and usefulness of our XAI solution in practical applications such as data science on customer churn.\n",
            "----\n",
            "Paper 116:\n",
            "Title: Developing and Implementing Predictive Models in a Learning Healthcare System: Traditional and Artificial Intelligence Approaches in the Veterans Health Administration.\n",
            "Abstract: Predicting clinical risk is an important part of healthcare and can inform decisions about treatments, preventive interventions, and provision of extra services. The field of predictive models has been revolutionized over the past two decades by electronic health record data; the ability to link such data with other demographic, socioeconomic, and geographic information; the availability of high-capacity computing; and new machine learning and artificial intelligence methods for extracting insights from complex datasets. These advances have produced a new generation of computerized predictive models, but debate continues about their development, reporting, validation, evaluation, and implementation. In this review we reflect on more than 10 years of experience at the Veterans Health Administration, the largest integrated healthcare system in the United States, in developing, testing, and implementing such models at scale. We report lessons from the implementation of national risk prediction models and suggest an agenda for research. Expected final online publication date for the Annual Review of Biomedical Data Science, Volume 5 is August 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\n",
            "----\n",
            "Paper 117:\n",
            "Title: Information Extraction from Security-Related Datasets\n",
            "Abstract: There are various approaches to executing security breaches which are nowadays massively occurring in electronic communication environments, and phishing attacks are one of the most applied ones. A vast majority of phishing attacks are initiated using electronic messages, which attackers utilize to direct users to harmful or fake websites, to infect computers or to obtain personal or sensitive data for malicious purposes. Consequently, it is necessary to identify phishing messages in order to provide suitable user protection. Research and numerous studies have included machine learning algorithms and techniques from the field of artificial intelligence which predominantly depend on language-specific datasets and characteristics of phishing messages, and which have demonstrated to be effective for extracting critical information and for data-driven decision making. However, phishing datasets exist mainly for the English language. The aim of this paper is to present an information extraction pipeline that encompasses phases, such as corpus pre-processing, generating predictions of phishing messages using selected machine learning algorithms, along with a basic analysis, confusion matrices and evaluation scores for Croatian phishing messages. This type of key information can be used for teaching in higher education, e.g. in security-related courses or subjects that deal with artificial intelligence, machine learning, big data analysis, computational linguistics etc. This is essential as it can provide deeper insights into phishing attack strategies and potential countermeasures.\n",
            "----\n",
            "Paper 118:\n",
            "Title: Integration of Artificial Intelligence and Macro-Economic Analysis: A Novel Approach with Distributed Information Systems\n",
            "Abstract: INTRODUCTION: This study introduces a groundbreaking approach that integrates Artificial Intelligence (AI) with macro-economic analysis to address a critical gap in existing economic forecasting methodologies. By leveraging diverse economic data sources, the study aims to transcend traditional analytical boundaries and provide a more comprehensive understanding of macroeconomic trends. \n",
            "OBJECTIVE: The primary objective is to pioneer a scalable framework for economic data analysis by combining AI with macroeconomic analysis. The study aims to utilize advanced machine learning algorithms to analyze and synthesize macroeconomic indicators, offering enhanced accuracy and predictive power. A key focus is on dynamically incorporating real-time data to adapt to evolving economic landscapes. \n",
            "METHODS: The research employs advanced machine learning algorithms to analyze and synthesize macroeconomic indicators. The integration of AI allows for a more nuanced understanding of complex economic dynamics. The methodology uniquely adapts to real-time data, providing a scalable framework for economic data analysis. \n",
            "RESULTS: The findings demonstrate the model's efficacy in predicting economic trends, surpassing conventional models in both precision and reliability. The study showcases the potential of AI-driven economic analysis to offer insights into economic dynamics with unprecedented accuracy. \n",
            "CONCLUSION: This study significantly contributes to the fields of AI and economics by proposing a transformative approach to macroeconomic analysis. The integration of technology and economics sets a new precedent, paving the way for future innovations in economic forecasting. The research also explores the implications of AI-driven economic analysis for policy-making, emphasizing its potential to inform more effective economic strategies.\n",
            "----\n",
            "Paper 119:\n",
            "Title: A review of artificial intelligence in marine science\n",
            "Abstract: Utilization and exploitation of marine resources by humans have contributed to the growth of marine research. As technology progresses, artificial intelligence (AI) approaches are progressively being applied to maritime research, complementing traditional marine forecasting models and observation techniques to some degree. This article takes the artificial intelligence algorithmic model as its starting point, references several application trials, and methodically elaborates on the emerging research trend of mixing machine learning and physical modeling concepts. This article discusses the evolution of methodologies for the building of ocean observations, the application of artificial intelligence to remote sensing satellites, smart sensors, and intelligent underwater robots, and the construction of ocean big data. We also cover the method of identifying internal waves (IW), heatwaves, El Niño-Southern Oscillation (ENSO), and sea ice using artificial intelligence algorithms. In addition, we analyze the applications of artificial intelligence models in the prediction of ocean components, including physics-driven numerical models, model-driven statistical models, traditional machine learning models, data-driven deep learning models, and physical models combined with artificial intelligence models. This review shows the growth routes of the application of artificial intelligence in ocean observation, ocean phenomena identification, and ocean elements forecasting, with examples and forecasts of their future development trends from several angles and points of view, by categorizing the various uses of artificial intelligence in the ocean sector.\n",
            "----\n",
            "Paper 120:\n",
            "Title: The role of data science in transforming business operations: Case studies from enterprises\n",
            "Abstract: Data science has emerged as a pivotal force in transforming business operations across various industries, driving innovation, operational efficiency, and strategic decision-making. This review paper explores the multifaceted role of data science in business, examining key concepts, historical integration, and strategic advantages. It discusses the application of data science in diverse business domains, highlighting techniques such as predictive analytics, sentiment analysis, and optimization that have revolutionized marketing, supply chain management, finance, and customer service. The paper further analyzes data science's tangible and intangible benefits, including cost reduction, improved customer experience, and enhanced productivity, which collectively contribute to a competitive edge in the market. The paper reflects on emerging trends like artificial intelligence, machine learning, ethical data practices, and the integration of blockchain and IoT, which are set to shape the future of data-driven business operations. It offers recommendations for businesses to prepare for this evolving landscape, emphasizing the importance of a data-centric culture, robust infrastructure, and collaboration. The paper concludes by underscoring the critical role of data science in fostering sustained business success in an increasingly data-driven world. \n",
            "Keywords: Data Science, Business Transformation, Predictive Analytics, Operational Efficiency, Artificial Intelligence, Strategic Decision-Making.\n",
            "----\n",
            "Paper 121:\n",
            "Title: The Future of Material Scientists in an Age of Artificial Intelligence\n",
            "Abstract: Material science has historically evolved in tandem with advancements in technologies for characterization, synthesis, and computation. Another type of technology to add to this mix is machine learning (ML) and artificial intelligence (AI). Now increasingly sophisticated AI‐models are seen that can solve progressively harder problems across a variety of fields. From a material science perspective, it is indisputable that machine learning and artificial intelligence offer a potent toolkit with the potential to substantially accelerate research efforts in areas such as the development and discovery of new functional materials. Less clear is how to best harness this development, what new skill sets will be required, and how it may affect established research practices. In this paper, those question are explored with respect to increasingly more sophisticated ML/AI‐approaches. To structure the discussion, a conceptual framework of an AI‐ladder is introduced. This AI‐ladder ranges from basic data‐fitting techniques to more advanced functionalities such as semi‐autonomous experimentation, experimental design, knowledge generation, hypothesis formulation, and the orchestration of specialized AI modules as stepping‐stones toward general artificial intelligence. This ladder metaphor provides a hierarchical framework for contemplating the opportunities, challenges, and evolving skill sets required to stay competitive in the age of artificial intelligence.\n",
            "----\n",
            "Paper 122:\n",
            "Title: Artificial Intelligence and Machine Learning based Legal Application: The State-of-the-Art and Future Research Trends\n",
            "Abstract: The advancement of science and technology has facilitated adaptation of human intelligence into its computerized platform for logical analysis of any event. This porting of human intelligence to machine is known as Artificial Intelligence (AI). AI enhances human life since inception with the help of these intelligent machines, human potentials will be augmented in multiple spheres. An enormous improvement in this area of AI has been noticed in the past two decades that has given rise to expert systems. AI has huge impact on different fields of business, engineering, law, medicine, science, weather forecasting, etc. to enhance the quality and efficiency in our day to day life to solve complex problems. For the past few decades, AI has been playing an emerging role in the legal field and will definitely have an effect on the legal practices over the next few years. AI has the potential to analyses legal information based on semantics and make legal predictions from the legal data set, and hence it helps the judiciary system in automation thereby increasing the efficiency within affordable budget. For better understanding of the concept, in this paper authors have performed relevant survey on this field.\n",
            "----\n",
            "Paper 123:\n",
            "Title: Predictive privacy: Collective data protection in the context of artificial intelligence and big data\n",
            "Abstract: Big data and artificial intelligence pose a new challenge for data protection as these techniques allow predictions to be made about third parties based on the anonymous data of many people. Examples of predicted information include purchasing power, gender, age, health, sexual orientation, ethnicity, etc. The basis for such applications of “predictive analytics” is the comparison between behavioral data (e.g. usage, tracking, or activity data) of the individual in question and the potentially anonymously processed data of many others using machine learning models or simpler statistical methods. The article starts by noting that predictive analytics has a significant potential to be abused, which manifests itself in the form of social inequality, discrimination, and exclusion. These potentials are not regulated by current data protection law in the EU; indeed, the use of anonymized mass data takes place in a largely unregulated space. Under the term “predictive privacy,” a data protection approach is presented that counters the risks of abuse of predictive analytics. A person's predictive privacy is violated when personal information about them is predicted without their knowledge and against their will based on the data of many other people. Predictive privacy is then formulated as a protected good and improvements to data protection with regard to the regulation of predictive analytics are proposed. Finally, the article points out that the goal of data protection in the context of predictive analytics is the regulation of “prediction power,” which is a new manifestation of informational power asymmetry between platform companies and society.\n",
            "----\n",
            "Paper 124:\n",
            "Title: IoT Based Smart Systems using Artificial Intelligence and Machine Learning: Accessible and Intelligent Solutions\n",
            "Abstract: The Internet of Things (IoT) has grown in importance in both the technological and social spheres as consumers want smarter homes, more efficient businesses, and more efficient healthcare systems. The social setting is very important for this. Communication networks provide uninterrupted internet connectivity for both fixed and mobile devices like PCs and smartphones. Customers may now keep up their level of engagement while interacting with one or more apps at any time and from any location, thanks to developments in information and communication technologies. Many kinds of smart devices might potentially coordinate their responses to events, sending out alerts or keeping records of the system’s condition in line with a predetermined policy. Constant progress toward intelligent solutions is improving many facets of modern life, including manufacturing, transportation, electronic healthcare, electronic education, and many more. Machine-to-machine (M2M) communication in the IoT is autonomous and dynamic. Diverse approaches are being developed to address the challenges posed by disparate data sources, fluctuating signal quality, and overwhelming quantities of data. From the perspectives of intelligent system technologies, security, vulnerabilities, and the potential role that intelligent solutions based on Machine Learning (ML) and Artificial Intelligence (AI) may play, this article examines the IoT. Concerns about privacy and safety have arisen as a major obstacle to the widespread adoption of the Internet of Things (IoT). We propose that the Standard Security Framework (SSF) be made mandatory for all distributed networks to ensure that hardware and software modules may communicate with one another regardless of their respective platforms. In addition, the effectiveness and security of new innovations in using fog and edge computing in conjunction with cloud applications have been studied.\n",
            "----\n",
            "Paper 125:\n",
            "Title: Machine learning for leaf disease classification: data, techniques and applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 126:\n",
            "Title: Towards a training data model for artificial intelligence in earth observation\n",
            "Abstract: Abstract Artificial Intelligence Machine Learning (AI/ML), in particular Deep Learning (DL), is reorienting and transforming Earth Observation (EO). A consistent data model for delivery of training data will support the FAIR data principles (findable, accessible, interoperable, reusable) and enable Web-based use of training data in a spatial data infrastructure (SDI). Existing training datasets, including open source benchmark datasets, are usually packaged into public or personal repositories and lack discoverability and accessibility. Moreover, there is no unified method to describe the training data. Here we propose a training data model for AI in EO to allow documentation, storage, and sharing of geospatial training data in a distributed infrastructure. We present design rationales, information models, and an encoding method. Several scenarios illustrate the intended uses and benefits for EO DL applications in an open Web environment. The relationship with Open Geospatial Consortium (OGC) standards is also discussed, as is the impact on an AI-ready SDI.\n",
            "----\n",
            "Paper 127:\n",
            "Title: MPpredictor: An Artificial Intelligence-Driven Web Tool for Composition-Based Material Property Prediction\n",
            "Abstract: The applications of artificial intelligence, machine learning, and deep learning techniques in the field of materials science are becoming increasingly common due to their promising abilities to extract and utilize data-driven information from available data and accelerate materials discovery and design for future applications. In an attempt to assist with this process, we deploy predictive models for multiple material properties, given the composition of the material. The deep learning models described here are built using a cross-property deep transfer learning technique, which leverages source models trained on large data sets to build target models on small data sets with different properties. We deploy these models in an online software tool that takes a number of material compositions as input, performs preprocessing to generate composition-based attributes for each material, and feeds them into the predictive models to obtain up to 41 different material property values. The material property predictor is available online at http://ai.eecs.northwestern.edu/MPpredictor.\n",
            "----\n",
            "Paper 128:\n",
            "Title: Unraveling COVID-19 Dynamics via Machine Learning and XAI: Investigating Variant Influence and Prognostic Classification\n",
            "Abstract: Machine learning (ML) has been used in different ways in the fight against COVID-19 disease. ML models have been developed, e.g., for diagnostic or prognostic purposes and using various modalities of data (e.g., textual, visual, or structured). Due to the many specific aspects of this disease and its evolution over time, there is still not enough understanding of all relevant factors influencing the course of COVID-19 in particular patients. In all aspects of our work, there was a strong involvement of a medical expert following the human-in-the-loop principle. This is a very important but usually neglected part of the ML and knowledge extraction (KE) process. Our research shows that explainable artificial intelligence (XAI) may significantly support this part of ML and KE. Our research focused on using ML for knowledge extraction in two specific scenarios. In the first scenario, we aimed to discover whether adding information about the predominant COVID-19 variant impacts the performance of the ML models. In the second scenario, we focused on prognostic classification models concerning the need for an intensive care unit for a given patient in connection with different explainability AI (XAI) methods. We have used nine ML algorithms, namely XGBoost, CatBoost, LightGBM, logistic regression, Naive Bayes, random forest, SGD, SVM-linear, and SVM-RBF. We measured the performance of the resulting models using precision, accuracy, and AUC metrics. Subsequently, we focused on knowledge extraction from the best-performing models using two different approaches as follows: (a) features extracted automatically by forward stepwise selection (FSS); (b) attributes and their interactions discovered by model explainability methods. Both were compared with the attributes selected by the medical experts in advance based on the domain expertise. Our experiments showed that adding information about the COVID-19 variant did not influence the performance of the resulting ML models. It also turned out that medical experts were much more precise in the identification of significant attributes than FSS. Explainability methods identified almost the same attributes as a medical expert and interesting interactions among them, which the expert discussed from a medical point of view. The results of our research and their consequences are discussed.\n",
            "----\n",
            "Paper 129:\n",
            "Title: Using Artificial Intelligence to Filter out Barking, Typing, and other Noise from Video Calls in Microsoft Teams\n",
            "Abstract: The normal method for analyzing technology is formulating many search queries to extract patent datasets and filter the data physically. The purpose of filtering the collected data is to remove noise to guarantee accurate information analysis. With the advancement in technology and machine learning, the work of physical analysis of the patent can be programmed so the system can remove noise depending on the results based on the previous data. Microsoft team generates a new artificial intelligence model that provides solutions on how individuals respond to speakers. Microsoft team, workplace, Facebook, and Google collected data from many active users hence developing artificial intelligence to minimize distracting background noise, barking and typing during the call.\n",
            "----\n",
            "Paper 130:\n",
            "Title: Artificial intelligence and entrepreneurship education: A paradigm in Qatari higher education institutions after covid-19 pandemic\n",
            "Abstract: The spread of the Covid-19 pandemic and the interruption of personal communication between the teacher and students in higher education led to the need for finding solutions that enable the continuation of the educational process and ensure access to accurate information that improves the level of human capital in dealing with dynamic environments. Therefore, this research sought to analyse the impact of the application of artificial intelligence in entrepreneurship education in Qatari higher education institutions after the Corona pandemic. The measurement of artificial intelligence was based on dimensions (machine learning, natural language processing, expert systems, and machine vision), while entrepreneurship education was measured by dimensions of (entrepreneurial cognition, entrepreneurial competence, and innovation spirit). The research followed an experimental quantitative approach based on collecting data from Qatari university students using a questionnaire developed for the research purpose. Hence, the convenience sample used in the research was composed of 402 students from various Qatari universities, which represents a response rate of 67% from the distributed questionnaires. The statistical analysis of the research data was based on the covariance-based structural equation modeling technique (CB-SEM). The results of the research indicated that all dimensions of artificial intelligence had a positive impact on entrepreneurial education, with the highest impact being machine vision and the lowest impact being natural language processing. Accordingly, the results of the research revealed the need to invest in technological capabilities for supporting the educational system aimed at generating innovative human resources capable of coping with the uncertainty of the work environment.\n",
            "----\n",
            "Paper 131:\n",
            "Title: A Study on Music Genre Classification using Machine Learning\n",
            "Abstract: Artificial Intelligence (AI) and Machine Learning can be cited as one of the greatest technological advancements in this century. They are revolutionizing the fields of computing, finance, healthcare, agriculture, music, space and tourism. Powerful models have achieved excellent performance on a myriad of complex learning tasks. One such subset of AI is audio analysis. It entails music information retrieval, music generation and music classification. Music data is one the most abstruse type of source data present, mainly because it is a tough work to extract meaningful correlating features from it. Hence a myriad of algorithms ranging from classical to hybrid neural networks have been tried on music data for a getting a good accuracy. This paper studies the various methods that can be used for music genre classification and compares between them. The accuracies we obtained on a small sample of the Free Music Archive (FMA) dataset were: 46% using Support Vector Classifier (SVC), 40% using Logistic Regression, 67% using Artificial Neural Network (ANN), 77% using Convolutional Neural Networks (CNN), 90% using Convolution-Recurrent Neural Network (CRNN), 88% using Parallel Convolution-Recurrent Neural Network (PCRNN), 73% without using Ensemble technique and 85% using Ensemble technique of Ada Boost. We defined SVC as our baseline model which had 46% accuracy, and we defined the succeeding models to achieve accuracy greater than that. ANN gave us 67% on the test dataset, which was surpassed by CNN at 77%. We noticed image based features worked better at classifying the labels than normal extracted features from the audio. A combination of CNN and RNN worked the best for the dataset, with a series CRNN model giving the best accuracy. Succeeding that, we tried to fit an ensemble model onto our dataset and analyzed its workings. This paper presents a comprehensive study of the various methods that can be used for music genre classification, with a focus on some parallel models and ensembling techniques.\n",
            "----\n",
            "Paper 132:\n",
            "Title: Application of Machine Learning to Predict Mental Health Disorders and Interpret Feature Importance\n",
            "Abstract: The mental health and mental illness crisis has become increasingly acute in recent years, and many digital solutions with artificial intelligence at their core offer hope for reversing the deterioration of our mental health. Machine deep learning techniques can be used to analyse big data to build predictive models for psycho-education, assessment and screening to assess the mental health status of subjects, and can help the clinical community discover information that is not available to many traditional psychological research tools. This paper presents an in-depth analysis of a mental health survey and examines how it can be applied to the Al/ML domain of mental health research and how machine learning models can be used in this domain for fitting and prediction. Based on this, the importance of the presence or absence of current mental health disorders on other characteristics of respondents is assessed and visualised. It was found that the Cross Gradient Booster (Random Forest) model gave the best prediction fit among the various types of machine learning models, and the Grid Search algorithm was used to confirm that the final model had the highest accuracy of 0.79784 at a learning rate of 0.1. The Permutation Importance analysis revealed that the most important characteristic is whether or not the person has suffered from a mental health disorder in the past.\n",
            "----\n",
            "Paper 133:\n",
            "Title: Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods\n",
            "Abstract: One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identification and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, finance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken.\n",
            "----\n",
            "Paper 134:\n",
            "Title: Artificial Intelligence (AI): Evolution, Methodologies, and Applications\n",
            "Abstract: Abstract: Artificial intelligence, also known as AI, is a technology that enables computers and machines to emulate human intelligence and problem-solving abilities. Computers can perform a wide range of advanced functions thanks to the use of artificial intelligence (AI) technologies. These functions include the ability of machines to see, comprehend, and translate spoken and written language, analyzing data, making recommendations, and more. Artificial intelligence is considered a field of computer science that encompasses other areas like machine learning and deep learning, data analytics, linguistics, software engineering, and so on. These disciplines often involve the development of AI algorithms that are based on the decision-making processes of the human brain, that has the ability to learn and memorize from existing data and allows more precise classifications or predictions over a period of time. AI is the foundation for innovation in modern computing, discovering the value for both individuals and businesses. For illustration, AI is used to extricate content and information from pictures and documents, turns unstructured content into business-ready structured data, and unlocks valuable insights. AI has the ability to perform tasks that would otherwise require human intelligence or intervention, when combined with other technologies such as sensors, and robotics. It is used in many areas of life, including education, finance, healthcare, and manufacturing. Here are some examples of AI in different areas: Facial detection and recognition, Text editors, Digital assistants, Self-driving cars, and many more.\n",
            "----\n",
            "Paper 135:\n",
            "Title: Machine Learning Algorithms, Perspectives, and Real - World Application: Empirical Evidence from United States Trade Data\n",
            "Abstract: : Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without being explicitly programmed. It is one of today’s most rapidly growing technical fields, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Various types of machine learning algorithms such as supervised, unsupervised, semi - supervised, and reinforcement learning exist in this area. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (commonly known as “big - data”) and low - cost computation. The adoption of data - intensive ML - based methods can be found throughout science, technology, and commerce, leading to more evidence - based decision - making across many walks of life, including finance, manufacturing, international trade, economics, education, healthcare, marketing, policymaking, and data governance. The present paper provides a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and capabilities of an application. Moreover, the paper attempts to determine the accurate clusters of similar industries in United States that collectively account for more than 85 percent of economy’s aggregate export and import flows over the period 2002 - 2021 through clustering algorithm (unsupervised learning). Four clusters of mapping labels have been used, namely the low investment (LL), category 1 medium investment (HL), category 2 medium investment (LH) and high investment (HH). The empirical results indicate that machinery and electrical equipment is classified as a high investment sector due to its efficient production mechanism. The analysis further underlines the need for upstream value chain integration through skill - augmentation and innovation especially in low investment industries. Overall, this paper aims to explain the trends of ML approaches and their applicability in various real - world domains, as well as serve as a reference point for academia\n",
            "----\n",
            "Paper 136:\n",
            "Title: Secure Knowledge Management and Cybersecurity in the Era of Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 137:\n",
            "Title: Artificial Intelligence (AI) and the Prediction of Climate Change Impacts\n",
            "Abstract: This comprehensive review analyses the confluence of Artificial Intelligence (AI) and climate change anticipation, examining the possibilities of AI approaches in forecasting and mitigating the effects of climate change. The research delves into diverse AI implementations, including machine learning algorithms, neural networks, and data extraction, employed in climate simulation, weather projection, and ecological surveillance. By analysing vast datasets, AI algorithms enhance our understanding of climate patterns, enabling accurate predictions of extreme weather events, sea-level rise, and ecosystem shifts. The review also assesses the challenges and ethical considerations associated with AI implementation in climate science, emphasizing the importance of responsible AI development. The integration of AI technologies in climate change research signifies a significant leap toward proactive adaptation and sustainable decision-making, offering invaluable insights for policymakers and researchers striving to address the global climate crisis.\n",
            "----\n",
            "Paper 138:\n",
            "Title: Explainable artificial intelligence and machine learning: A reality rooted perspective\n",
            "Abstract: As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics.\n",
            "----\n",
            "Paper 139:\n",
            "Title: The Need to Prioritize Model-Updating Processes in Clinical Artificial Intelligence (AI) Models: Protocol for a Scoping Review\n",
            "Abstract: Background With an increase in the number of artificial intelligence (AI) and machine learning (ML) algorithms available for clinical settings, appropriate model updating and implementation of updates are imperative to ensure applicability, reproducibility, and patient safety. Objective The objective of this scoping review was to evaluate and assess the model-updating practices of AI and ML clinical models that are used in direct patient-provider clinical decision-making. Methods We used the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) checklist and the PRISMA-P protocol guidance in addition to a modified CHARMS (Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies) checklist to conduct this scoping review. A comprehensive medical literature search of databases, including Embase, MEDLINE, PsycINFO, Cochrane, Scopus, and Web of Science, was conducted to identify AI and ML algorithms that would impact clinical decision-making at the level of direct patient care. Our primary end point is the rate at which model updating is recommended by published algorithms; we will also conduct an assessment of study quality and risk of bias in all publications reviewed. In addition, we will evaluate the rate at which published algorithms include ethnic and gender demographic distribution information in their training data as a secondary end point. Results Our initial literature search yielded approximately 13,693 articles, with approximately 7810 articles to consider for full reviews among our team of 7 reviewers. We plan to complete the review process and disseminate the results by spring of 2023. Conclusions Although AI and ML applications in health care have the potential to improve patient care by reducing errors between measurement and model output, currently there exists more hype than hope because of the lack of proper external validation of these models. We expect to find that the AI and ML model-updating methods are proxies for model applicability and generalizability on implementation. Our findings will add to the field by determining the degree to which published models meet the criteria for clinical validity, real-life implementation, and best practices to optimize model development, and in so doing, reduce the overpromise and underachievement of the contemporary model development process. International Registered Report Identifier (IRRID) PRR1-10.2196/37685\n",
            "----\n",
            "Paper 140:\n",
            "Title: Study of water resources parameters using artificial intelligence techniques and learning algorithms: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 141:\n",
            "Title: Review on the Application of Artificial Intelligence in Bioinformatics\n",
            "Abstract: Compared with traditional data, biomedical data has the characteristics of less samples, high dimension, unstructured data, more types of data, and huge amount of data. Bioinformatics processing requires the intersection of statistics, mathematics, and computer science. Artificial intelligence technology has been used in genome annotation, drug design, structure prediction and other research, helping biologists to screen, process, interpret and utilize the huge data collected in biological research. This paper reviews the application of artificial intelligence in biological information, introduces the application and research progress of machine learning in gene analysis, disease diagnosis, brain image processing, etc., and finally summarizes the full text.\n",
            "----\n",
            "Paper 142:\n",
            "Title: Artificial Intelligence and Machine Learning for Healthcare Solutions\n",
            "Abstract: Trends of teaching and learning has changed its shape from offline teaching to online teaching as full mode and physical mode of teaching may become substitution of academic keeping in view the pandemic covid-19. Data science has become part of parcel of our daily life and most of the technical apps we are using contains machine Learning algorithms and helps us in many ways. With rising conditions, artificial intelligence will be the most prominent transformative technology and enabler for society in the present era. here is no uncertainty that AI and analogous frameworks are built to change global efficiency, working habits, and lifestyles and support healthcare, Pharma Industry and Transformation in diagnosis process, disease treatment and early identification of symptoms has been fuelled machine learning techniques and tools such as Generative Adversarial Networks (GAN), Deep Convolutional Networks, Deep Reinforcement Learning (DRL), Gradient-boosted-tree models (GBM), etc. MRI and other sophisticated imaging systems immensely used for neural disorders, cancer diagnostics. In this chapter we are discussing various resources of medical datasets which can be used for diagnosis of dementia with the usage of machine learning approaches. We are presenting how various machine learning approaches can be useful in early diagnosis of many diseases and explained where machine learning and deep learning can be used on electronically stored medical data. Recent developments are achieved in what way machine learning can be applicable in multi-disciplinary research areas. The main emphasis of this chapter is to elaborate on the applicability of machine learning in the domain of healthcare. In the past, there had been substantial signs of progress in the way where machine learning can apply in innumerable research and industries. This chapter deliberates the prospect of using machine learning technologies in the healthcare sector and sketches several industry ingenuities implementing machine learning initiatives. © 2021 Scrivener Publishing LLC.\n",
            "----\n",
            "Paper 143:\n",
            "Title: Dimensions of Legal And Moral Use of Artificial Intelligence In Education\n",
            "Abstract: The purpose of the research is to critically analyze the legal aspects of the use of artificial intelligence (AI) in the field of education, as well as to study the role of chatbots and the Chat GPT model, plagiarism issues, educational modeling and the impact of AI on the labor market. To achieve this goal, various research methods are used, including the analysis of current legal norms and international legislation that relate to the use of AI in education. The study also includes an analysis of intellectual property issues, data privacy, ethical standards and liability. \n",
            "The results of the study highlight the problem of plagiarism in chat rooms and emphasize the importance of careful use of information to ensure academic integrity. Despite the possible misuse of AI by students, such as chatbots and GPT models, for plagiarism, these technologies can also facilitate plagiarism detection. The research also examines the use of machine learning and data analytics to create personalized learning experiences, improve learning effectiveness, and retain knowledge. \n",
            "The overall conclusion is that the integration of artificial intelligence in education has the potential to improve the quality and accessibility of education, but this requires a sound legal framework. The article also evaluates the effectiveness of various AI tools, including chatbots that provide information on demand and Chat GPT, useful for processing textual materials. The paper also examines the role of learning simulation in personalizing education, using AI to analyze performance data, and tailoring individual learning pathways.\n",
            "----\n",
            "Paper 144:\n",
            "Title: Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process\n",
            "Abstract: Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.\n",
            "----\n",
            "Paper 145:\n",
            "Title: Unraveling the Impact of Land Cover Changes on Climate Using Machine Learning and Explainable Artificial Intelligence\n",
            "Abstract: A general issue in climate science is the handling of big data and running complex and computationally heavy simulations. In this paper, we explore the potential of using machine learning (ML) to spare computational time and optimize data usage. The paper analyzes the effects of changes in land cover (LC), such as deforestation or urbanization, on local climate. Along with green house gas emission, LC changes are known to be important causes of climate change. ML methods were trained to learn the relation between LC changes and temperature changes. The results showed that random forest (RF) outperformed other ML methods, and especially linear regression models representing current practice in the literature. Explainable artificial intelligence (XAI) was further used to interpret the RF method and analyze the impact of different LC changes on temperature. The results mainly agree with the climate science literature, but also reveal new and interesting findings, demonstrating that ML methods in combination with XAI can be useful in analyzing the climate effects of LC changes. All parts of the analysis pipeline are explained including data pre-processing, feature extraction, ML training, performance evaluation, and XAI.\n",
            "----\n",
            "Paper 146:\n",
            "Title: Preparing Radiologists to Lead in the Era of Artificial Intelligence: Designing and Implementing a Focused Data Science Pathway for Senior Radiology Residents.\n",
            "Abstract: Artificial intelligence and machine learning (AI-ML) have taken center stage in medical imaging. To develop as leaders in AI-ML, radiology residents may seek a formative data science experience. The authors piloted an elective Data Science Pathway (DSP) for 4th-year residents at the authors' institution in collaboration with the MGH & BWH Center for Clinical Data Science (CCDS). The goal of the DSP was to provide an introduction to AI-ML through a flexible schedule of educational, experiential, and research activities. The study describes the initial experience with the DSP tailored to the AI-ML interests of three senior radiology residents. The authors also discuss logistics and curricular design with common core elements and shared mentorship. Residents were provided dedicated, full-time immersion into the CCDS work environment. In the initial DSP pilot, residents were successfully integrated into AI-ML projects at CCDS. Residents were exposed to all aspects of AI-ML application development, including data curation, model design, quality control, and clinical testing. Core concepts in AI-ML were taught through didactic sessions and daily collaboration with data scientists and other staff. Work during the pilot period led to 12 accepted abstracts for presentation at national meetings. The DSP is a feasible, well-rounded introductory experience in AI-ML for senior radiology residents. Residents contributed to model and tool development at multiple stages and were academically productive. Feedback from the pilot resulted in establishment of a formal AI-ML curriculum for future residents. The described logistical, planning, and curricular considerations provide a framework for DSP implementation at other institutions. Supplemental material is available for this article. © RSNA, 2020.\n",
            "----\n",
            "Paper 147:\n",
            "Title: Integrated Evolutionary Learning: An Artificial Intelligence Approach to Joint Learning of Features and Hyperparameters for Optimized, Explainable Machine Learning\n",
            "Abstract: Artificial intelligence and machine learning techniques have proved fertile methods for attacking difficult problems in medicine and public health. These techniques have garnered strong interest for the analysis of the large, multi-domain open science datasets that are increasingly available in health research. Discovery science in large datasets is challenging given the unconstrained nature of the learning environment where there may be a large number of potential predictors and appropriate ranges for model hyperparameters are unknown. As well, it is likely that explainability is at a premium in order to engage in future hypothesis generation or analysis. Here, we present a novel method that addresses these challenges by exploiting evolutionary algorithms to optimize machine learning discovery science while exploring a large solution space and minimizing bias. We demonstrate that our approach, called integrated evolutionary learning (IEL), provides an automated, adaptive method for jointly learning features and hyperparameters while furnishing explainable models where the original features used to make predictions may be obtained even with artificial neural networks. In IEL the machine learning algorithm of choice is nested inside an evolutionary algorithm which selects features and hyperparameters over generations on the basis of an information function to converge on an optimal solution. We apply IEL to three gold standard machine learning algorithms in challenging, heterogenous biobehavioral data: deep learning with artificial neural networks, decision tree-based techniques and baseline linear models. Using our novel IEL approach, artificial neural networks achieved ≥ 95% accuracy, sensitivity and specificity and 45–73% R2 in classification and substantial gains over default settings. IEL may be applied to a wide range of less- or unconstrained discovery science problems where the practitioner wishes to jointly learn features and hyperparameters in an adaptive, principled manner within the same algorithmic process. This approach offers significant flexibility, enlarges the solution space and mitigates bias that may arise from manual or semi-manual hyperparameter tuning and feature selection and presents the opportunity to select the inner machine learning algorithm based on the results of optimized learning for the problem at hand.\n",
            "----\n",
            "Paper 148:\n",
            "Title: Machine Learning and Artificial Intelligence\n",
            "Abstract: This chapter proposes a cost-effective and scalable approach to obtain information on the current living standards and development in rural areas across India. The model utilizes a CNN to analyze satellite images of an area and predict its land type and level of development. A decision tree classifies a region as rural or urban based on the analysis. A summary describing the area is generated from inferences made on the recorded statistics. The CNN is able to predict the land and development distribution with an accuracy of 95.1%. The decision tree predicts rural areas with a precision of 99.6% and recall of 88.9%. The statistics obtained for a dataset of more than 1000 villages in India are cross-validated against the Census of India 2011 data. The proposed technique is in contrast to traditional door-to-door surveying methods as the information retrieved is relevant and obtained without human intervention. Hence, it can aid efforts in tracking poverty at a finer level and provide insight on improving the economic livelihood in rural areas.\n",
            "----\n",
            "Paper 149:\n",
            "Title: Machine Learning and Artificial Intelligence for Digital Twin to Accelerate Sustainability in Positive Energy Districts\n",
            "Abstract: None\n",
            "----\n",
            "Paper 150:\n",
            "Title: Developing Machine Learning Algorithm Literacy with Novel Plugged and Unplugged Approaches\n",
            "Abstract: Data science and machine learning should not only be research areas for scientists and researchers but should also be accessible and understandable to the general audience. Enabling students to understand the details behind the technology will support them in becoming aware consumers and encourage them to become active participants. In this paper, we present instructional materials developed for introducing students to two key machine learning algorithms: decision trees and k-nearest neighbors. The materials were tested in a middle school's afterschool artificial intelligence program with four participating students aged 12 to 13. A combination of hands-on activities, innovative technology, and intuitive examples facilitated student learning. With hand-drawn decision trees and penguin species classifications, students used the algorithms to solve problems and anticipate other possible applications. We present the technology used, curriculum materials developed, and classroom structure. Following the guidelines from AI4K12 and introducing foundational machine learning algorithms, we hope to foster student interest in STEM fields.\n",
            "----\n",
            "Paper 151:\n",
            "Title: RETRACTED ARTICLE: Simulation of English feature recognition based on machine learning and artificial intelligence technology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 152:\n",
            "Title: Stock Market Prediction through Artificial Intelligence, Machine Learning and Neural Networks\n",
            "Abstract: Stock prices and their fluctuations have a major impact on our daily lives. Therefore, it is necessary to discuss this forum today and study its various aspects. he use of machine learning(ML) and artificial intelligence(AI) in this field can bring us new insights, and the use of computers to predict prices can give us significant advantages in this field. In this paper, there is a significant attempt to achieve this stock market forecasting with the help of two techniques as follows: The first technique uses neural networking :It is used to collect and analyse the data to calculate a price by finding a suitable balance of past information that equals the present information. The final report which is generated by the above process is then upgraded by combining the actual prices in the past associated with the market. The next technique which is being involved here is linear regression. Linear regression is used to forecast prices that will involve the coming price having a calculated and nearly accurate probability. This model uses the previous data available and gives accurate results for the stock price for the next day. The model will further assist in the future research and will be useful for the growing scientific community in this field.\n",
            "----\n",
            "Paper 153:\n",
            "Title: Multi-Modal Comparative Analysis on Execution of Phishing Detection Using Artificial Intelligence\n",
            "Abstract: Phishing is the process of deceiving or stealing private or confidential information through illicit means. This could lead to financial loss, loss of reputation, and identity theft. Hence, identifying and preventing the use of such phishing sites becomes crucial. In data science, the term outlier, also termed an anomaly refers to points or series of data that opt out of the normal behaviour of the system under study. Anomaly detection touches down on the concepts related to studying the authentic outlier in a data set. The paper aims to present the optimised techniques and multiple modes of executing the process for detecting phishing websites. The most relevant features are chosen for execution by applying feature extraction. The Mendley phishing websites dataset is used to detect phishing websites, along with the SPAM-HAM publicly available dataset, which is used for detecting SPAM/HAM classification for SMS data in this research study. The experiments are also carried out on a custom dataset to avoid any bias present in a publicly available dataset. The study is carried out in three modes, namely offline, batch, and incremental, using machine learning models. The performance evaluation metrics such as accuracy, f1 score, precision, recall, and time complexity of the machine learning models and accuracy and loss metrics of the deep learning models are compared between the different modes. The study is then summarised by detailing the pros and cons of each of the modes and models used for the study. The incremental mode of execution suits better for real-time processing, with an accuracy of 97.1% on the custom dataset using the adaptive random forest (ARF) classifier available in the Python’s River Framework. But if we make use of the deep learning approach with Keras sequential model, the accuracy obtained was 99.28%.\n",
            "----\n",
            "Paper 154:\n",
            "Title: A new era: artificial intelligence and machine learning in prostate cancer\n",
            "Abstract: None\n",
            "----\n",
            "Paper 155:\n",
            "Title: Machine Learning and Artificial Intelligence - Proceedings of MLIS 2020, Virtual Event, October 25-28, 2020\n",
            "Abstract: This course is an introduction to data science for non-computer scientists. The course covers topics from data preparation, clustering, regression and classification, model evaluation\n",
            "----\n",
            "Paper 156:\n",
            "Title: Healthcare predictive analytics using machine learning and deep learning techniques: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 157:\n",
            "Title: Industry 4.0 and Society 5.0 through Lens of Condition Based Maintenance (CBM) and Machine Learning of Artificial Intelligence (MLAI)\n",
            "Abstract: This paper provides preliminary discourse on buzz words about Industry 4.0 and Society 5.0. This discourse focuses on the lens of Condition Based Maintenance (CBM) and Machine Learning of Artificial Intelligence (MLAI). To some extent several companies have embarked Industry 4.0 and Society 5.0 within Internet of Things (IoT) technology. Through the wave of IoT Technology, Industries are adopting automated machinery. Predictive maintenance (PM) is indispensable not only toward the machines” vitality and longevity purpose, but also toward the human error reduction. This paper elaborates its discourse of Industry 4.0 and Society through the lens of CBM and MLAI. The mentioned Machine Learning, in this paper, refers to research methodology, as methodological frameworks. Those frameworks comprise several phases, which are: 1. Equipment Analysis; 2. Data Evaluation; 3. Data Selection and Process; 4. Modeling; 5. Decision Support Model Evaluation. The MLAI techniques are based upon the identification of behaviour patterns. This identification comprises datasets that exclude mathematical models or prior historical knowledge. The discourse in this paper intertwines CBM process and MLAI through data cleaning and processing, features stratification and extraction, model stratification and validation. This paper elaborates two renowned maintenance approaches which are preventive and corrective maintenance. Discourse in this paper focuses on corrective action, known as predictive maintenance (PM), or condition based maintenance (CBM) within Reliability Centered Maintenance (RCM). CBM is chosen as the most desirable strategy, as it involves the intervention as the consequence of the machine breakdown. It also provides cost savings toward spare parts consumption, and optimizes production.\n",
            "----\n",
            "Paper 158:\n",
            "Title: Outlook for artificial intelligence and machine learning at the NSLS-II\n",
            "Abstract: We describe the current and future plans for using artificial intelligence and machine learning (AI/ML) methods at the National Synchrotron Light Source II (NSLS-II), a scientific user facility at the Brookhaven National Laboratory. We discuss the opportunity for using the AI/ML tools and techniques developed in the data and computational science areas to greatly improve the scientific output of large scale experimental user facilities. We describe our current and future plans in areas including from detecting and recovering from faults, optimizing the source and instrument configurations, streamlining the pipeline from measurement to insight, through data acquisition, processing, analysis. The overall strategy and direction of the NSLS-II facility in relation to AI/ML is presented.\n",
            "----\n",
            "Paper 159:\n",
            "Title: Machine learning & artificial intelligence in the quantum domain: a review of recent progress\n",
            "Abstract: Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research—quantum information versus machine learning (ML) and artificial intelligence (AI)—have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our ‘big data’ world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement—exploring what ML/AI can do for quantum physics and vice versa—researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.\n",
            "----\n",
            "Paper 160:\n",
            "Title: Artificial Intelligence and Machine Learning Applications in Smart Production: Progress, Trends and Directions\n",
            "Abstract: The history of Artificial Intelligence (AI) development dates to the 40s. The researchers showed strong expectations until the 70s, when they began to encounter serious difficulties and investments were greatly, reduced. With the introduction of the Industry 4.0, one of the techniques adopted for AI implementation is Machine Learning (ML) that focuses on the machines ability to receive data series and learn on their own. Given the considerable importance of the subject, researchers have completed many studies on ML to ensure that machines are able to replace or relieve human tasks. This research aims to analyze, systematically, the literature on several aspects, including publication year, authors, scientific sector, country, institution, keywords. Analyzing existing literature on AI is a necessary stage to recommend policy on the matter. The analysis has been done using Web of Science and SCOPUS database. Furthermore, UCINET and NVivo 12 software have been used to complete them. Literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were reviewed and classified. A first interesting result is the greater number of works published by USA and the increasing interest after the birth of Industry 4.0.\n",
            "----\n",
            "Paper 161:\n",
            "Title: Artificial intelligence and machine learning in design of mechanical materials.\n",
            "Abstract: Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.\n",
            "----\n",
            "Paper 162:\n",
            "Title: Probabilistic machine learning and artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 163:\n",
            "Title: DATA SCIENCE IN ENERGY CONSUMPTION ANALYSIS: A REVIEW OF AI TECHNIQUES IN IDENTIFYING PATTERNS AND EFFICIENCY OPPORTUNITIES\n",
            "Abstract: This review critically examines the role of Data Science and Artificial Intelligence (AI) techniques in energy consumption analysis, focusing on their efficacy in identifying patterns and uncovering efficiency opportunities. The primary objective is to assess how AI methodologies are transforming energy consumption analysis, with an emphasis on pattern recognition and optimization of energy efficiency. The study adopts a systematic literature review approach, scrutinizing peer-reviewed articles published between 2015 and 2022. This methodological framework ensures a comprehensive and relevant analysis of current AI applications in the energy sector. Key findings reveal a significant evolution from traditional energy analysis methods to sophisticated AI-driven techniques. AI has proven instrumental in accurately predicting energy consumption patterns, facilitating enhanced decision-making for energy management. The review identifies various AI techniques, including machine learning, deep learning, and predictive analytics, and their specific applications in energy consumption analysis. The study also delves into the technological, economic, and environmental implications of integrating AI in energy analysis, highlighting both the challenges and potential solutions. It underscores the growing trend of AI applications in enhancing energy efficiency and the emerging opportunities therein. This offers a comprehensive overview of current trends and future directions, serving as a guide for industry stakeholders, policymakers, and researchers in harnessing AI for more efficient and sustainable energy consumption analysis. \n",
            "Keywords: Artificial Intelligence, Efficiency Optimization, Pattern Recognition, Energy Consumption Analysis.\n",
            "----\n",
            "Paper 164:\n",
            "Title: A Review on Use of Data Science for Visualization and Prediction of the COVID-19 Pandemic and Early Diagnosis of COVID-19 Using Machine Learning Models\n",
            "Abstract: None\n",
            "----\n",
            "Paper 165:\n",
            "Title: Survey on Systematic Analysis of Deep Learning Models Compare to Machine Learning\n",
            "Abstract: This survey provides a comprehensive analysis of the systematic differences and advancements between deep learning (DL) and traditional machine learning (ML) models. By examining a wide array of research papers, the study highlights the unique strengths and applications of both methodologies. Deep learning, with its multi-layered neural networks, excels in handling large, unstructured datasets, making significant strides in image and speech recognition, natural language processing, and complex pattern recognition tasks. Conversely, traditional machine learning models, which rely on feature extraction and simpler algorithms, remain highly effective in structured data scenarios such as classification, regression, and clustering problems. The survey elucidates the criteria for choosing between DL and ML, focusing on factors like data size, computational resources, and specific application requirements. Furthermore, it discusses the evolving landscape of hybrid models that integrate DL and ML techniques to leverage the strengths of both approaches. This analysis provides valuable insights for researchers and practitioners aiming to deploy the most suitable AI models for their specific needs, emphasizing the importance of contextual understanding in the rapidly advancing field of artificial intelligence.\n",
            "----\n",
            "Paper 166:\n",
            "Title: Systematic Analysis of Deep Learning Models vs. Machine Learning\n",
            "Abstract: Deep learning (DL) and classical machine learning (ML) models are compared and contrasted in this study, which offers a complete overview of the differences and technological improvements between the two types of models. Through an analysis of a diverse range of research publications, the study draws attention to the distinct advantages and uses of both techniques. Deep learning, which is characterized by its use of neural networks with several layers, is particularly effective at managing massive datasets that are not organized. It has also made great progress in the areas of image and audio recognition, natural language processing, and complicated pattern identification exercises. On the other hand, classic machine learning models, which are based on the extraction of features and simpler methods, continue to be quite successful in structured data situations such as classification, regression, and clustering challenges. By concentrating on aspects such as data quantity, computing resources, and unique application needs, the survey sheds light on the parameters that should be considered when selecting between deep learning and machine learning. In addition to this, it addresses the ever-changing environment of hybrid models, which combine methods from both deep learning and machine learning in order to capitalize on the advantages of both approaches. This study highlights the significance of contextual awareness in the fast-developing area of artificial intelligence by providing researchers and practitioners with useful insights that can be used to deploy the AI models that are the most appropriate for their particular requirements.\n",
            "----\n",
            "Paper 167:\n",
            "Title: Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda\n",
            "Abstract: None\n",
            "----\n",
            "Paper 168:\n",
            "Title: Predicting Sea Level Rise Using Artificial Intelligence: A Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 169:\n",
            "Title: Beyond Predictive Learning Analytics Modelling and onto Explainable Artificial Intelligence with Prescriptive Analytics and ChatGPT\n",
            "Abstract: None\n",
            "----\n",
            "Paper 170:\n",
            "Title: PhD position in Data Science & Artiﬁcial Intelligence Latent Data Models for Large-Scale Clustering\n",
            "Abstract: S tatistical M odeling and I nference for unsupervised L earning at larg E - S cale (SMILES) is a collaborative fundamental research project funded by ANR (2018-2022) in the framework of the plan of the French state towards Artiﬁcial Intelligence (AI). Large-scale data analysis is an inherently multidisciplinary area and is becoming of broader interest for today’s society. SMILES aims at introducing an unsupervised statistical modeling framework and scaled inference algorithms for transforming large-scale data into knowledge. It considers the large-scale context as a whole, with its main issues related to inference from a big volume of data of very high dimension and underlying complex hidden structures. The key tenet of SMILES is to introduce large-scale latent data models (LDM) for unsupervised data classiﬁcation and representation. The knowledge extraction will namely consist in automatically retrieving hidden structures, summarizing prototypes, groups, sparse representations. We consider different data settings, including functional data, multimodal bioacoustical data, and biological data. SMILES gathers experts in statistical modeling, inference, optimisation, sparse representation, information processing and machine learning. The consortium is composed of four research organisms:\n",
            "----\n",
            "Paper 171:\n",
            "Title: Exploring explainable AI: category theory insights into machine learning algorithms\n",
            "Abstract: Explainable artificial intelligence (XAI) is a growing field that aims to increase the transparency and interpretability of machine learning (ML) models. The aim of this work is to use the categorical properties of learning algorithms in conjunction with the categorical perspective of the information in the datasets to give a framework for explainability. In order to achieve this, we are going to define the enriched categories, with decorated morphisms, LearnLearn , ParaPara and MNetMNet of learners, parameterized functions, and neural networks over metric spaces respectively. The main idea is to encode information from the dataset via categorical methods, see how it propagates, and lastly, interpret the results thanks again to categorical (metric) information. This means that we can attach numerical (computable) information via enrichment to the structural information of the category. With this, we can translate theoretical information into parameters that are easily understandable. We will make use of different categories of enrichment to keep track of different kinds of information. That is, to see how differences in attributes of the data are modified by the algorithm to result in differences in the output to achieve better separation. In that way, the categorical framework gives us an algorithm to interpret what the learning algorithm is doing. Furthermore, since it is designed with generality in mind, it should be applicable in various different contexts. There are three main properties of category theory that help with the interpretability of ML models: formality, the existence of universal properties, and compositionality. The last property offers a way to combine smaller, simpler models that are easily understood to build larger ones. This is achieved by formally representing the structure of ML algorithms and information contained in the model. Finally, universal properties are a cornerstone of category theory. They help us characterize an object, not by its attributes, but by how it interacts with other objects. Thus, we can formally characterize an algorithm by how it interacts with the data. The main advantage of the framework is that it can unify under the same language different techniques used in XAI. Thus, using the same language and concepts we can describe a myriad of techniques and properties of ML algorithms, streamlining their explanation and making them easier to generalize and extrapolate.\n",
            "----\n",
            "Paper 172:\n",
            "Title: A survey on artificial intelligence techniques for security event correlation: models, challenges, and opportunities\n",
            "Abstract: None\n",
            "----\n",
            "Paper 173:\n",
            "Title: TRACING THE EVOLUTION OF AI AND MACHINE LEARNING APPLICATIONS IN ADVANCING MATERIALS DISCOVERY AND PRODUCTION PROCESSES\n",
            "Abstract: This research paper examines the transformative role of artificial intelligence (AI) and machine learning (ML) in advancing materials discovery and production processes. The paper explores the historical evolution of AI and ML techniques, their application in materials science, challenges and limitations, emerging technologies, and ethical considerations. Key findings highlight how AI and ML accelerate materials discovery, optimize production processes, and enhance quality control. Emerging technologies such as generative models, reinforcement learning, and AI integration with experimental techniques are discussed. Ethical considerations encompass data privacy, intellectual property, job displacement, bias mitigation, transparency, and human-AI collaboration. The implications for the future underscore the profound impact of AI and ML on materials science, enabling faster discovery, efficient production, and novel material development. \n",
            "Keywords: Artificial Intelligence, Machine Learning, Materials Discovery, Materials Production, Generative Models, Reinforcement Learning, Data Privacy, Ethical Considerations.\n",
            "----\n",
            "Paper 174:\n",
            "Title: Exploring new depths: Applying machine learning for the analysis of student argumentation in chemistry\n",
            "Abstract: Constructing arguments is essential in science subjects like chemistry. For example, students in organic chemistry should learn to argue about the plausibility of competing chemical reactions by including various sources of evidence and justifying the derived information with reasoning. While doing so, students face significant challenges in coherently structuring their arguments and integrating chemical concepts. For this reason, a reliable assessment of students' argumentation is critical. However, as arguments are usually presented in open‐ended tasks, scoring assessments manually is resource‐consuming and conceptually difficult. To augment human diagnostic capabilities, artificial intelligence techniques such as machine learning or natural language processing offer novel possibilities for an in‐depth analysis of students' argumentation. In this study, we extensively evaluated students' written arguments about the plausibility of competing chemical reactions based on a methodological approach called computational grounded theory. By using an unsupervised clustering technique, we sought to evaluate students' argumentation patterns in detail, providing new insights into the modes of reasoning and levels of granularity applied in students' written accounts. Based on this analysis, we developed a holistic 20‐category rubric by combining the data‐driven clusters with a theory‐driven framework to automate the analysis of the identified argumentation patterns. Pre‐trained large language models in conjunction with deep neural networks provided almost perfect machine‐human score agreement and well‐interpretable results, which underpins the potential of the applied state‐of‐the‐art deep learning techniques in analyzing students' argument complexity. The findings demonstrate an approach to combining human and computer‐based analysis in uncovering written argumentation.\n",
            "----\n",
            "Paper 175:\n",
            "Title: Machine Learning for Credit Risk Prediction: A Systematic Literature Review\n",
            "Abstract: In this systematic review of the literature on using Machine Learning (ML) for credit risk prediction, we raise the need for financial institutions to use Artificial Intelligence (AI) and ML to assess credit risk, analyzing large volumes of information. We posed research questions about algorithms, metrics, results, datasets, variables, and related limitations in predicting credit risk. In addition, we searched renowned databases responding to them and identified 52 relevant studies within the credit industry of microfinance. Challenges and approaches in credit risk prediction using ML models were identified; we had difficulties with the implemented models such as the black box model, the need for explanatory artificial intelligence, the importance of selecting relevant features, addressing multicollinearity, and the problem of the imbalance in the input data. By answering the inquiries, we identified that the Boosted Category is the most researched family of ML models; the most commonly used metrics for evaluation are Area Under Curve (AUC), Accuracy (ACC), Recall, precision measure F1 (F1), and Precision. Research mainly uses public datasets to compare models, and private ones to generate new knowledge when applied to the real world. The most significant limitation identified is the representativeness of reality, and the variables primarily used in the microcredit industry are data related to the Demographic, Operation, and Payment behavior. This study aims to guide developers of credit risk management tools and software towards the existing ability of ML methods, metrics, and techniques used to forecast it, thereby minimizing possible losses due to default and guiding risk appetite.\n",
            "----\n",
            "Paper 176:\n",
            "Title: Knowledge mining and social dangerousness assessment in criminal justice: metaheuristic integration of machine learning and graph-based inference\n",
            "Abstract: None\n",
            "----\n",
            "Paper 177:\n",
            "Title: Utilising a Machine Learning Model with Feature Selection from the Salp Swarm Algorithm for Automated Spam Detection\n",
            "Abstract: Automated spam detection is a prevalent use case that involves the filtration and identification of unsolicited or undesirable messages, specifically spam comments or spam emails With the advent of mass mailing technology came an explosion of spam, and spam detection systems became an absolute must for keeping up with the problem. In order to identify spam using machine learning (ML), it is necessary to build a model that can separate incoming messages into two groups: spam and legitimate communications. This categorization relies on detecting specific traits and patterns derived from an appropriately annotated dataset. The dataset has provided these traits and trends. A number of approaches based on machine learning algorithms have been proposed for use in spam identification. Improving spam detection rates while lowering processing cost was the goal of developing algorithms for feature selection and parameter optimisation. The model described in this article is Auto-Spam Detection with Salp Swarm Algorithm based Feature Selection with Machine Learning (ASD-SSFSML). In order to detect and categorise spam, the ASD-SSFSML methodology employs feature selection and classification methods. The utilisation of technology allows for this to be achieved. This goal is ultimately achieved by employing the ASD-SSFSML technique. Improving Mini Batch K-Means Normalised Mutual Information enables feature extraction and optimum centroid selection with the use of the Fire Hawk Optimizer (FHO) algorithm. Applying the technique allows one to achieve this goal. In addition, the Salp Swarm (SS) method, when used for feature selection, improves classification accuracy while complicating training. More specifically, we use the Radial Bias Neural Network (RBNN) classifier to hunt for spammy emails. To ensure that the ASD-SSFSML method would yield superior outcomes, a comprehensive experimental validation process is executed. According to the results of the comparison, the ASD-SSFSML model is far more advanced than other, more recent models.\n",
            "----\n",
            "Paper 178:\n",
            "Title: What Information on Volatile Organic Compounds Can Be Obtained from the Data of a Single Measurement Site Through the Use of Artificial Intelligence?\n",
            "Abstract: None\n",
            "----\n",
            "Paper 179:\n",
            "Title: Special issue: Recent advances in deep learning, biometrics, health informatics and data science\n",
            "Abstract: Deep learning is a growing scientific research trend in machine learning and artificial intelligence due to its better performance compared to other machine learning techniques. This special issue focuses on recent advances in deep learning for human health care applications such as biometrics, medical imaging, and data science. articles reviewed Bhurane, Dhok, Sharma, Yuvaraj, Murugappan and Acharya, ‘ Diagnosis of Parkinson's disease from electroencephalography signals using lin-ear and self-similarity features ’ . In this paper, the authors propose a natural (time) domain technique for diagnosing Parkinson's disease (PD). The presented computer-aided diagnosis system can act as an assistive tool to confirm the finding of PD for the clinicians. They demonstrate that using the support vector machines (SVM) classifier, the feature ranking, and the principal component analysis technique, the proposed system can detect the PD signals automatically with maximum accuracy of 99.1% ± 0.1%. Khan, Sharif, Raza, Anjum, Saba and Shad, ‘ Skin lesion segmentation and classification: A unified framework of deep neural network features fusion and selection. ’ This paper addresses the problem of automated skin lesion diagnosis from dermoscopic images overcoming challenges such as hairs, irregularities, lesion shape, and irrelevant feature extraction. The authors propose a hybrid approach combining optimized colour feature of lesion segmentation improved by an existing saliency approach fused with a novel pixel-based method and deep convolutional neural network (DCNN)-based skin lesion classification. Experimental results of the proposed approach demonstrate remarkable performance on three different datasets. Sivan, Sellappa and Peter J, ‘ Proximity-based cloud resource provisioning for deep learning applications in smart healthcare. ’ Health profes-sionals can use smart mobile devices to convey recordings of patients and use machine learning-based approaches to process results and get pre-dictions through smart mobile healthcare applications. Due to the nature of deep learning techniques, learning and prediction processes are moved to the cloud. This paper proposes a proximity-based resource provisioning technique that guarantees minimal delay in obtaining inference results with a local mobile cloud system. The authors implemented a healthcare cloud-based system that outperforms the state-of-the-art methods in terms of response time, deadline meeting percentage and system utilization. ‘ A multilevel paradigm for deep convolutional neural network features selection with an application to human gait recognition. ’ This paper proposes an integrated framework for human gait recognition using deep neural network features fusion and fuzzy entropy controlled skewness approach for best feature selection. Pre-trained CNN models (VGG19 and AlexNet) are used, and their information is mixed by the parallel fusion approach. Remarkable results on four gait analysis datasets show that the fusion of multiple CNN frameworks improves the recognition accuracy and the selection of the best features enhances the system accuracy and even minimizes the execution time. from used for propose novel selection use the genetic algorithm to determine the hyper-parameters of the SVM kernels. The system with the proposed approach demonstrates high accuracy for the stenosis diagnosis of each main coronary artery, which can help the clinicians validate their manual stenosis diagnosis of coronary artery (RCA), coronary artery (RCA), circumflex (LCX) and artery and left anterior descending (LAD) coronary arteries. The results show that discretization and assurance feature selection can significantly improve the efficiency of classification algorithms. retrieving large due variations in the size shape for image retrieval that are relevant to a given query image. Various features such as colour, shape, and texture are exploited using the K-nearest neighbour algorithm to find the minimum distance between query and database images. The authors focus on the application of retrieving the brain MRI images of different planes (coronal, sagittal and transverse) from a dataset of normal and demented subjects. The results demonstrate high accuracy of 95%. Such a tool can be helpful in radiology image retrieval and classification.\n",
            "----\n",
            "Paper 180:\n",
            "Title: Neural Decoding of EEG Signals with Machine Learning: A Systematic Review\n",
            "Abstract: Electroencephalography (EEG) is a non-invasive technique used to record the brain’s evoked and induced electrical activity from the scalp. Artificial intelligence, particularly machine learning (ML) and deep learning (DL) algorithms, are increasingly being applied to EEG data for pattern analysis, group membership classification, and brain-computer interface purposes. This study aimed to systematically review recent advances in ML and DL supervised models for decoding and classifying EEG signals. Moreover, this article provides a comprehensive review of the state-of-the-art techniques used for EEG signal preprocessing and feature extraction. To this end, several academic databases were searched to explore relevant studies from the year 2000 to the present. Our results showed that the application of ML and DL in both mental workload and motor imagery tasks has received substantial attention in recent years. A total of 75% of DL studies applied convolutional neural networks with various learning algorithms, and 36% of ML studies achieved competitive accuracy by using a support vector machine algorithm. Wavelet transform was found to be the most common feature extraction method used for all types of tasks. We further examined the specific feature extraction methods and end classifier recommendations discovered in this systematic review.\n",
            "----\n",
            "Paper 181:\n",
            "Title: Emerging Directions for Carbon Capture Technologies: A Synergy of High-Throughput Theoretical Calculations and Machine Learning.\n",
            "Abstract: As the world grapples with the challenges of energy transition and industrial decarbonization, the development of carbon capture technologies presents a promising solution. The Scalable Modeling, Artificial Intelligence (AI), and Rapid Theoretical calculations, referred as SMART here, is an interdisciplinary approach that combines high-throughput calculation and data-driven modeling with expertise from chemical, materials, environmental, computer and data science and engineering, leading to the development of advanced capabilities in simulating and optimizing carbon capture processes. This perspective discusses the state-of-the-art material discovery research enabled by high-throughput calculation and data-driven modeling. Further, we propose a framework for material discovery, and illustrate the synergies among deep learning models, pretrained models, and comprehensive data sets, emerging as a robust framework for data-driven design and development in carbon capture. In essence, the adoption of the SMART approach promises a revolutionary impact on efforts in energy transition and industrial decarbonization.\n",
            "----\n",
            "Paper 182:\n",
            "Title: Heart Failure Prediction Using Artificial Intelligence Methods\n",
            "Abstract: Heart disease is a significant global health concern, and accurate diagnosis is essential for the effective treatment. In this study, we focus on utilizing the Support Vector Machine (SVM) algorithm with the radial basis function (RBF) kernel to develop a heart disease classification model. The SVM model with the RBF kernel achieves an accuracy of 91.85%, with precision, recall, and F1-score metrics supporting the model's ability to correctly identify positive instances. To support our results, a 5-mean clustering method classified the data. We apply K-means clustering analysis method to reveal hidden patterns within the data. K-means clustering is an unsupervised learning technique that allows the algorithm to process unlabeled and unclassified data independently. The dataset is meticulously preprocessed to handle missing values, categorical variables, and feature scaling, followed by feature extraction to optimize clustering performance. The application of K-means clustering offers valuable insights into potential heart disease subgroups, supporting early detection and personalized care strategies with 84% accuracy.\n",
            "----\n",
            "Paper 183:\n",
            "Title: Biomonitoring and precision health in deep space supported by artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 184:\n",
            "Title: How can Automated Machine Learning Help Business Data Science Teams?\n",
            "Abstract: Artificial intelligence and machine learning have attracted the attention of many commercial and non-profit organizations aiming to leverage advanced analytics, in order to provide a better service to their customers, increase their revenues through creating new or improving their existing internal processes, and better exploit their data by discovering complex hidden patterns. Such advanced solutions require data scientists with rare (and generally expensive) skill sets. Moreover, such solutions are often perceived as complex black boxes to decision-makers. Automated machine learning tools aim to reduce the expertise gap between the technical teams and stakeholders involved in business data science projects, by reducing the amount of time and specialized skills required to generate predictive models. We systematically benchmarked five automated machine learning tools against seven supervised learning problems of a business nature. Our results suggest that such tools, in fully automated mode, must be used cautiously, only where predictive models support low-impact decisions and do not need to be explainable, and only by data scientists capable to ensure that all phases of the data mining process have been performed adequately.\n",
            "----\n",
            "Paper 185:\n",
            "Title: Materials information extraction via automatically generated corpus\n",
            "Abstract: None\n",
            "----\n",
            "Paper 186:\n",
            "Title: Using Federated Artificial Intelligence System of Intrusion Detection for IoT Healthcare System Based on Blockchain\n",
            "Abstract: Recently Internet of things (IoT)-based healthcare system has expanded significantly, however, they are restricted by the absence of an intrusion detection mechanism (IDS). Modern technologies like blockchain (BC), edge computing (EC), and machine learning (ML) provide a robust security solution that is well-suited to protecting patients' medical information. In this study, we offer an intelligent intrusion detection mechanism FIDANN that protects the confidentiality of medical data by completing the intrusion detection task by utilising Dwarf mongoose-optimized artificial neural networks (DMO-ANN) through a federated learning (FL) technique. In the context of recent developments in blockchain technology, such as the elimination of contaminating attacks and the provision of complete visibility and data integrity over the decentralized system with minimal additional effort. Using the model at the edges secures the cloud from attacks by limiting information from its gateway with less computing time and processing power as FL works with fewer datasets. The findings demonstrate that our suggested models perform better when dealing with the diversity of data produced by IoT devices.\n",
            "----\n",
            "Paper 187:\n",
            "Title: Geospatial Artificial Intelligence: Potentials of Machine Learning for 3D Point Clouds and Geospatial Digital Twins\n",
            "Abstract: None\n",
            "----\n",
            "Paper 188:\n",
            "Title: Car Crash Detection System using Machine Learning and Deep Learning Algorithm\n",
            "Abstract: Over 80% of mishaps are caused by a lack of identifying the accident on time, as well as failure to arrive in time to provide emergency care for the victim. The point is to distinguish and utilize machine learning to decide the best means of detecting car crash with light of the live transfer of dash cam data in the vehicle. The thought is to take every pixel and run it with a deep learning model prepared to recognize video outlines into mishap or non-mishap. Essentially, in Artificial Intelligence (AI), informational indexes are collected. Gathered information bases will be refined and given to AI calculations to prepare for image recognition with the help of computer vision. After fruitful preparation, AI calculations will be tried and the outcomes will be recorded for examination purposes. In the proposed vehicle crash location framework, the impact identification is performed utilizing the Convolutional Neural Network (CNN), taking a bunch of pictures as information, the framework distinguishes the crash, the effect of the vehicle and the greatness of the mishap. Based on the performance of machine learning algorithms, comparative analysis is performed and the results will be tabulated. Two machine learning algorithms are considered i.e. Random Forest Classifier and Logistic Regression which enables the output regarding the generated index from the CNN and running through the indices with location impact and severity of the damage.\n",
            "----\n",
            "Paper 189:\n",
            "Title: Machine learning \\& artificial intelligence in the quantum domain\n",
            "Abstract: Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our \"big data\" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.\n",
            "----\n",
            "Paper 190:\n",
            "Title: ReactionDataExtractor 2.0: A Deep Learning Approach for Data Extraction from Chemical Reaction Schemes\n",
            "Abstract: Knowledge in the chemical domain is often disseminated graphically via chemical reaction schemes. The task of describing chemical transformations is greatly simplified by introducing reaction schemes that are composed of chemical diagrams and symbols. While intuitively understood by any chemist, like most graphical representations, such drawings are not easily understood by machines; this poses a challenge in the context of data extraction. Currently available tools are limited in their scope of extraction and require manual preprocessing, thus slowing down the speed of data extraction. We present a new tool, ReactionDataExtractor v2.0, which uses a combination of neural networks and symbolic artificial intelligence to effectively remove this barrier. We have evaluated our tool on a test set composed of reaction schemes that were taken from open-source journal articles and realized F1 score metrics between 75 and 96%. These evaluation metrics can be further improved by tuning our object-detection models to a specific chemical subdomain thanks to a data-driven approach that we have adopted with synthetically generated data. The system architecture of our tool is modular, which allows it to balance speed and accuracy to afford an autonomous, high-throughput solution for image-based chemical data extraction.\n",
            "----\n",
            "Paper 191:\n",
            "Title: Predicting Student Success Using Big Data and Machine Learning Algorithms\n",
            "Abstract: The prediction of student performance, allows teachers to track student results to react and make decisions that affect their learning and performance, given the importance of monitoring students to fight against academic failure. We realized a system of the prediction of academic success and failure of the students, which is the overall result and the goal of the educational system. We used the personal information of the students, the academic evaluation, the activities of the students in VLE, ​​Psychological, the student environment, and we added practical work and homework, mini projects, and the number of student absences which gives a vision of the quality of the student. Then we applied the methods of artificial intelligence and educational Data mining such as KNN, C4.5 and SVM for the prediction of the academic success of students, but these methods are not sufficient given the progressive number of students, specialties, learning methods and the diversity of data sources as well as student data processing time. To solve this problem, Big Data technology was used to distribute the processing in order to minimize the execution time without losing the efficiency of the algorithms used. In this system we cleaned the data and then applied the property selection algorithms to find the useful properties in order to improve the algorithm prediction rate and also to reduce the execution time. Finally, we stored the data in HDFS and we applied the classification algorithms for the prediction of student success using MAPREDUCE. We compared the results before and after the use of big data and we found that the results after the use of Big Data are very good at execution time and we arrived at a recognition rate of 87.32% by the SVM algorithm.\n",
            "----\n",
            "Paper 192:\n",
            "Title: A Machine Learning Technique for Detection of Diabetes Mellitus\n",
            "Abstract: The need for early detection of diabetes mellitus has led to the development of various intelligent systems using machine learning and artificial intelligence for the recognition of the presence of the disease. However, most of the techniques have yielded a comparatively lower accuracy. This research applied data science techniques to a dataset of diabetes mellitus to improve the accuracy of the prediction of the disease. This was achieved by pre-processing the data with dummy categories and applying principal components analysis for reduced dimensionality. Support vector machine, random forest classifier, and deep neural networks were then used to train the system. Support vector machine, random forest classifier, and deep neural networks yielded accuracies of 0.76, 0.77, and 0.89 respectively. Correspondingly, deep neural networks yielded the highest accuracy. The study concluded that better pre-processing will improve the accuracy of machine learning algorithms in the prediction of diabetes mellitus.\n",
            "----\n",
            "Paper 193:\n",
            "Title: Implementation of Grey Scale Normalization in Machine Learning & Artificial Intelligence for Bioinformatics using Convolutional Neural Networks\n",
            "Abstract: Machine Learning is a trending field nowadays and is very well known as an application of Artificial Intelligence (AI). Machine learning makes use of secure arithmetical algorithms to construct computers assignment in a constructive way without being unequivocally programmed. The algorithms acquire freedom of a participated value and estimate output for this by utilizing definite arithmetical methods. The key function of machine learning is to create smart machines that can visualize and work related to human beings. Artificial Intelligence has been witnessing a massive improvement in bridging the gap among the capabilities of humans and technologies. Similarly, one of the characteristics of the field were tried to combine the outstanding effects. A Convolution Neural Network (CNN) is a sort of Deep Learning algorithm which can obtain an input image, assign consequence to different aspects in the image and capable to differentiate one from the other. The pre-processing requirement in a CNN is lesser as compared to other classification algorithms. CNN has the capability to find out these filter's characteristics. Bioinformatics is a term that is a mixture of two terms bio and informatics. Bio means associated with biology and informatics means in a sequence of information. Thus bioinformatics is an area that deals with handing out and accepting biological statistics using the computational and arithmetical approaches. Machine Learning has added up to of applications in the area of bioinformatics. Machine Learning finds its submission in the subsequent subfields of bioinformatics. The aim of this article is to perform a grayscale normalization of a selected image and thereafter to reduce the effect of illumination differences. Normalization is considered so that CNN works in a faster manner. Different models are available but Keras model is selected to perform this task. Keras supports the style of data preparation for Image statistics via the Image Data Generator group and Application Programming Interface (API). The Image Data Generator group in Keras provides a matching set of techniques for scaling pixel standards in the image dataset subsequent to modeling. The Keras functional API provides an additional flexible approach for significant models. It particularly allows Identifving several input or output models as well as models that allocate layers.\n",
            "----\n",
            "Paper 194:\n",
            "Title: Integrating Machine Learning in Business Decision Making: Application and Future directions\n",
            "Abstract: The copious amount of data generated as a result of Industry 4.0 revolution across every domain including Internet of things (IoT) data, cybersecurity data, business data, social data, and medical data has opened new avenues and business opportunities for organizations. The information imbibed in the data should be intelligently analysed to reap tangible benefits that can improve the efficiency of organizations. The knowledge of Artificial Intelligence (AI), and particularly Machine Learning (ML) algorithms is the requisite to decipher meanings and patterns in the data to gain useful insights. Multiple ML algorithms namely supervised, unsupervised, semi-supervised, and reinforcement learning exists in this field. In this study, an attempt has been made to provide a comprehensive understanding of various ML algorithms that can be applied to enhance the intelligence of the business operations thereby leading to effective solutions for various business problems. The potential contribution of this study is to understand different ML techniques and its applications in various real world business problems such as e-commerce, manufacturing, healthcare, etc. The major challenges and future research directions are presented in the study to further the research in the domain of ML. This study can be used as a reference to gain information about various prominent ML algorithms that have maximum utility in the business domain. Further, this study can assist researchers and practitioners in extending the knowledge to other business problems and aid in problem solving.\n",
            "----\n",
            "Paper 195:\n",
            "Title: Machine learning and artificial neural network accelerated computational discoveries in materials science\n",
            "Abstract: Artificial intelligence (AI) has been referred to as the “fourth paradigm of science,” and as part of a coherent toolbox of data‐driven approaches, machine learning (ML) dramatically accelerates the computational discoveries. As the machinery for ML algorithms matures, significant advances have been made not only by the mainstream AI researchers, but also those work in computational materials science. The number of ML and artificial neural network (ANN) applications in the computational materials science is growing at an astounding rate. This perspective briefly reviews the state‐of‐the‐art progress in some supervised and unsupervised methods with their respective applications. The characteristics of primary ML and ANN algorithms are first described. Then, the most critical applications of AI in computational materials science such as empirical interatomic potential development, ML‐based potential, property predictions, and molecular discoveries using generative adversarial networks (GAN) are comprehensively reviewed. The central ideas underlying these ML applications are discussed, and future directions for integrating ML with computational materials science are given. Finally, a discussion on the applicability and limitations of current ML techniques and the remaining challenges are summarized.\n",
            "----\n",
            "Paper 196:\n",
            "Title: Artificial intelligence in the creative industries: a review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 197:\n",
            "Title: Artificial Intelligence Techniques for SQL Injection Attack Detection\n",
            "Abstract: In recent years, web-based platforms and business applications have been rising in popularity deeming themselves indispensable as they constitute the main backbone of business processes and information sharing. However, the unprecedented increased number of cyber-attacks have been threatening their day-to-day operations. In particular, the Standard Query Language Injection Attack (SQLIA) remains one of the most prevalent cyber attacks targeting web-based applications. As a consequence, the SQLIA detection techniques need to be constantly revamped and stay up-to-date in order to achieve the full potential of mitigating such threats. In this paper, we propose an artificial intelligence model based on supervised machine learning techniques to detect SQLIA. As part of the proposed model, we introduce an input string validation technique as a primary anomaly identifier using pattern matching for SQL Query data with anomalies-injections. To evaluate our approach we injected one type of SQLIA that is tautology attacks and measured the performance of our model. We used three main classifiers in our model and our findings indicate a model prediction accuracy of 98.3605% for Support Vector Machine (SVM), 96.296% for K-Nearest Neighbors (KNN), and 97.530% for Random Forest. The approach proposed in this paper has the potential of being used to integrate an automated SQL Injection detection mechanism with Intrusion Detection Systems (IDS) and Intrusion Protection Systems (IPS).\n",
            "----\n",
            "Paper 198:\n",
            "Title: A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases\n",
            "Abstract: None\n",
            "----\n",
            "Paper 199:\n",
            "Title: Traffic Prediction for Intelligent Transportation Systems using Machine Learning\n",
            "Abstract: Over the past few decades, ITS have spiked an increasing research interest as a promising discipline for revolutionizing the\n",
            "transportation sector and solving common traffic and vehicle-related problems. ITS comprise a multitude of interconnected\n",
            "engineering feats that function as an entity for optimizing network-scale travel experiences from a technical, social, economic,\n",
            "and environmental aspect. Such optimizations necessitate the advancement of information and communication technologies,\n",
            "electronic sensors, control systems, and computers, which high-lights the data-driven nature of modern ITS.\n",
            "In this paper we design a system which uses machine learning algorithm using SVM, KNN and CNN algorithm which is a novel\n",
            "system which will provide intelligence to the current traffic control system present at a four-way junction. This ML technique is\n",
            "mainly aimed to replace the existing traffic light control system with artificial intelligence system. Nowadays most cities are\n",
            "equipped with CCTV cameras on the roads and the junctions, the basic idea is to collect the live video from the CCTV cameras and\n",
            "detect the number of vehicles on each lane and feed the data into another machine learning algorithm. according to the data of\n",
            "each lane changes into the light phase of the green signal. This system mainly aims to increase the traffic efficiency by increasing\n",
            "vehicle flow which will reduce waiting time for the vehicles. We are using HOG algorithm for feature extraction. In the\n",
            "implementation of the proposed architecture, we have achieved an accuracy of 86.34% for binary classification and 90.23% for\n",
            "multi-class classification\n",
            "----\n",
            "Paper 200:\n",
            "Title: Artificial intelligence with multi-functional machine learning platform development for better healthcare and precision medicine\n",
            "Abstract: Abstract Precision medicine is one of the recent and powerful developments in medical care, which has the potential to improve the traditional symptom-driven practice of medicine, allowing earlier interventions using advanced diagnostics and tailoring better and economically personalized treatments. Identifying the best pathway to personalized and population medicine involves the ability to analyze comprehensive patient information together with broader aspects to monitor and distinguish between sick and relatively healthy people, which will lead to a better understanding of biological indicators that can signal shifts in health. While the complexities of disease at the individual level have made it difficult to utilize healthcare information in clinical decision-making, some of the existing constraints have been greatly minimized by technological advancements. To implement effective precision medicine with enhanced ability to positively impact patient outcomes and provide real-time decision support, it is important to harness the power of electronic health records by integrating disparate data sources and discovering patient-specific patterns of disease progression. Useful analytic tools, technologies, databases, and approaches are required to augment networking and interoperability of clinical, laboratory and public health systems, as well as addressing ethical and social issues related to the privacy and protection of healthcare data with effective balance. Developing multifunctional machine learning platforms for clinical data extraction, aggregation, management and analysis can support clinicians by efficiently stratifying subjects to understand specific scenarios and optimize decision-making. Implementation of artificial intelligence in healthcare is a compelling vision that has the potential in leading to the significant improvements for achieving the goals of providing real-time, better personalized and population medicine at lower costs. In this study, we focused on analyzing and discussing various published artificial intelligence and machine learning solutions, approaches and perspectives, aiming to advance academic solutions in paving the way for a new data-centric era of discovery in healthcare.\n",
            "----\n",
            "Paper 201:\n",
            "Title: Artificial Intelligence Data Science Methodology for Earth Observation\n",
            "Abstract: This chapter describes a Copernicus Access Platform Intermediate Layers Small-Scale Demonstrator, which is a general platform for the handling, analysis, and interpretation of Earth observation satellite images, mainly exploiting big data of the European Copernicus Programme by artificial intelligence (AI) methods. From 2020, the platform will be applied at a regional and national level to various use cases such as urban expansion, forest health, and natural disasters. Its workflows allow the selection of satellite images from data archives, the extraction of useful information from the metadata, the generation of descriptors for each individual image, the ingestion of image and descriptor data into a common database, the assignment of semantic content labels to image patches, and the possibility to search and to retrieve similar content-related image patches. The main two components, namely, data mining and data fusion, are detailed and validated. The most important contributions of this chapter are the integration of these two components with a Copernicus platform on top of the European DIAS system, for the purpose of large-scale Earth observation image annotation, and the measurement of the clustering and classification performances of various Copernicus Sentinel and third-party mission data. The average classification accuracy is ranging from 80 to 95% depending on the type of images.\n",
            "----\n",
            "Paper 202:\n",
            "Title: Leakage Prediction in Machine Learning Models When Using Data from Sports Wearable Sensors\n",
            "Abstract: One of the major problems in machine learning is data leakage, which can be directly related to adversarial type attacks, raising serious concerns about the validity and reliability of artificial intelligence. Data leakage occurs when the independent variables used to teach the machine learning algorithm include either the dependent variable itself or a variable that contains clear information that the model is trying to predict. This data leakage results in unreliable and poor predictive results after the development and use of the model. It prevents the model from generalizing, which is required in a machine learning problem and thus causes false assumptions about its performance. To have a solid and generalized forecasting model, which will be able to produce remarkable forecasting results, we must pay great attention to detecting and preventing data leakage. This study presents an innovative system of leakage prediction in machine learning models, which is based on Bayesian inference to produce a thorough approach to calculating the reverse probability of unseen variables in order to make statistical conclusions about the relevant correlated variables and to calculate accordingly a lower limit on the marginal likelihood of the observed variables being derived from some coupling method. The main notion is that a higher marginal probability for a set of variables suggests a better fit of the data and thus a greater likelihood of a data leak in the model. The methodology is evaluated in a specialized dataset derived from sports wearable sensors.\n",
            "----\n",
            "Paper 203:\n",
            "Title: Artificial Intelligence-based Learning Techniques for Diabetes Prediction: Challenges and Systematic Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 204:\n",
            "Title: Artificial intelligence for decision support in surgical oncology - a systematic review\n",
            "Abstract: Aim: We systematically review current clinical applications of artificial intelligence (AI) that use machine learning (ML) methods for decision support in surgical oncology with an emphasis on clinical translation. Methods: MEDLINE, Web of Science, and CENTRAL were searched on 19 January 2021 for a combination of AI and ML-related terms, decision support, and surgical procedures for abdominal malignancies. Data extraction included study characteristics, description of algorithms and their respective purpose, and description of key steps for scientific validation and clinical translation. Results: Out of 8302 articles, 107 studies were included for full-text analysis. Most of the studies were conducted in a retrospective setting (n = 105, 98%), with 45 studies (42%) using data from multiple centers. The most common tumor entities were colorectal cancer (n = 35, 33%), liver cancer (n = 21, 20%), and gastric cancer (n = 17, 16%). The most common prediction task was survival (n = 36, 34%), with artificial neural networks being the most common class of ML algorithms (n = 52, 49%). Key reporting and validation steps included, among others, a complete listing of patient features (n = 95, 89%), training of multiple algorithms (n = 73, 68%), external validation (n = 13, 12%), prospective validation (n = 2, 2%), robustness in terms of cross-validation or resampling (n = 89, 83%), treatment recommendations by ML algorithms (n = 9, 8%), and development of an interface (n = 12, 11%). Conclusion: ML for decision support in surgical oncology is receiving increasing attention with promising results, but robust and prospective clinical validation is mostly lacking. Furthermore, the integration of ML into AI applications is necessary to foster clinical translation.\n",
            "----\n",
            "Paper 205:\n",
            "Title: Digital Marketing in the Artificial Intelligence and Machine Learning Age\n",
            "Abstract: We are living in a period of profound change driven by digitization, information and communication technology, artificial intelligence, machine learning, and robotics (Gupta, Keen, Shah, and Verdier, 2017; Wang and Siau, 2019). Traditional marketing is shifting to digital marketing enabled by AI and machine learning. Customer consumption behavior has changed from traditional in-store shopping to online shopping (Thiraviyam, 2018). The large volume of transaction and demographic data enables business analytics, AI, and machine learning to analyze and predict customer behavior to improve customer satisfaction and enhance sales (Siau and Wang, 2018). For example, predictive analytics uses different algorithms to predict the relationship between results and variables and to identify data patterns (Turban, Sharda and Delen, 2010). Marketers use data mining techniques to analyze the data trend to predict customer consumption hobbies. With AI and machine learning, marketers can now automate the pattern searching and identify processes to enable personalized and one-to-one digital marketing to deliver individualized messages and product offerings to existing and new customers. This research will focus on AI and machine learning effort on digital marketing and customer behavior. This research involves both interview and survey. This qualitative study will examine how marketers are capitalizing on the capabilities of AI and machine learning to predict customer behavior, offer one-to-one digital marketing, enhance sales, and increase customer satisfaction. Interviews will be conducted with executives from digital marketing companies and traditional brick and mortar companies that are embracing digital marketing. Survey will be used in these companies to capture data from digital marketing and sales personnel. Studying the impact of AI and machine learning on digital marketing and customer behavior is of great significance to the marketing and sales industry. The results from this research will provide both academics and practitioners a deeper understanding of the use and impact of AI and machine learning in digital marketing.\n",
            "----\n",
            "Paper 206:\n",
            "Title: Machine Learning and Data Analytics for Environmental Science: A Review, Prospects and Challenges\n",
            "Abstract: Innovations in Machine Learning and Data Analytics can possibly affect numerous aspects of Environmental Science (ES). Data Analytics refers to a collection of data resources indicated in terms of variety, velocity, veracity and volume. Big data contributes to the ES arena in applications such as weather forecasting, energy sustainability and disaster management with the advent of techniques such as Remote Sensing, Information and Communication technologies. Though big data is used to accomplish data analysis and interpretation for ES, there are still requirements for efficient ways of data storage, processing and retrieval. Machine Learning and Deep Learning are the sub fields of artificial intelligence which deals with training the models to learn from data without being explicitly programmed. When Machine Learning and Deep Learning are combined together it is possible to unleash the supremacy of data analytics. These techniques show high prospective for process optimization, information-centric decision making and scientific discovery. Scientific developments like these will assist ES to make real time autonomous decisions by extracting useful insights from huge data. These advancements also aid in bridging the gap between the theoretical backgrounds on ES to practical implementation. The primary objective of this survey is to figure out the basic concepts of Machine Learning, Deep Learning, and Data Analytics and find the state-of-the-art applications in ES, and observe the impending benefits of information-centric investigation on ES.\n",
            "----\n",
            "Paper 207:\n",
            "Title: Artificial Intelligence Powered Early Detection of Heart Disease\n",
            "Abstract: In the healthcare industry, Machine Learning (ML) plays a crucial role in disease prediction. A patient must go through a series of tests before a condition can be diagnosed. However, using machine learning techniques, the number of tests can be reduced. This simplified test has a significant impact on both time and performance. Early patient care has benefited from sound medical data analysis due to the growing amount of data generated by the medical and healthcare sectors. With the help of disease data, massive amounts of medical data can be mined for hidden pattern information. With a focus on heart diseases, this study evaluates and suggests a heart disease prediction based on the patient's symptoms using machine learning techniques such as SVM, MLR, and RF algorithms. The proposed method outperforms those currently in use in terms of accuracy, forecast speed, and consistency of outcomes. It is also appropriate to classify lung cancers using trained datasets for accurate identification.\n",
            "----\n",
            "Paper 208:\n",
            "Title: Analysis of IoT Security Challenges and Its Solutions Using Artificial Intelligence\n",
            "Abstract: The Internet of Things (IoT) is a well-known technology that has a significant impact on many areas, including connections, work, healthcare, and the economy. IoT has the potential to improve life in a variety of contexts, from smart cities to classrooms, by automating tasks, increasing output, and decreasing anxiety. Cyberattacks and threats, on the other hand, have a significant impact on intelligent IoT applications. Many traditional techniques for protecting the IoT are now ineffective due to new dangers and vulnerabilities. To keep their security procedures, IoT systems of the future will need AI-efficient machine learning and deep learning. The capabilities of artificial intelligence, particularly machine and deep learning solutions, must be used if the next-generation IoT system is to have a continuously changing and up-to-date security system. IoT security intelligence is examined in this paper from every angle available. An innovative method for protecting IoT devices against a variety of cyberattacks is to use machine learning and deep learning to gain information from raw data. Finally, we discuss relevant research issues and potential next steps considering our findings. This article examines how machine learning and deep learning can be used to detect attack patterns in unstructured data and safeguard IoT devices. We discuss the challenges that researchers face, as well as potential future directions for this research area, considering these findings. Anyone with an interest in the IoT or cybersecurity can use this website’s content as a technical resource and reference.\n",
            "----\n",
            "Paper 209:\n",
            "Title: The Role of Artificial Intelligence Model Documentation in Translational Science: Scoping Review\n",
            "Abstract: Background Despite the touted potential of artificial intelligence (AI) and machine learning (ML) to revolutionize health care, clinical decision support tools, herein referred to as medical modeling software (MMS), have yet to realize the anticipated benefits. One proposed obstacle is the acknowledged gaps in AI translation. These gaps stem partly from the fragmentation of processes and resources to support MMS transparent documentation. Consequently, the absence of transparent reporting hinders the provision of evidence to support the implementation of MMS in clinical practice, thereby serving as a substantial barrier to the successful translation of software from research settings to clinical practice. Objective This study aimed to scope the current landscape of AI- and ML-based MMS documentation practices and elucidate the function of documentation in facilitating the translation of ethical and explainable MMS into clinical workflows. Methods A scoping review was conducted in accordance with PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. PubMed was searched using Medical Subject Headings key concepts of AI, ML, ethical considerations, and explainability to identify publications detailing AI- and ML-based MMS documentation, in addition to snowball sampling of selected reference lists. To include the possibility of implicit documentation practices not explicitly labeled as such, we did not use documentation as a key concept but as an inclusion criterion. A 2-stage screening process (title and abstract screening and full-text review) was conducted by 1 author. A data extraction template was used to record publication-related information; barriers to developing ethical and explainable MMS; available standards, regulations, frameworks, or governance strategies related to documentation; and recommendations for documentation for papers that met the inclusion criteria. Results Of the 115 papers retrieved, 21 (18.3%) papers met the requirements for inclusion. Ethics and explainability were investigated in the context of AI- and ML-based MMS documentation and translation. Data detailing the current state and challenges and recommendations for future studies were synthesized. Notable themes defining the current state and challenges that required thorough review included bias, accountability, governance, and explainability. Recommendations identified in the literature to address present barriers call for a proactive evaluation of MMS, multidisciplinary collaboration, adherence to investigation and validation protocols, transparency and traceability requirements, and guiding standards and frameworks that enhance documentation efforts and support the translation of AI- and ML-based MMS. Conclusions Resolving barriers to translation is critical for MMS to deliver on expectations, including those barriers identified in this scoping review related to bias, accountability, governance, and explainability. Our findings suggest that transparent strategic documentation, aligning translational science and regulatory science, will support the translation of MMS by coordinating communication and reporting and reducing translational barriers, thereby furthering the adoption of MMS.\n",
            "----\n",
            "Paper 210:\n",
            "Title: Artificial intelligence in environmental monitoring: in-depth analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 211:\n",
            "Title: P-394 Using machine learning to predict the risk of miscarriage in infertile couples undergoing assisted reproductive cycles\n",
            "Abstract: \n",
            " \n",
            " \n",
            " Can an artificial intelligence (AI)-based model predict chemical abortion using basic clinical data of infertile couples undergoing assisted reproductive technology (ART)?\n",
            " \n",
            " \n",
            " \n",
            " We have combined basic clinical data and machine learning algorithms to create a noninvasive AI model for prediction of chemical abortion.\n",
            " \n",
            " \n",
            " \n",
            " Approximately 10 to 15 percent of natural pregnancies encounter the issue of abortion, it significantly gets worse in pregnancies resulting from assisted reproductive methods. The emotional burden on infertile couples during their self-belief therapy journey is substantial. Predicting treatment outcomes can significantly decrease the anxiety experienced by these couples. This research aims to develop a machine learning-based model for predicting chemical abortion in infertile women who have conceived through assisted reproductive methods, offering an initial assessment.\n",
            " \n",
            " \n",
            " \n",
            " This study involved analysis of retrospectively collected records including basic clinical data and pregnancy outcomes of 1234 records between March 2020 and August 2022. It designs in two fundamental phases: data extraction and model implementation. The extracted data from the study group was evaluated with registered physical and electronic records to validate and correct the information.\n",
            " \n",
            " \n",
            " \n",
            " This study was conducted at Yazd Reproductive Sciences Institute, Yazd, Iran. Data pre-processing was performed for data mining and model implementation. This process yielded a dataset of 1234 samples containing 32 selected variables. Machine learning algorithms, including decision tree, random forest, logistic regression, and support vector machine, were employed for models’ assessment. Regarding the imbalance nature of the gathered dataset, balancing techniques were performed to enhance model performance. Balancing was applied using SMOTE, ADYNS algorithms.\n",
            " \n",
            " \n",
            " \n",
            " Evaluation of the above-mentioned models across unbalanced, balanced, and reduced datasets revealed the random forest model as the most robust choice, achieving accuracies of 79.80%, 88.90%, and 89.90% in the respective test phases. In terms of balancing process, ADYNS was superior to SMOTE. In addition, this research identified 18 influential variables for predicting abortion.\n",
            " \n",
            " \n",
            " \n",
            " This investigation was performed using retrospective data; some incomplete records were excluded. In addition, the data heterogenicity may affect the analysis. The model developed is limited to the analysis of basic clinical data, further modification of the model is needed to incorporate information from different stages of the ART cycle.\n",
            " \n",
            " \n",
            " \n",
            " This study demonstrated a model to predict chemical miscarriage. The high accuracy of this model showed that this model could be implemented in ART clinics as a primary tool to assist clinicians in making an accurate initial judgment on the condition of patients with infertility and history of pregnancy loss.\n",
            " \n",
            " \n",
            " \n",
            " not applicable\n",
            "\n",
            "----\n",
            "Paper 212:\n",
            "Title: Evaluation of artificial intelligence techniques in disease diagnosis and prediction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 213:\n",
            "Title: Automated detection of myocardial infarction using ECG-based artificial intelligence models: a systematic review\n",
            "Abstract: Clinical decision making in the emergency room needs to be fast and accurate, especially for myocardial infarction (MI) cases. The best way to address data-based decisions is through artificial intelligence techniques (AI), which haven’t been systematize for MI detection. Thereby, we performed a systematic review (PROSPERO: CRD42021229084). The literature search from Pubmed, Web of Science, Scopus, IEEE Xplore and Embase resulted in n = 48 included articles. 71% of those articles implemented deep-learning models, while the other 29% developed machine-learning models, from which Convolutional Neural Networks and Support Vector Machines were the most common architectures. Data pre-processing methods, ECG-derived features with their corresponding feature extraction techniques, dimensionality reduction and redundancy evaluation algorithms and classifier are discussed in the present work. Furthermore, public and private datasets are analyzed, and class balance is addressed. To the extent of our knowledge, the present work is one of the most comprehensive reviews that addressed systematically the characteristics of artificial intelligence algorithms for the detection of MI based on ECG information.\n",
            "----\n",
            "Paper 214:\n",
            "Title: Artificial Intelligence in Information Technology\n",
            "Abstract: Artificial Intelligence is a broad field that encompasses various concepts in Information Technology. This research paper focuses on different technologies in AI and how they apply to improve the performance of multiple sectors. The purpose of this study is to discuss Artificial Intelligence and its present and future applications. AI is the foundation of multiple concepts, such as computing, software creation, and data transmission. The technologies that use AI are machine learning, deep learning, Natural Language Generation, speech recognition, robotics, and biometric identification. AI applies to many sectors such as healthcare sectors, assembling and manufacturing industries, business organizations, and in the automotive industries. AI also has various advantages that make it gain more popularity in many areas. The AI-powered machine can perform many jobs at once; they are not costly compared to human beings and are accurate and efficient. AI also encounters multiple problems that undermine its application. AI is prone to technical difficulties, security snags, data difficulties, and can cause accidents if users fail to understand the AI system. The increased use of AI has transformed various sectors by boosting the organization's performance and facilitating data safety.\n",
            "----\n",
            "Paper 215:\n",
            "Title: From the Digital Data Revolution toward a Digital Society: Pervasiveness of Artificial Intelligence\n",
            "Abstract: Technological progress has led to powerful computers and communication technologies that penetrate nowadays all areas of science, industry and our private lives. As a consequence, all these areas are generating digital traces of data amounting to big data resources. This opens unprecedented opportunities but also challenges toward the analysis, management, interpretation and responsible usage of such data. In this paper, we discuss these developments and the fields that have been particularly effected by the digital revolution. Our discussion is AI-centered showing domain-specific prospects but also intricacies for the method development in artificial intelligence. For instance, we discuss recent breakthroughs in deep learning algorithms and artificial intelligence as well as advances in text mining and natural language processing, e.g., word-embedding methods that enable the processing of large amounts of text data from diverse sources such as governmental reports, blog entries in social media or clinical health records of patients. Furthermore, we discuss the necessity of further improving general artificial intelligence approaches and for utilizing advanced learning paradigms. This leads to arguments for the establishment of statistical artificial intelligence. Finally, we provide an outlook on important aspects of future challenges that are of crucial importance for the development of all fields, including ethical AI and the influence of bias on AI systems. As potential end-point of this development, we define digital society as the asymptotic limiting state of digital economy that emerges from fully connected information and communication technologies enabling the pervasiveness of AI. Overall, our discussion provides a perspective on the elaborate relatedness of digital data and AI systems.\n",
            "----\n",
            "Paper 216:\n",
            "Title: Research Challenges for the Design of Human-Artificial Intelligence Systems (HAIS)\n",
            "Abstract: Artificial intelligence (AI) capabilities are increasingly common components of all socio-technical information systems that integrate human and machine actions. The impacts of AI components on the design and use of application systems are evolving rapidly as improved deep learning techniques and fresh big data sources afford effective and efficient solutions for broad ranges of applications. New goals and requirements for Human-AI System (HAIS) functions and qualities are emerging, whereas the boundaries between human and machine behaviors continue to blur. This research commentary identifies and addresses the design science research (DSR) challenges facing the field of Information Systems as the demand for human-machine synergies in Human-Artificial Intelligence Systems surges in all application areas. The design challenges of HAIS are characterized by a taxonomy of eight C's - composition, complexity, creativity, confidence, controls, conscience, certification, and contribution. By applying a design science research frame to structure and investigate HAIS design, implementation, use, and evolution, we propose a forward-thinking agenda for relevant and rigorous information systems research contributions.\n",
            "----\n",
            "Paper 217:\n",
            "Title: Epidemic Prediction using Machine Learning and Deep Learning Models on COVID-19 Data\n",
            "Abstract: ABSTRACT A catastrophic epidemic of Severe Acute Respiratory Syndrome-Coronavirus, commonly recognised as COVID-19, introduced a worldwide vulnerability to human community. All nations around the world are making enormous effort to tackle the outbreak towards this deadly virus through various aspects such as technology, economy, relevant data, protective gear, lives-risk medications and all other instruments. The artificial intelligence-based researchers apply knowledge, experience and skill set on national level data to create computational and statistical models for investigating such a pandemic condition. In order to make a contribution to this worldwide human community, this paper recommends using machine-learning and deep-learning models to understand its daily accelerating actions together with predicting the future reachability of COVID-19 across nations by using the real-time information from the Johns Hopkins dashboard. In this work, a novel Exponential Smoothing Long-Short-Term Memory Networks Model (ESLSTM) learning model is proposed to predict the virus spread in the near future. The results are evaluated using RMSE and R-Squared values.\n",
            "----\n",
            "Paper 218:\n",
            "Title: Societal Impact of Data Science and Artificial Intelligence\n",
            "Abstract: The explosion of interest in KDD and other Data Science/Machine Learning/AI conferences is just one of the many signs that these technologies are no longer confined to the realms of academia and a hand-full of tech companies. As our daily lives seamlessly integrate more and more data-driven applications, people's excitement is tempered by worry about the technologies' potential to disrupt their existence. Having worked for almost 30 years to design and develop these technologies, the KDD community now should examine and debate the impact of Machine Learning & AI on the broader world. Beyond the hype, where do we stand with respect to the dangers? What role can our community play to alleviate concerns around AI taking jobs, or taking over? How can the value derived from data be distributed fairly? Are concerns about inequity well-founded or rather largely problems of perception? What can be done to bring data hunger and data sharing concerns to a level of equilibrium? How do we prepare people to interact with intelligent systems at scale? Can we unleash the incredible responsiveness of the KDD community toward longer-term more impactful projects across sectors that are essential for social good, such as Health, Environmental Sustainability, and Public Welfare.\n",
            "----\n",
            "Paper 219:\n",
            "Title: Implications for Artificial Intelligence and ESG Data\n",
            "Abstract: The use of Machine Learning in relation to ESG factors is still poorly explored and is a topic that has undoubtedly experienced a surge in popularity in recent years even if the term is often used more for marketing purposes than for practical aims. However, in the last few years, AI capabilities have proven to become helpful for ESG investing, which often relied on self-disclosed, annualised corporate information, exposed to inherent data challenges and biases. Investment managers are coming under increasing pressure to measure ESG criteria in their portfolios. Here, AI can provide an answer with analysis technologies that filter essential data acting as the catalyst for sustainable investing - at scale. However, challenges of how to accurately measure and rate a company’s environmental and social impact, particularly given that ESG remains an evolving concept and that there are multiple reporting standards and frameworks, have led to the so called “aggregate confusion” - among companies and investors.\n",
            "----\n",
            "Paper 220:\n",
            "Title: Machine intelligence and the data-driven future of marine science\n",
            "Abstract: \n",
            " Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.\n",
            "----\n",
            "Paper 221:\n",
            "Title: ALGORITHMIC LITERACY: Generative Artificial Intelligence Technologies for Data Librarians\n",
            "Abstract: INTRODUCTION: Artificial intelligence (AI) is a novel type of library technology. AI technologies and the needs of data librarians are hybrid and symbiotic, because academic libraries must insert AI technologies into their information and data services. Library services need AI to interpret the context of big data.OBJECTIVES: In this context, we explore the use of the the OpenAI Codex, a deep learning model trained on Python code from repositories, to generate code scripts for data librarians. This investigation examines the practices, models, and methodologies for obtaining code script insights from complex code environments linked to AI GPT technologies.  METHODS: The proposed AI-powered method aims to assist data librarians in creating code scripts using Python libraries and plugins such as the integrated development environment PyCharm, with additional support from the Machinet AI and Bito AI plugins. The process involves collaboration between the data librarian and the AI agent, with the librarian providing a natural language description of the programming problem and the OpenAI Codex generating the solution code in Python.RESULTS: Five specific web-scraping problems are presented. The scripts demonstrate how to extract data, calculate metrics, and write the results to files.CONCLUSION: Overall, this study highlights the application of AI in assisting data librarians with code script creation for web scraping tasks. AI may be a valuable resource for data librarians dealing with big data challenges on the Web. The possibility of creating Python code with AI is of great value, as AI technologies can help data librarians work with various types of data sources. The Python code in Data Science web scraping projects uses a machine-learning model that can generate human-like code to help create and improve the library service for extracting data from a web collection. The ability of nonprogramming data librarians to use AI technologies facilitates their interactions with all types and data sources. The Python programming language has artificial intelligence modules, packages, and plugins such as the OpenAI Codex, which serialises automation and navigation in web browsers to simulate human behaviour on pages by entering passwords, selecting captcha options, collecting data, and creating different collections of datasets to be viewed.\n",
            "----\n",
            "Paper 222:\n",
            "Title: A Review of Orebody Knowledge Enhancement Using Machine Learning on Open-Pit Mine Measure-While-Drilling Data\n",
            "Abstract: Measure while drilling (MWD) refers to the acquisition of real-time data associated with the drilling process, including information related to the geological characteristics encountered in hard-rock mining. The availability of large quantities of low-cost MWD data from blast holes compared to expensive and sparsely collected orebody knowledge (OBK) data from exploration drill holes make the former more desirable for characterizing pre-excavation subsurface conditions. Machine learning (ML) plays a critical role in the real-time or near-real-time analysis of MWD data to enable timely enhancement of OBK for operational purposes. Applications can be categorized into three areas, focused on the mechanical properties of the rock mass, the lithology of the rock, as well as, related to that, the estimation of the geochemical species in the rock mass. From a review of the open literature, the following can be concluded: (i) The most important MWD metrics are the rate of penetration (rop), torque (tor), weight on bit (wob), bit air pressure (bap), and drill rotation speed (rpm). (ii) Multilayer perceptron analysis has mostly been used, followed by Gaussian processes and other methods, mainly to identify rock types. (iii) Recent advances in deep learning methods designed to deal with unstructured data, such as borehole images and vibrational signals, have not yet been fully exploited, although this is an emerging trend. (iv) Significant recent developments in explainable artificial intelligence could also be used to better advantage in understanding the association between MWD metrics and the mechanical and geochemical structure and properties of drilled rock.\n",
            "----\n",
            "Paper 223:\n",
            "Title: Automated Registration of Multiangle SAR Images Using Artificial Intelligence\n",
            "Abstract: Traditionally, nonlinear data processing has been approached via the use of polynomial filters, which are straightforward expansions of many linear methods, or through the use of neural network techniques. In contrast to linear approaches, which often provide algorithms that are simple to apply, nonlinear learning machines such as neural networks demand more computing and are more likely to have nonlinear optimization difficulties, which are more difficult to solve. Kernel methods, a recently developed technology, are strong machine learning approaches that have a less complicated architecture and give a straightforward way to transforming nonlinear optimization issues into convex optimization problems. Typical analytical tasks in kernel-based learning include classification, regression, and clustering, all of which are compromised. For image processing applications, a semisupervised deep learning approach, which is driven by a little amount of labeled data and a large amount of unlabeled data, has shown excellent performance in recent years. For their part, today’s semisupervised learning methods operate on the assumption that both labeled and unlabeled information are distributed in a similar manner, and their performance is mostly impacted by the fact that the two data sets are in a similar state of distribution as well. When there is out-of-class data in unlabeled data, the system’s performance will be adversely affected. When used in real-world applications, the capacity to verify that unlabeled data does not include data that belongs to a different category is difficult to obtain, and this is especially true in the field of synthetic aperture radar image identification (SAR). Using threshold filtering, this work addresses the problem of unlabeled input, including out-of-class data, having a detrimental influence on the performance of the model when it is utilized to train the model in a semisupervised learning environment. When the model is being trained, unlabeled data that does not belong to a category is filtered out by the model using two different sets of data that the model selects in order to optimize its performance. A series of experiments was carried out on the MSTAR data set, and the superiority of our method was shown when it was compared against a large number of current semisupervised classification algorithms of the highest level of sophistication. This was especially true when the unlabeled data had a significant proportion of data that did not fall into any of the categories. The performance of each kernel function is tested independently using two metrics, namely, the false alarm (FA) and the target miss (TM), respectively. These factors are used to calculate the proportion of incorrect judgments made using the techniques.\n",
            "----\n",
            "Paper 224:\n",
            "Title: Explainable artificial intelligence models using real-world electronic health record data: a systematic scoping review\n",
            "Abstract: OBJECTIVE\n",
            "To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions.\n",
            "\n",
            "\n",
            "MATERIALS AND METHODS\n",
            "We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges.\n",
            "\n",
            "\n",
            "RESULTS\n",
            "Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5).\n",
            "\n",
            "\n",
            "DISCUSSION\n",
            "XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view.\n",
            "\n",
            "\n",
            "CONCLUSION\n",
            "Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.\n",
            "----\n",
            "Paper 225:\n",
            "Title: Machine Learning and Knowledge Extraction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 226:\n",
            "Title: Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks - A Survey\n",
            "Abstract: Due to extensive spread of fake news on social and news media it became an emerging research topic now a days that gained attention. In the news media and social media the information is spread highspeed but without accuracy and hence detection mechanism should be able to predict news fast enough to tackle the dissemination of fake news. It has the potential for negative impacts on individuals and society. Therefore, detecting fake news on social media is important and also a technically challenging problem these days. We knew that Machine learning is helpful for building Artificial intelligence systems based on tacit knowledge because it can help us to solve complex problems due to real word data. On the other side we knew that Knowledge engineering is helpful for representing experts knowledge which people aware of that knowledge. Due to this we proposed that integration of Machine learning and knowledge engineering can be helpful in detection of fake news. In this paper we present what is fake news, importance of fake news, overall impact of fake news on different areas, different ways to detect fake news on social media, existing detections algorithms that can help us to overcome the issue, similar application areas and at the end we proposed combination of data driven and engineered knowledge to combat fake news. We studied and compared three different modules text classifiers, stance detection applications and fact checking existing techniques that can help to detect fake news. Furthermore, we investigated the impact of fake news on society. Experimental evaluation of publically available datasets and our proposed fake news detection combination can serve better in detection of fake news.\n",
            "----\n",
            "Paper 227:\n",
            "Title: The Potential Energy of Artificial Intelligence Technology in University Education Reform from the Perspective of Communication Science\n",
            "Abstract: In today’s rapid development of science and technology, science is everywhere in people’s lives, and science communication is everywhere. Science and communication are not only not far away but also very close. Since machine learning algorithms with deep learning as a theme have achieved great success in the fields of vision and speech recognition, as well as the large amount of data resources that cloud computing, big data, and other technologies can provide, the development speed of artificial intelligence has been greatly improved, and it has had a significant impact in various industries in the society, and the country has put forward the concept of intelligent education for this purpose. However, there have been few systematic discussions on the combination of artificial intelligence with education and teaching. Therefore, this article uses artificial intelligence technology to study the potential energy space of artificial intelligence technology in college education reform from the perspective of science communication, designs and implements an online education platform for colleges and universities, and conducts a trial of platform use in a domestic college and universities. Some teachers and students conduct a satisfaction survey after the platform is used, and the conclusions show that whether in the teacher group or the student group, most teachers and students are relatively satisfied with the online education platform designed in this article. The reform of college education includes many aspects. This article is a research study on the form of college education, changing from traditional offline education to online platform education. This research can provide a certain reference for the reform of college education.\n",
            "----\n",
            "Paper 228:\n",
            "Title: FedRecovery: Differentially Private Machine Unlearning for Federated Learning Frameworks\n",
            "Abstract: Over the past decades, the abundance of personal data has led to the rapid development of machine learning models and important advances in artificial intelligence (AI). However, alongside all the achievements, there are increasing privacy threats and security risks that may cause significant losses for data providers. Recent legislation requires that the private information about a user should be removed from a database as well as machine learning models upon certain deletion requests. While erasing data records from memory storage is straightforward, it is often challenging to remove the influence of particular data samples from a model that has already been trained. Machine unlearning is an emerging paradigm that aims to make machine learning models “forget” what they have learned about particular data. Nevertheless, the unlearning issue for federated learning has not been completely addressed due to its special working mode. First, existing solutions crucially rely on retraining-based model calibration, which is likely unavailable and can pose new privacy risks for federated learning frameworks. Second, today’s efficient unlearning strategies are mainly designed for convex problems, which are incapable of handling more complicated learning tasks like neural networks. To overcome these limitations, we took advantage of differential privacy and developed an efficient machine unlearning algorithm named FedRecovery. The FedRecovery erases the impact of a client by removing a weighted sum of gradient residuals from the global model, and tailors the Gaussian noise to make the unlearned model and retrained model statistically indistinguishable. Furthermore, the algorithm neither requires retraining-based fine-tuning nor needs the assumption of convexity. Theoretical analyses show the rigorous indistinguishability guarantee. Additionally, the experiment results on real-world datasets demonstrate that the FedRecovery is efficient and is able to produce a model that performs similarly to the retrained one.\n",
            "----\n",
            "Paper 229:\n",
            "Title: GLCM Based Feature Extraction and Medical X-RAY Image Classification using Machine Learning Techniques\n",
            "Abstract: The machine learning and artificial intelligence play a vital role to solve the challenging issues in Clinical imaging. The machine learning and artificial intelligence ease the daily life of both medical practitioner and patient's. Nowadays, the automatic system is designed with high accuracy to perceive abnormality in bone X-ray images. To achieve high accuracy system has less resource available image pre-processing tools are used to enhance the medical images quality. The image pre-processing involves the process like noise removal and contrast enhancement which provides instantaneous abnormality diagnosis system. The Gray Level Co-occurrence Matrix (GLCM) texture features are widely used in image classification problems. GLCM represents the second-order statistical information of gray levels between neighboring pixels in an image[1]. In the paper, we implemented different machine learning approaches to classify the bone X-ray images of MURA (musculoskeletal radiographs) dataset into fractures and no fracture category. The four different classifiers LBF SVM (Radial Basis Function support vector machine), linear SVM, Logistic Regression and Decision tree are used for abnormality detection. The performance evaluation of the above abnormality detection in X-ray images is performed by using five statistical parameters such as Sensitivity, Specificity, Precision, Accuracy and F1 Score, which shows significant improvement.\n",
            "----\n",
            "Paper 230:\n",
            "Title: Artificial Intelligence: A Universal Virtual Tool to Augment Tutoring in Higher Education\n",
            "Abstract: Artificial intelligence is an emerging technology that revolutionizes human lives. Despite the fact that this technology is used in higher education, many professors are unaware of it. In this current scenario, there is a huge need to arise, implement information bridge technology, and enhance communication in the classroom. Through this paper, the authors try to predict the future of higher education with the help of artificial intelligence. This research article throws light on the current education system the problems faced by the subject faculties, students, changing government rules, and regulations in the educational sector. Various arguments and challenges on the implementation of artificial intelligence are prevailing in the educational sector. In this concern, we have built a use case model by using a student assessment data of our students and then built a synthesized using generative adversarial network (GAN). The dataset analyzed, visualized, and fed to different machine learning algorithms such as logistic Regression (LR), linear discriminant analysis (LDA), K-nearest neighbors (KNN), classification and regression trees (CART), naive Bayes (NB), support vector machines (SVM), and finally random forest (RF) algorithm and achieved a maximum accuracy of 58%. This article aims to bridge the gap between human lecturers and the machine. We are also concerned about the psychological emotions of the faculty and the students when artificial intelligence takes control.\n",
            "----\n",
            "Paper 231:\n",
            "Title: Advanced mass spectrometric and spectroscopic methods coupled with machine learning for in vitro diagnosis\n",
            "Abstract: In vitro diagnosis (IVD) is one vital component of medical tests that detects biological samples of tissues or bio‐fluids. Recently, mass spectrometry and spectroscopy have been increasingly employed in the field of IVD, due to their high accuracy, facile sample preparation, and rapid detection. Notably, the large datasets generated by these two technology methods provide a wealth of information but subsequently involve complex and time‐consuming processing works. Machine learning (ML), an important branch of artificial intelligence (AI), has emerged as a promising solution for the decoding of big data. ML imitates the human brain to process data, significantly improving accuracy and efficiency compared with traditional processing methods. In this review, we first introduce the commonly used ML algorithms and advanced mass spectrometry and spectroscopy techniques in the field of IVD, respectively. The ML algorithms are summarized as four aspects according to different learning tasks. Then, the combinations of ML with mass spectrometry, spectroscopy, and multi‐modal analysis for IVD are presented, and the roles of ML in these combinations are elucidated by some representative examples. This review aims to provide a systematic and comprehensive summary of the literature on ML‐assisted mass spectrometry or spectroscopy. We believe that it will facilitate researchers to select suitable ML algorithms for supplementing existing detection techniques or to develop the potential of coupling more detection techniques with ML, thus promoting the development of mass spectrometry and spectroscopy in IVD.\n",
            "----\n",
            "Paper 232:\n",
            "Title: Artificial intelligence and IoT driven technologies for environmental pollution monitoring and management\n",
            "Abstract: Detecting hazardous substances in the environment is crucial for protecting human wellbeing and ecosystems. As technology continues to advance, artificial intelligence (AI) has emerged as a promising tool for creating sensors that can effectively detect and analyze these hazardous substances. The increasing advancements in information technology have led to a growing interest in utilizing this technology for environmental pollution detection. AI-driven sensor systems, AI and Internet of Things (IoT) can be efficiently used for environmental monitoring, such as those for detecting air pollutants, water contaminants, and soil toxins. With the increasing concerns about the detrimental impact of legacy and emerging hazardous substances on ecosystems and human health, it is necessary to develop advanced monitoring systems that can efficiently detect, analyze, and respond to potential risks. Therefore, this review aims to explore recent advancements in using AI, sensors and IOTs for environmental pollution monitoring, taking into account the complexities of predicting and tracking pollution changes due to the dynamic nature of the environment. Integrating machine learning (ML) methods has the potential to revolutionize environmental science, but it also poses challenges. Important considerations include balancing model performance and interpretability, understanding ML model requirements, selecting appropriate models, and addressing concerns related to data sharing. Through examining these issues, this study seeks to highlight the latest trends in leveraging AI and IOT for environmental pollution monitoring.\n",
            "----\n",
            "Paper 233:\n",
            "Title: Applicability of machine learning techniques in food intake assessment: A systematic review\n",
            "Abstract: Abstract The evaluation of food intake is important in scientific research and clinical practice to understand the relationship between diet and health conditions of an individual or a population. Large volumes of data are generated daily in the health sector. In this sense, Artificial Intelligence (AI) tools have been increasingly used, for example, the application of Machine Learning (ML) algorithms to extract useful information, find patterns, and predict diseases. This systematic review aimed to identify studies that used ML algorithms to assess food intake in different populations. A literature search was conducted using five electronic databases, and 36 studies met all criteria and were included. According to the results, there has been a growing interest in the use of ML algorithms in the area of nutrition in recent years. Also, supervised learning algorithms were the most used, and the most widely used method of nutritional assessment was the food frequency questionnaire. We observed a trend in using the data analysis programs, such as R and WEKA. The use of ML in nutrition is recent and challenging. Therefore, it is encouraged that more studies are carried out relating these themes for the development of food reeducation programs and public policies.\n",
            "----\n",
            "Paper 234:\n",
            "Title: Neural machine translation in foreign language teaching and learning: a systematic review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 235:\n",
            "Title: Data-driven materials research enabled by natural language processing and information extraction\n",
            "Abstract: Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.\n",
            "----\n",
            "Paper 236:\n",
            "Title: Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science\n",
            "Abstract: Abstract Given the growing use of Artificial intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help reduce climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences.\n",
            "----\n",
            "Paper 237:\n",
            "Title: Data Integration Using Advances in Machine Learning in Drug Discovery and Molecular Biology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 238:\n",
            "Title: The New Legal Landscape for Text Mining and Machine Learning\n",
            "Abstract: Individually and collectively, copyrighted works have the potential to generate information that goes far beyond what their individual authors expressed or intended. Various methods of computational and statistical analysis of text — usually referred to as text data mining (“TDM”) or just text mining — can unlock that information. However, because almost every use of TDM involves making copies of the text to be mined, the legality of that copying has become a fraught issue in copyright law in United States and around the world. One of the most fundamental questions for copyright law in the Internet age is whether the protection of the author’s original expression should stand as an obstacle to the generation of insights about that expression. How this question is answered will have a profound influence on the future of research across the sciences and the humanities, and for the development of the next generation of information technology: machine learning and artificial intelligence. \n",
            " \n",
            "This Article consolidates a theory of copyright law should that I have advanced in a series of articles and amicus briefs over the past decade. It explains why applying copyright’s fundamental principles in the context of new technologies necessarily implies that copying expressive works for non-expressive purposes should not be counted as infringement and must be recognized as fair use. The Article shows how that theory was adopted and applied in the recent high-profile test cases, Authors Guild v. HathiTrust and Authors Guild v. Google, and takes stock of the legal context for TDM research in the United States in the aftermath of those decisions. \n",
            " \n",
            "The Article makes important contributions to copyright theory, but is also integrates that theory with a practical assessment various interrelated legal issues that text mining researchers and their supporting institutions must confront if they are to realize the full potential of these technologies. These issues range from the enforceability of website terms of service, the effect of laws prohibiting computer hacking and the circumvention of technological protection measures (i.e., encryption and other digital locks), and cross-border copyright issues.\n",
            "----\n",
            "Paper 239:\n",
            "Title: Artificial intelligence for climate change adaptation\n",
            "Abstract: Although artificial intelligence (AI; inclusive of machine learning) is gaining traction supporting climate change projections and impacts, limited work has used AI to address climate change adaptation. We identify this gap and highlight the value of AI especially in supporting complex adaptation choices and implementation. We illustrate how AI can effectively leverage precise, real‐time information in data‐scarce settings. We focus on supervised learning, transfer learning, reinforcement learning, and multimodal learning to illustrate how innovative AI methods can enable better‐informed choices, tailor adaptation measures to heterogenous groups and generate effective synergies and trade‐offs.\n",
            "----\n",
            "Paper 240:\n",
            "Title: A review of machine learning applications in wildfire science and management\n",
            "Abstract: Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.\n",
            "----\n",
            "Paper 241:\n",
            "Title: Sustainable Smart Industry: A Secure and Energy Efficient Consensus Mechanism for Artificial Intelligence Enabled Industrial Internet of Things\n",
            "Abstract: In recent years, the Internet of Things (IoT) has been industrializing in various real-world applications, including smart industry and smart grids, to make human existence more reliable. An overwhelming volume of sensing data is produced from numerous sensor devices as the Industrial IoT (IIoT) becomes more industrialized. Artificial Intelligence (AI) plays a vital part in big data analyses as a powerful analytic tool that provides flexible and reliable information insights in real-time. However, there are some difficulties in designing and developing a useful big data analysis tool using machine learning, such as a centralized approach, security, privacy, resource limitations, and a lack of sufficient training data. On the other hand, Blockchain promotes a decentralized architecture for IIoT applications. It encourages the secure data exchange and resources among the various nodes of the IoT network, removing centralized control and overcoming the industry's current challenges. Our proposed approach goal is to design and implement a consensus mechanism that incorporates Blockchain and AI to allow successful big data analysis. This work presents an improved Delegated Proof of Stake (DPoS) algorithm-based IIoT network that combines Blockchain and AI for real-time data transmission. To accelerate IIoT block generation, nodes use an improved DPoS to reach a consensus for selecting delegates and store block information in the trading node. The proposed approach is evaluated regarding energy consumption and transaction efficiency compared with the exciting consensus mechanism. The evaluation results reveal that the proposed consensus algorithm reduces energy consumption and addresses current security issues.\n",
            "----\n",
            "Paper 242:\n",
            "Title: Prediction of COVID-19 using machine learning techniques\n",
            "Abstract: In December 2019, the world faced an unprecedented and formidable challenge in the form of COVID-19. Since its first reported case in December 2019 and subsequent classification as a pandemic by the World Health Organization (WHO), it has imposed an enormous impact, taking lives, disrupting diverse economic sectors, and introducing numerous challenges. Predicting and controlling COVID-19 precisely remains a pivotal concern for the future. To enhance the precision of COVID-19 disease prediction and alleviate the burden on healthcare systems, we explore the application of diverse machine learning classifiers. Leveraging datasets comprising confirmed cases, recoveries, and fatalities, our study seeks to improve the predictive accuracy, ensuring efficient and precise evaluation and triage of patients. This, in turn, the load on overburdened emergency departments is reduced, and mitigates the pressures faced by healthcare professionals. The pivotal role of artificial intelligence (AI) and machine learning (ML) in this context cannot be overstated. Our research endeavors to refine the accuracy and quality of results by employing advanced machine learning techniques such as Random Forest (RF), Decision Tree (DT), Support Vector Machine (SVM), Naïve Bayes (NB), Gradient Boosting (GB), and Extreme Gradient Boosting (XGBoost). Our dataset is sourced from Kaggle, a well-established platform for data science and machine learning. Our comprehensive analysis compares the outcomes of six distinct models. Notably, the Gradient Boosting classifier surpasses other techniques, achieving an impressive accuracy rate of 90% for confirmed cases, 90% for recoveries, and 92% for fatalities. This represents a significant improvement over the baseline paper, which achieved accuracy rates of 83% for confirmed cases, 72% for recoveries, and 81% for fatalities using the same dataset. Further our research enhances COVID-19 prediction, aiding healthcare professionals in addressing the other global epidemic for future.\n",
            "----\n",
            "Paper 243:\n",
            "Title: Experts' View on Challenges and Needs for Fairness in Artificial Intelligence for Education\n",
            "Abstract: None\n",
            "----\n",
            "Paper 244:\n",
            "Title: Current Advances, Trends and Challenges of Machine Learning and Knowledge Extraction: From Machine Learning to Explainable AI\n",
            "Abstract: None\n",
            "----\n",
            "Paper 245:\n",
            "Title: Recent trends in computational intelligence for educational big data analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 246:\n",
            "Title: A Review of Artificial Intelligence, Big Data, and Blockchain Technology Applications in Medicine and Global Health\n",
            "Abstract: Artificial intelligence (AI) programs are applied to methods such as diagnostic procedures, treatment protocol development, patient monitoring, drug development, personalized medicine in healthcare, and outbreak predictions in global health, as in the case of the current COVID-19 pandemic. Machine learning (ML) is a field of AI that allows computers to learn and improve without being explicitly programmed. ML algorithms can also analyze large amounts of data called Big data through electronic health records for disease prevention and diagnosis. Wearable medical devices are used to continuously monitor an individual’s health status and store it in cloud computing. In the context of a newly published study, the potential benefits of sophisticated data analytics and machine learning are discussed in this review. We have conducted a literature search in all the popular databases such as Web of Science, Scopus, MEDLINE/PubMed and Google Scholar search engines. This paper describes the utilization of concepts underlying ML, big data, blockchain technology and their importance in medicine, healthcare, public health surveillance, case estimations in COVID-19 pandemic and other epidemics. The review also goes through the possible consequences and difficulties for medical practitioners and health technologists in designing futuristic models to improve the quality and well-being of human lives.\n",
            "----\n",
            "Paper 247:\n",
            "Title: Artificial Intelligence and Information Processing: A Systematic Literature Review\n",
            "Abstract: This study aims to understand the development trends and research structure of articles on artificial intelligence (AI) and information processing in the past 10 years. In particular, this study analyzed 13,294 papers published from 2012 to 2021 in the Web of Science, used the bibliometric analysis method to visualize the data of the papers, and drew a scientific knowledge map. By exploring the development of mainstream journals, author and country rankings, keyword evolution, and research field rankings in the past 10 years, this study uncovered key trends affecting AI progress and information processing that provide insights and serve as an important reference for future AI research and information processing. The results revealed a gradual increase in publications over the past decade, with explosive growth after 2020. The most prolific researchers in this field were Xu, Z.S.; Pedrycz, W.; Herrera-Viedma, E.; the major contributing countries were China, the USA, and Spain. In the AI and information processing research, keywords including “Deep learning”, “Machine learning”, and “Feature extraction” are components that play a crucial role. Additionally, the most representative research areas were “Engineering”, “Operations Research and Management Science”, and “Automation Control Systems”. Overall, this study used bibliometric analysis to provide an overview of the latest trends in artificial intelligence and information processing. Although AI and information processing have been applied to various research areas, many other sub-topics can be further applied. Based on the findings, this study presented research insights and proposed suggestions for future research directions on AI and information processing.\n",
            "----\n",
            "Paper 248:\n",
            "Title: Prediction of Volleyball Competition Using Machine Learning and Edge Intelligence\n",
            "Abstract: Data analysis and machine learning are the backbones of the current era. Human society has entered machine learning and data science that increases the data capacity. It has been widely acknowledged that not only does the number of information increase exponentially, but also the way of human information management and processing is completed to be changed from manual to computer, mainly depending on the transformation of information technology including a computer, network, and communication. This paper is aimed at a solution to the lag of the methods and means of volleyball technique prediction in China. Through field visits, it is found that the way of analysis and research of techniques and tactics in Chinese volleyball practice is relatively backward, which to a certain extent affected the rapid development of Chinese volleyball. Therefore, it is a necessary and urgent task to realize the reform of the methods and means of volleyball technical and tactical analysis in China. The data analysis and prediction are based on the machine learning and data mining algorithm applied to volleyball in this paper is an inevitable trend. The proposed model is applied to the data produced at the edges of the systems and thoroughly analyzed. The Apriori algorithm of the machine learning algorithm is utilized to process the data and provide a prediction about the strategies of a volleyball match. The Apriori algorithm of machine learning is also optimized to perform better data analysis. The effectiveness of the proposed model is also highlighted.\n",
            "----\n",
            "Paper 249:\n",
            "Title: A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.\n",
            "Abstract: Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.\n",
            "----\n",
            "Paper 250:\n",
            "Title: Piano Performance and Music Automatic Notation Algorithm Teaching System Based on Artificial Intelligence\n",
            "Abstract: Artificial intelligence is a subject that studies all kinds of human intelligent activities and their laws. It is developed on the basis of the cohesion of many disciplines such as computer science, politics, information system, neurophysiology, psychology, philosophy, and language. This paper aims to study how to build a computer or intelligent machine, including hardware and software, imitate and expand the human brain to perform thinking functions such as thinking, programming, arithmetic, and learning, solve complex problems that need to be handled by professionals, and better apply the artificial intelligence assistance system to the teaching of piano performance. In this paper, Prolog language and music-assisted learning system based on the ARM and SA algorithm are proposed, the principle and operation process of music automatic recording technology are deeply studied, and the system data of artificial intelligence are summarized and analyzed by using internal database, so as to find out the implementation principle and law of piano automatic recording system. So that the artificial intelligence assistant system can be better applied to music teaching. The experimental results show that, in the teaching system of piano performance and music automatic notation algorithm, the utilization rate of artificial intelligence auxiliary technology has reached 56.81 and is growing rapidly. Therefore, we can find that the artificial intelligence assistant system plays an important role in the teaching system of piano performance and automatic music notation.\n",
            "----\n",
            "Paper 251:\n",
            "Title: Cancer Prognosis Using Artificial Intelligence-Based Techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 252:\n",
            "Title: Research and Application of Visual Recognition Technology for Epoxy Resin Automatic Liquid Separation Process Based on Machine Learning\n",
            "Abstract: The liquid separation process of epoxy resin is usually realized by means of pipeline window video monitoring and artificial control of pipeline switching. Aiming at the problem of low level of intelligence and judgment accuracy during the liquid separation process, a method of identification of solution state and intelligent control by using machine vision technology is proposed. By configuring the appropriate explosion-proof camera and light source the solution features can ensure stable. By using feature extraction algorithm of HSV and gray level co-occurrence matrix, the combination characteristic of solution state can be extracted. The image feature array and solution state markers are trained by using the random forest machine learning algorithm until a stable training model is obtained. Finally, the judgment results are transmitted to the DCS control system to realize the intelligent control of liquid separation process of epoxy resin. The experiment proves that the accuracy rate of solution state recognition is above 98%, and the machine learning model will be gradually improved with the accumulation and iterative evolution of liquid separation production data.\n",
            "----\n",
            "Paper 253:\n",
            "Title: Machine Learning in liver disease diagnosis: Current progress and future opportunities\n",
            "Abstract: There has been a rapid growth in the use of automatic decision-making systems and tools in the medical domain. By using the concepts of big data, deep learning, and machine learning, these systems extract useful information from large medical datasets and help physicians in making accurate and timely decisions regarding predictions and diagnosis of diseases. In this regard, this study provides an extensive review of the progress of applying Artificial Intelligence in forecasting and detecting liver diseases and then summarizes related limitations of the studies followed by future research.\n",
            "----\n",
            "Paper 254:\n",
            "Title: Machine learning: Trends, perspectives, and prospects\n",
            "Abstract: Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.\n",
            "----\n",
            "Paper 255:\n",
            "Title: Predictive Maintenance of Actuated Quarter-Turn Valves Using Artificial Intelligence\n",
            "Abstract: This paper proposes a predictive maintenance method for actuated quarter-turn valves used in oil and gas applications and carbon capture systems. The identification and classification of the degradation process allow the management of the most critical parts by adopting specific strategies. In this way, it is possible to avoid service interruptions and reduce recovery times. Therefore, the objective of the work is to develop a classification approach based on artificial intelligence algorithms capable of detecting the aging stage of the valves by processing the stem vibration signals, measured with accelerometers, in the initial portion of an opening-closing cycle. Several machine learning techniques are presented and compared in the processing of the information extracted from an experimental test bench: Support Vector Machines, feed-forward Neural Networks and Classification Trees process the statistical characteristics of the measurements; furthermore, a neural network with complex values is used to classify the same measurements in the frequency domain. A data augmentation procedure is implemented to create a meaningful training dataset for these supervised learning algorithms. Moreover, additional test datasets are generated simulating different disturbing effects (increasing noise level, lowpass filtering effects, superimposing DC components). The results show the possibility of identifying valve degradation in different situations and offer a very accurate comparison of the proposed classification techniques in terms of robustness against simulated disturbances.\n",
            "----\n",
            "Paper 256:\n",
            "Title: The Era of Artificial Intelligence in Pharmaceutical Industries - A Review\n",
            "Abstract: As a growing sector, the Era of Artificial Intelligence, Machine Learning and Data Science in the Pharmaceutical Industry contributes in the drug discovery process, giving emphasis on how new technologies have improved effectiveness. As in the current scenario artificial intelligence including machine learning may be considered the future for a wide range of disciplines and industries specially the pharmaceutical industry. As we know today pharmaceutical industries producing a single approved drug cost the company millions with many years of rigorous testing prior to its approval, reducing costs and time is of high interest. The involvement of Artificial Intelligence will be useful to the pharmaceutical industry and also be of interest to anyone doing research in chemical biology, computational chemistry, medicinal chemistry and bioinformatics.\n",
            "----\n",
            "Paper 257:\n",
            "Title: Rho AI – Leveraging artificial intelligence to address climate change: Financing, implementation and ethics\n",
            "Abstract: The case focusses on Rho AI, a data science firm, and its attempt to leverage artificial intelligence to encourage environmental, social and governance investments to limit the impact of climate change. Rho AI’s proposed open-source artificial intelligence tool integrates automated web scraping technology and machine learning with natural language processing. The aim of the tool is to enable investors to evaluate the climate impact of companies and to use this evaluation as a basis for making investments in companies. The case study allows for students to gain an insight into some of the strategic choices that need to be considered when developing an artificial intelligence–based tool. Students will be able to explore the role of ethics in decision-making related to artificial intelligence, while familiarising themselves with key technical terminology and possible business models. The case encourages students to see beyond the technical granularities and to consider the multi-faceted, wider corporate and societal issues and priorities. This case contributes to students recognising that business is not conducted in a vacuum and enhances students’ understanding of the role of business in society during new developments triggered by digital technology.\n",
            "----\n",
            "Paper 258:\n",
            "Title: Text Analytics: the convergence of Big Data and Artificial Intelligence\n",
            "Abstract: The analysis of the text content in emails, blogs, tweets, forums and other forms of textual communication constitutes what we call text analytics. Text analytics is applicable to most industries: it can help analyze millions of emails; you can analyze customers� comments and questions in forums; you can perform sentiment analysis using text analytics by measuring positive or negative perceptions of a company, brand, or product. Text Analytics has also been called text mining, and is a subcategory of the Natural Language Processing (NLP) field, which is one of the founding branches of Artificial Intelligence, back in the 1950s, when an interest in understanding text originally developed. Currently Text Analytics is often considered as the next step in Big Data analysis. Text Analytics has a number of subdivisions: Information Extraction, Named Entity Recognition, Semantic Web annotated domain�s representation, and many more. Several techniques are currently used and some of them have gained a lot of attention, such as Machine Learning, to show a semisupervised enhancement of systems, but they also present a number of limitations which make them not always the only or the best choice. We conclude with current and near future applications of Text Analytics.\n",
            "----\n",
            "Paper 259:\n",
            "Title: Heart Failure Prediction by Feature Ranking Analysis in Machine Learning\n",
            "Abstract: Heart disease is one of the major cause of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the field of clinical data analysis. With the advanced development in machine learning (ML), artificial intelligence (AI) and data science has been shown to be effective in assisting in decision making and predictions from the large quantity of data produced by the healthcare industry. ML approaches has brought lot of improvements and broadens the study in medical field which recognizes patterns in the human body by using various algorithms and correlation techniques. One such reality is coronary heart disease, various studies gives impression into predicting heart disease with ML techniques. Initially ML was used to find degree of heart failure, but also used to identify significant features that affects the heart disease by using correlation techniques. There are many features/factors that lead to heart disease like age, blood pressure, sodium creatinine, ejection fraction etc. In this paper we propose a method to finding important features by applying machine learning techniques. The work is to design and develop prediction of heart disease by feature ranking machine learning. Hence ML has huge impact in saving lives and helping the doctors, widening the scope of research in actionable insights, drive complex decisions and to create innovative products for businesses to achieve key goals.\n",
            "----\n",
            "Paper 260:\n",
            "Title: Data science in organizations: Conceptualizing its breakthroughs and blind spots\n",
            "Abstract: The field of data science emerged in recent years, building on advances in computational statistics, machine learning, artificial intelligence, and big data. Modern organizations are immersed in data and are turning toward data science to address a variety of business problems. While numerous complex problems in science have become solvable through data science, not all scientific solutions are equally applicable to business. Many data-intensive business problems are situated in complex socio-political and behavioral contexts that still elude commonly used scientific methods. To what extent can such problems be addressed through data science? Does data science have any inherent blind spots in this regard? What types of business problems are likely to be addressed by data science in the near future, which will not, and why? We develop a conceptual framework to inform the application of data science in business. The framework draws on an extensive review of data science literature across four domains: data, method, interfaces, and cognition. We draw on Ashby’s Law of Requisite Variety as theoretical principle. We conclude that data-scientific advances across the four domains, in aggregate, could constitute requisite variety for particular types of business problems. This explains why such problems can be fully or only partially addressed, solved, or automated through data science. We distinguish between situations that can be improved due to cross-domain compensatory effects, and problems where data science, at best, only contributes merely to better understanding of complex phenomena.\n",
            "----\n",
            "Paper 261:\n",
            "Title: Towards Integrative Machine Learning and Knowledge Extraction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 262:\n",
            "Title: Finding Anomalies in the Operation of Automated Control Systems Using Machine Learning\n",
            "Abstract: This article deals with the problem of detecting anomalies in the operation of automated systems. Anomaly detection is useful for preventing breakdowns and improving the performance of automated systems. Using various sensors in automated systems, it is possible to read information about the state of certain system parameters, which in turn helps to monitor the state of the system at the moment. But simply viewing system indicators is not a very optimal option, as human resources are wasted. It is also possible to set the limit values of the sensors and if some indicator goes beyond them, the system can show a message about it. Still, not all graphs can do with this solution to the problem, because abnormal values can be recorded in these limits and ignored by such a system, or the sensor can change the range of work and in this case, thus operators will receive a large number of false messages. In this way, it is possible to implement a system that will detect anomalies automatically using artificial intelligence, which will learn from existing previous data and notify the operator of a malfunction.\n",
            "----\n",
            "Paper 263:\n",
            "Title: Acquiring businesss intelligence through data science: A practical approach\n",
            "Abstract: Throughout the history of humanity, the way that humans transmit intelligence from generation to generation has changed multiple times. Beginning verbally and through manuscripts, continuing with patented inventions, official and private documents, nowadays, the different ways of adapting and implementing the knowledge acquired through data are being highlighted. Whether with regards to human, artificial, or mixed intelligence, data can provide consistent and meaningful answers to address the challenges of today's businesses. This scientific paper contributes to the vision of a hybrid human and artificial intelligence approach, thus explaining, exemplifying, and presenting research on how today's organizations apply the concept of data efficiency and effectiveness from a business intelligence perspective. The fact that decision-makers can be more performant with the help of data science and machine learning has the power of unlocking strengths and opportunities at an unprecedented rate and therefor is the new norm in the modern business world.\n",
            "----\n",
            "Paper 264:\n",
            "Title: Diving into the Deep End: Machine Learning for the Chemist\n",
            "Abstract: I the past year, ACS Omega has seen a dramatic increase in the number of articles published with an Artificial Intelligence (AI) or Machine Learning, Deep Learning, Neural Networks theme. In 2021, 105 articles were published vs 45 articles published in the previous year, an increase of 133%. This exceptional growth for a fully open access broad scope journal pairs with the growth seen at many other journals in the ACS portfolio. Interestingly, the growth registered in the past 2−3 years is not confined to the journals that specialize in chemical informatics: the Journal of Chemical Information and Modeling, the Journal of Chemical Theory and Computation, and, to some extent, the Journal of Physical Chemistry C. It also encompasses journals in the materials sciences, the physical sciences, measurement science, chemical engineering, and environmental science in the broader ACS portfolio (Figure 1). Looking at the wider publication landscape, the Directory of Open Access Journal lists 65 journal entries for scientific publications that pertain to the topic of AI. Eleven of these were added in the last year alone, and this includes only those journals queried in the computational science category. In addition to these, numerous other open access, broader scope journals also publish work without any perceived evaluation of immediate impact and where existing AI tools have been successfully applied to a variety of chemistry questions. Over the past 10 to 15 years, AI, especially deep learning, has effected dramatic technological progress and proven success in areas such as computer vision, speech recognition, natural language processing, common sense knowledge, strategic reasoning, and robotics. Exceptional results have also been reported in the medical sciences; for example, deep neural networks facilitated accurate diagnosis of skin cancer, and deep learning enabled extraction of new knowledge from old data, enabling accurate prediction of age, gender, smoking status, blood pressure, and heart attack propensity of individuals just by analyzing previously acquired retinal images. But, what about the status of AI and its perception in chemistry? In the ensuing text, as an entreé to the associated Virtual Issue, I will present a brief overview of the perceived usefulness of AI at this time in some fields of the chemical sciences and related areas, based on recently published reviews and perspectives by experts in the area, as well as other resources. A review by Baum et al. charted the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection, which includes patents as well as research articles. In their paper, they refer to the “Hype Cycle of Emerging Technologies”, and from the data gathered, they determine that AI adoption in life sciences and analytical chemistry has navigated the so-called “peak of inflated expectations” and “trough of disillusionment” and successfully progressed to the “plateau of productivity”. It is common to overestimate the effect of a technology in the short run and underestimate it in the long run (anecdotally known as Amara’s law). For example, despite commercial interests and consequent investments being enormous, to this day, no new drug has yet been synthesized using AI. As another (counter) example, Peiretti and Brunel, in their Perspective published in 2018, ponder whether organic chemistry and in particular retrosynthesis might be the ideal next application of AI techniques. After all, it “f its perfectly” the definition of AI as a problem with “complex input−output relationships [that] are dif f icult or impractical to model procedurally”. However, Baum et al. firmly assess that “there are still areas of Chemistry like organic synthetic chemistry where AI is yet to make an impact”. Still, work is in progress, as standardized formats for reporting a chemical synthesis procedure are being developed and even classified (an essential step for scientific fields to exist) in the new taxonomy of digital chemistry or chemputation. At this stage in the discussion, it may be interesting to explore what the drivers are to greater or lesser success in applying AI methods to scientific problems. This point is examined in the Editorial by Jones et al., which accompanies an excellent JACS Au special collection on “Emerging Chemistry & Machine Learning”. In their overview, three main reasons are indicated. Two of them are quite intrinsic to the method (algorithm development and theoretical derivation of descriptors), while one of them is, notably, not specific but resides in the availability of a collection of standardized, highquality data. A case in point, and unanimously defined as the most spectacular recent success of AI, is the recent prediction of a protein’s three-dimensional structure from its amino acid sequence via AlphaFold. The tool was trained on large publicly available databases, such as the RSCB Protein Data Bank, as well as protein sequences of unknown structure. In return, it is somewhat expected, although not guaranteed, that the new information generated is also made available to the public. The AlphaFold code is now publicly available. It can\n",
            "----\n",
            "Paper 265:\n",
            "Title: Cardiac Diagnosis with Machine Learning: A Paradigm Shift in Cardiac Care\n",
            "Abstract: ABSTRACT For successful prognosis of cardiovascular diseases (CVDs), an early and quick diagnosis is essential. Heart disease and strokes are the predominant causes and account for more than 80% of CVD deaths, whilst one-third of these deaths occurs prematurely in people under 70 years of age. For CVD diagnosis, patients need to show an elevated level of biomarkers in the blood sample associated with severe pain in the chest, and diagnostic electrocardiogram (ECG). The majority of CVD patients making CVD diagnosis difficult for physicians show a surprisingly normal ECG pattern. Artificial intelligence techniques can radically improve and optimize CVD outcomes. AI has the potential to provide novel tools and techniques to collect and interpret data and make faster and more accurate decisions reducing hospitalization cost, thereby increasing the quality of life. AI has also improved medical knowledge by unlocking clinically relevant information from the voluminous and complex data received from various resources. This paper reviews the present biosensors and describes various AI techniques, which can effectively be used for early and accurate detection of CVD, thereby improving cardiac care.\n",
            "----\n",
            "Paper 266:\n",
            "Title: Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 267:\n",
            "Title: Human-Machine Collaboration for Democratizing Data Science\n",
            "Abstract: Everybody wants to analyse their data, but only few posses the data science expertise to do this. Motivated by this observation, we introduce a novel framework and system VisualSynth for human-machine collaboration in data science. Its aim is to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. VisualSynth relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.\n",
            "----\n",
            "Paper 268:\n",
            "Title: Discovering the evolution of artificial intelligence in cancer research using dynamic topic modeling\n",
            "Abstract: The rapid growth of healthcare data in recent years calls for more advanced and efficient analytic techniques. Artificial intelligence facilitates finding insightful patterns in massive high-dimensional data. Considering the latest movements towards using machine learning and deep learning techniques in the medical domain, in this study, we focused on the publications in which researchers employed artificial intelligence techniques for cancer diagnosis and treatment. Using dynamic topic modeling and natural language processing techniques, we analyzed the contents and trends of more than 12,000 scientific publications within the period of 2000 to 2018, extracted from two different sources, i.e., Elsevier’s Scopus and PubMed. While drawing the landscape of cancer research, our results also shed light on the evolution of artificial intelligence techniques and algorithms used for cancer diagnosis and treatment. Our findings confirm that modern computer science algorithms are being widely applied to extract patterns from large-scale medical images to cure different types of cancer with a special focus on deep learning techniques in recent years.\n",
            "----\n",
            "Paper 269:\n",
            "Title: Exploring different notions of literacy: a literature review analysis of literacy research related to Artificial Intelligence and Big Data application\n",
            "Abstract: Artificial Intelligence (AI) and Big Data projects have been rapidly applied by organizations and adopted by younger users, leading to an increasing number of various capacity building applications, as well as significant impacts on the well-being of both designers and users. This research presents a review of literacy-related literature on “Big Data”, “Artificial Intelligence”, “Machine Learning”, based on 73 articles retrieved from the Web of Science Database. The author produced a keyword co-occurrence network map highlighting three main domains: digital literacy, information literacy, and data literacy. This paper concluded that data and information literacy share similar semantic networks whereas digital literacy is related more to pedagogy and innovations, while being the more distant node from both data and information literacy. For stakeholders such as educators, academic researchers, and policymakers, the paper provides reference points for understanding technological literacy and the current relationship among key literacy concepts and domains in relation to AI and Big Data\n",
            "----\n",
            "Paper 270:\n",
            "Title: Principles and Practice of Explainable Machine Learning\n",
            "Abstract: Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.\n",
            "----\n",
            "Paper 271:\n",
            "Title: AI Based Student Bot for Academic Information System using Machine Learning\n",
            "Abstract: In General all the institutions like colleges sends their notes and information to students individually. Sometimes the student can�t access it quickly and repetition of data also increased. The realm of this work is to create a Chatbot for the college purpose. Our work reduces the human work to send every details and notes to all departments by email or some other medium. In this work, academic information's /details feed it to the database which will be available for the long time period. The academic information consists of information about placements details, exam time tables, semester notes and upcoming events. A Chatbot is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods. The chat bot stores the data by key words and when the user entered data is matched with the key it reply the assigned data for it. The Chatbot is created by using python language and Natural language processing. This project make use of the MySQL database to store the information. With the help of natural language processing the bot AI understand the message sent by the user and reply with the matched key value. In this Chatbot the user first need to login by their college roll number and Department. When the valid person asks about the particular information by text the information gets retrieved from the updated database that related to their department. Through this chat box the student can easily access whenever they want and the data need not to be update more than once.\n",
            "----\n",
            "Paper 272:\n",
            "Title: Roles and research foci of artificial intelligence in language education: an integrated bibliographic analysis and systematic review approach\n",
            "Abstract: ABSTRACT This study explores the roles and research foci of AILEd (Artificial Intelligence in Language Education). The AILEd studies published from 1990 to 2020 in the WOS (Web of Science) database were included in the present study. Based on the well-recognized Technology-based Learning Review model, several dimensions, such as research methods, research sample groups, adopted technology, language skills, the role of AI in language education, and learning outcomes, were taken into account. The review results show that the main application domains of AILEd research were writing, reading, and vocabulary acquisition. In terms of applied technology and algorithms, AI in language education mostly adopted ITS (Intelligent Tutoring System) and NLP (Natural Language Processing). Besides, several commonly used AI algorithms were Statistical Learning, Data Mining, Machine Learning, and Natural Language Parsing. It was also found that some research focused on learning anxiety, willingness to communicate, knowledge acquisition, and classroom interaction. However, higher order thinking, complex problem solving, critical thinking ability, and collaborative learning tendencies were rarely considered in AILEd studies. Accordingly, several suggestions are provided to researchers, teachers, and decision makers for applying or studying AI applications in language education in the future.\n",
            "----\n",
            "Paper 273:\n",
            "Title: Artificial Intelligence in Pharmaceutical and Healthcare Research\n",
            "Abstract: Artificial intelligence (AI) is a branch of computer science that allows machines to work efficiently, can analyze complex data. The research focused on AI has increased tremendously, and its role in healthcare service and research is emerging at a greater pace. This review elaborates on the opportunities and challenges of AI in healthcare and pharmaceutical research. The literature was collected from domains such as PubMed, Science Direct and Google scholar using specific keywords and phrases such as ‘Artificial intelligence’, ‘Pharmaceutical research’, ‘drug discovery’, ‘clinical trial’, ‘disease diagnosis’, etc. to select the research and review articles published within the last five years. The application of AI in disease diagnosis, digital therapy, personalized treatment, drug discovery and forecasting epidemics or pandemics was extensively reviewed in this article. Deep learning and neural networks are the most used AI technologies; Bayesian nonparametric models are the potential technologies for clinical trial design; natural language processing and wearable devices are used in patient identification and clinical trial monitoring. Deep learning and neural networks were applied in predicting the outbreak of seasonal influenza, Zika, Ebola, Tuberculosis and COVID-19. With the advancement of AI technologies, the scientific community may witness rapid and cost-effective healthcare and pharmaceutical research as well as provide improved service to the general public.\n",
            "----\n",
            "Paper 274:\n",
            "Title: Exploring the Ethical and Technical Data of Machine Consciousness: Hazards, Implications, and Future Directions\n",
            "Abstract: The study of machine consciousness has a wide range of potential and problems as it sits at the intersection of ethics, technology, and philosophy. This work explores the deep issues related to the effort to comprehend and maybe induce awareness in machines. Technically, developments in artificial intelligence, neurology, and cognitive science are required to bring about machine awareness. True awareness is still a difficult to achieve objective, despite significant progress being made in creating AI systems that are capable of learning and solving problems. The implications of machine awareness are profound in terms of ethics. Determining a machine's moral standing and rights would be crucial if it were to become sentient. It is necessary to give careful attention to the ethical issues raised by the development of sentient beings, the abuse of sentient machines, and the moral ramifications of turning off sentient technologies. Philosophically, the presence of machine consciousness may cast doubt on our conceptions of identity, consciousness, and the essence of life. It could cause us to reevaluate how we view mankind and our role in the cosmos. It is imperative that machine awareness grow responsibly in light of these challenges. The purpose of this study is to provide light on the present status of research, draw attention to possible hazards and ethical issues, and offer recommendations for safely navigating this emerging subject. We want to steer the evolution of machine consciousness in a way that is both morally just and technologically inventive by promoting an educated and transparent discourse.\n",
            "----\n",
            "Paper 275:\n",
            "Title: ASAS-NANP Symposium: Mathematical Modeling in Animal Nutrition: The progression of data analytics and artificial intelligence in support of sustainable development in animal science\n",
            "Abstract: Abstract A renewed interest in data analytics and decision support systems in developing automated computer systems is facilitating the emergence of hybrid intelligent systems by combining artificial intelligence (AI) algorithms with classical modeling paradigms such as mechanistic modeling (HIMM) and agent-based models (iABM). Data analytics have evolved remarkably, and the scientific community may not yet fully grasp the power and limitations of some tools. Existing statistical assumptions might need to be re-assessed to provide a more thorough competitive advantage in animal production systems towards sustainability. This paper discussed the evolution of data analytics from a competitive advantage perspective within academia and illustrated the combination of different advanced technological systems in developing HIMM. The progress of analytical tools was divided into three stages: collect and respond, predict and prescribe, and smart learning and policy making, depending on the level of their sophistication (simple to complicated analysis). The collect and respond stage is responsible for ensuring the data is correct and free of influential data points, and it represents the data and information phases for which data are cataloged and organized. The predict and prescribe stage results in gained knowledge from the data and comprises most predictive modeling paradigms, and optimization and risk assessment tools are used to prescribe future decision-making opportunities. The third stage aims to apply the information obtained in the previous stages to foment knowledge and use it for rational decisions. This stage represents the pinnacle of acquired knowledge that leads to wisdom, and AI technology is intrinsic. Although still incipient, HIMM and iABM form the forthcoming stage of competitive advantage. HIMM may not increase our ability to understand the underlying mechanisms controlling the outcomes of a system, but it may increase the predictive ability of existing models by helping the analyst explain more of the data variation. The scientific community still has some issues to be resolved, including the lack of transparency and reporting of AI that might limit code reproducibility. It might be prudent for the scientific community to avoid the shiny object syndrome (i.e., AI) and look beyond the current knowledge to understand the mechanisms that might improve productivity and efficiency to lead agriculture towards sustainable and responsible achievements.\n",
            "----\n",
            "Paper 276:\n",
            "Title: Physical laws meet machine intelligence: current developments and future directions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 277:\n",
            "Title: Proposing machine learning of Tafsir al-Quran: In search of objectivity with semantic analysis and Natural Language Processing\n",
            "Abstract: Computer technology is neutral information technology without any subjectivities and biases. The advantages of computer technology can help to minimize intervention and subjectivity in the interpretation of the Quran. The problem of subjectivity in interpretation of the Quran is a long discussion. However, Interpretation of Quran has interrelated each other called tafsir al-Quran bil Quran method. This article proposes objective methodology and design of machine learning of Tafsir al-Quran using the advances in data science technology. Using the latest artificial intelligence technology that is Machine Learning and Natural Language Processing (NLP). Furthermore, this proposal proves the novelty of the role of technology in the preparation of religious material in millennial life from a secondary role to a primary role.\n",
            "----\n",
            "Paper 278:\n",
            "Title: Catalysis in the digital age: Unlocking the power of data with machine learning\n",
            "Abstract: The design and discovery of new and improved catalysts are driving forces for accelerating scientific and technological innovations in the fields of energy conversion, environmental remediation, and chemical industry. Recently, the use of machine learning (ML) in combination with experimental and/or theoretical data has emerged as a powerful tool for identifying optimal catalysts for various applications. This review focuses on how ML algorithms can be used in computational catalysis and materials science to gain a deeper understanding of the relationships between materials properties and their stability, activity, and selectivity. The development of scientific data repositories, data mining techniques, and ML tools that can navigate structural optimization problems are highlighted, leading to the discovery of highly efficient catalysts for a sustainable future. Several data‐driven ML models commonly used in catalysis research and their diverse applications in reaction prediction are discussed. The key challenges and limitations of using ML in catalysis research are presented, which arise from the catalyst's intrinsic complex nature. Finally, we conclude by summarizing the potential future directions in the area of ML‐guided catalyst development.This article is categorized under:\n",
            "Structure and Mechanism > Reaction Mechanisms and Catalysis\n",
            "Data Science > Artificial Intelligence/Machine Learning\n",
            "Electronic Structure Theory > Density Functional Theory\n",
            "\n",
            "----\n",
            "Paper 279:\n",
            "Title: Predicting the level of generalized anxiety disorder of the coronavirus pandemic among college age students using artificial intelligence technology\n",
            "Abstract: Introduction: Emerging reports indicate heightened anxiety among university students during the Corona pandemic. Implications of which can impact their academic performance. Artificial intelligence (AI) through machine learning can be used to predict which students are more susceptible to anxiety which can inform closer monitoring and early intervention. To date, there are no studies that have explored the efficacy of AI to predict anxiety among college students. Objective: to develop the best fit model to predict anxiety and to rank the most important factors affecting anxiety. Method: Data was collected using an online survey that included general information; Covid-19 stressors and (GAD-7). This scale categorizes level of anxiety to none, mild, moderate, and severe. We received 917 survey answers. Several machine learning classifiers were used to develop the best fit model to predict student level of anxiety. Results: the best performance based on AUC is AdaBoost (0.943) followed by neural network (0.936). Highest accuracy and F1 were for neural network (0.754) and (0.749) respectively, then neural network selected to be the best fit model. The three scoring methods revealed that the top three features that predicted anxiety to be gender; sufficient support from family and friends; and fixed family income. Conclusion: Neural network model can assist college counselors to predict which students are going through anxiety and revealed the top three features for heightened student anxiety to be gender, a support system, and family fixed income. This information can alter college councilors for early mental intervention.\n",
            "----\n",
            "Paper 280:\n",
            "Title: Teaching Machine Learning to Computer Science Preservice Teachers: Human vs. Machine Learning\n",
            "Abstract: Machine learning is a fast-growing field with various applications in artificial intelligence and data science. Recently, a new machine learning program have been integrated into the Israeli high school computer science curriculum and thus we added a new machine learning module to the Methods of Teaching Computer Science (MTCS) course, which is part of the teachers' preparation program. This machine learning module provides us a unique opportunity to teach both pedagogy and content with the same subject matter. After teaching the basics of machine learning, we asked the students to find similarities between human learning theories and machine learning algorithms. Students identified several interesting parallels: (a) Supervised learning is similar to behavioral learning as the machine learns to connect training examples (stimuli) with labels (behavior). Also, the learning is based on minimizing error (punishment) function, (b) Reinforcement learning is similar to behavioral learning as learning is based on feedback from the environment, (c) Constructivism can be identified in the iterative convergence of the algorithm; the inner model improves each iteration based on the current knowledge, and (d) Social learning is reflected in clustering as each cluster affects the learning of the other clusters. In our talk, we present the idea that computational mental models may be used to reinforce pedagogical mental models and vice versa.\n",
            "----\n",
            "Paper 281:\n",
            "Title: Human-Centric Multimodal Machine Learning: Recent Advances and Testbed on AI-Based Recruitment\n",
            "Abstract: None\n",
            "----\n",
            "Paper 282:\n",
            "Title: Mapping knowledge structure of artificial intelligence research in Bangladesh based on co-word analysis\n",
            "Abstract: Purpose: This article aims to map the knowledge structure of artificial intelligence (AI) in Bangladesh through detecting the interdisciplinarity and topic hotspots in the light of co-word analysis. \n",
            "Methodology: This study adopted bibliometric analysis of publications collected from the Web of Science (WoS) database. The WoS database was searched and 1557 publications were found. 1359 papers were selected for final analysis after eliminating duplicates. Co-occurrence words matrix, keyword clusters, hot topics were mapped using co-word analysis. The results were mapped, clustered and presented by VOSviewer.Results: The result showed a rapidly increasing publication trajectory with 12 sub-domain cluster under the AI knowledge domain in Bangladesh. It also identified that AI, machine learning, classification, neural network, deep learning, artificial neural network, convolutional neural network, support vector machine and data mining are hot topics during the period of studied time. However, the findings also suggest that many research areas in the research domain of AI of Bangladesh is still nascent.Limitation: VOSviewer often avoid having overlapping terms when multiple terms are positioned very close to each other. So, the overlapping terms remain invisible sometimes.Practical implications: This study may have potential usefulness in uncovering the AI research fields’ intellectual structure within a discipline and also to anticipate future innovation pathways of AI field in Bangladesh.Originality: Bibliometric methods to explore the research trend and growth of AI research field as a ‘knowledge base’ in Bangladesh is one of the first attempts.\n",
            "----\n",
            "Paper 283:\n",
            "Title: Evaluation of Machine Learning to Early Detection of Highly Cited Papers\n",
            "Abstract: As one of the fastest-growing topics, machine learning has many applications that span through different domains including image and signal recognition, text mining, information retrieval, robotics, etc. It enables information extraction and analysis for better insights and decision-based systems. The Web of Science(WoS) citation database is a leading organization that provides citation data of high-quality published research. WoS has its metrics to label published articles as Highly Cited Paper(HCP). Machine learning (ML) can help researchers in identifying the key characteristics of HCP. Moreover, it can allow research evaluation units forecasting significant scientific articles. In other words, it may allow researchers and/or research evaluators to detect potential scientific breakthrough ideas and stay current. In this study, more than 26 thousand records of published articles indexed by WoS were analyzed. All the records are drawn from the Technology research area as defined by WoS. Four ML algorithms are evaluated to verify the HCP common factors influence in raising citations and interest in scientific articles. The ensemble algorithms show promising results to identify HCP articles using only four factors.\n",
            "----\n",
            "Paper 284:\n",
            "Title: Explainable and trustworthy artificial intelligence for correctable modeling in chemical sciences\n",
            "Abstract: The developed framework apportions model error to inputs, computes predictive guarantees, and enables model correctability. Data science has primarily focused on big data, but for many physics, chemistry, and engineering applications, data are often small, correlated and, thus, low dimensional, and sourced from both computations and experiments with various levels of noise. Typical statistics and machine learning methods do not work for these cases. Expert knowledge is essential, but a systematic framework for incorporating it into physics-based models under uncertainty is lacking. Here, we develop a mathematical and computational framework for probabilistic artificial intelligence (AI)–based predictive modeling combining data, expert knowledge, multiscale models, and information theory through uncertainty quantification and probabilistic graphical models (PGMs). We apply PGMs to chemistry specifically and develop predictive guarantees for PGMs generally. Our proposed framework, combining AI and uncertainty quantification, provides explainable results leading to correctable and, eventually, trustworthy models. The proposed framework is demonstrated on a microkinetic model of the oxygen reduction reaction.\n",
            "----\n",
            "Paper 285:\n",
            "Title: A New Machine Learning Framework for Effective Evaluation of English Education\n",
            "Abstract: With the rapid development of information technology, English education has attracted the interest of scholars for its ability to rely on computers to analyze and understand human language. Taking machine learning (ML) as the core, this paper tries to develop an evaluation system for ML-based English education. Specifically, a basic ML model was established with four stages: environment, knowledge base, learning process, and implementation process. The entire ML system was divided from top to bottom into a user layer, a business layer, and a data layer. Application results show that, during the ML, even users with similar personal data and the same goal have vastly different suitable learning materials, due to their gap in personal capabilities. The research provides an effective way to evaluate English education in the context of computer science and artificial intelligence.\n",
            "----\n",
            "Paper 286:\n",
            "Title: Spatial Metabolomics and Imaging Mass Spectrometry in the Age of Artificial Intelligence.\n",
            "Abstract: Spatial metabolomics is an emerging field of omics research that has enabled localizing metabolites, lipids, and drugs in tissue sections, a feat considered impossible just two decades ago. Spatial metabolomics and its enabling technology-imaging mass spectrometry-generate big hyper-spectral imaging data that have motivated the development of tailored computational methods at the intersection of computational metabolomics and image analysis. Experimental and computational developments have recently opened doors to applications of spatial metabolomics in life sciences and biomedicine. At the same time, these advances have coincided with a rapid evolution in machine learning, deep learning, and artificial intelligence, which are transforming our everyday life and promise to revolutionize biology and healthcare. Here, we introduce spatial metabolomics through the eyes of a computational scientist, review the outstanding challenges, provide a look into the future, and discuss opportunities granted by the ongoing convergence of human and artificial intelligence.\n",
            "----\n",
            "Paper 287:\n",
            "Title: A detailed survey on Prognostication of diabetes diagnosis on the basis of machine learning techniques and the detection approaches to diabetic retinopathy using Artificial Intelligence\n",
            "Abstract: The aim of the study is to compare, assess the optimum tools as well as the techniques and advanced features focused on prediction of diabetes diagnosis based on machine learning tactics and diabetic retinopathy using Artificial Intelligence. The literature on data science, Artificial Intelligence (AI) contains important knowledge and understanding of AI entities such as Data science, machine learning, deep learning, Medical image processing, feature extraction, classification techniques, etc. Diabetes diagnosis is a phenomenon that impacts individuals around the globe. Now, with diabetes impacting people from children to the elderly, the out-dated approaches to diabetes diagnosis should be replaced with new, time-saving technologies. There's several studies carried out by researchers to recognise and predict diabetes. Here plenty of classifiers in machine learning can be used, such as KNN, Random Tree, etc.They can save time and get more precise outcome when using these techniques to predict diabetes. Diabetic retinopathy (DR) is a typical disorder of diabetic disease that induces vision-impacting lesions in the retina. It also can turn to visual impairment if it is not addressed early. DR therapy only helps vision. Deep learning has in recent times being one of the most widely used approaches that has accomplished higher outcomes in so many fields, especially in the analysing and identification of medical image classification. In medical image processing, convolutional neural networks (CNN) using transfer learning are commonly used as a deep learning approach and they are incredibly beneficial. Key words: Diab\n",
            "----\n",
            "Paper 288:\n",
            "Title: A study on smart factory-based ambient intelligence context-aware intrusion detection system using machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 289:\n",
            "Title: Mapping the Landscape of Artificial Intelligence Applications against COVID-19\n",
            "Abstract: \n",
            " \n",
            " \n",
            "COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics. \n",
            " \n",
            " \n",
            "\n",
            "----\n",
            "Paper 290:\n",
            "Title: Leveraging Data Science to Combat COVID-19: A Comprehensive Review\n",
            "Abstract: COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.\n",
            "----\n",
            "Paper 291:\n",
            "Title: Artificial Intelligence in Biological Sciences\n",
            "Abstract: Artificial intelligence (AI), currently a cutting-edge concept, has the potential to improve the quality of life of human beings. The fields of AI and biological research are becoming more intertwined, and methods for extracting and applying the information stored in live organisms are constantly being refined. As the field of AI matures with more trained algorithms, the potential of its application in epidemiology, the study of host–pathogen interactions and drug designing widens. AI is now being applied in several fields of drug discovery, customized medicine, gene editing, radiography, image processing and medication management. More precise diagnosis and cost-effective treatment will be possible in the near future due to the application of AI-based technologies. In the field of agriculture, farmers have reduced waste, increased output and decreased the amount of time it takes to bring their goods to market due to the application of advanced AI-based approaches. Moreover, with the use of AI through machine learning (ML) and deep-learning-based smart programs, one can modify the metabolic pathways of living systems to obtain the best possible outputs with the minimal inputs. Such efforts can improve the industrial strains of microbial species to maximize the yield in the bio-based industrial setup. This article summarizes the potentials of AI and their application to several fields of biology, such as medicine, agriculture, and bio-based industry.\n",
            "----\n",
            "Paper 292:\n",
            "Title: Artificial Intelligence Techniques: Analysis, Application, and Outcome in Dentistry—A Systematic Review\n",
            "Abstract: Objective The objective of this systematic review was to investigate the quality and outcome of studies into artificial intelligence techniques, analysis, and effect in dentistry. Materials and Methods Using the MeSH keywords: artificial intelligence (AI), dentistry, AI in dentistry, neural networks and dentistry, machine learning, AI dental imaging, and AI treatment recommendations and dentistry. Two investigators performed an electronic search in 5 databases: PubMed/MEDLINE (National Library of Medicine), Scopus (Elsevier), ScienceDirect databases (Elsevier), Web of Science (Clarivate Analytics), and the Cochrane Collaboration (Wiley). The English language articles reporting on AI in different dental specialties were screened for eligibility. Thirty-two full-text articles were selected and systematically analyzed according to a predefined inclusion criterion. These articles were analyzed as per a specific research question, and the relevant data based on article general characteristics, study and control groups, assessment methods, outcomes, and quality assessment were extracted. Results The initial search identified 175 articles related to AI in dentistry based on the title and abstracts. The full text of 38 articles was assessed for eligibility to exclude studies not fulfilling the inclusion criteria. Six articles not related to AI in dentistry were excluded. Thirty-two articles were included in the systematic review. It was revealed that AI provides accurate patient management, dental diagnosis, prediction, and decision making. Artificial intelligence appeared as a reliable modality to enhance future implications in the various fields of dentistry, i.e., diagnostic dentistry, patient management, head and neck cancer, restorative dentistry, prosthetic dental sciences, orthodontics, radiology, and periodontics. Conclusion The included studies describe that AI is a reliable tool to make dental care smooth, better, time-saving, and economical for practitioners. AI benefits them in fulfilling patient demand and expectations. The dentists can use AI to ensure quality treatment, better oral health care outcome, and achieve precision. AI can help to predict failures in clinical scenarios and depict reliable solutions. However, AI is increasing the scope of state-of-the-art models in dentistry but is still under development. Further studies are required to assess the clinical performance of AI techniques in dentistry.\n",
            "----\n",
            "Paper 293:\n",
            "Title: A machine learning approach for predicting computational intensity and domain decomposition in parallel geoprocessing\n",
            "Abstract: ABSTRACT High performance computing is required for fast geoprocessing of geospatial big data. Using spatial domains to represent computational intensity (CIT) and domain decomposition for parallelism are prominent strategies when designing parallel geoprocessing applications. Traditional domain decomposition is limited in evaluating the computational intensity, which often results in load imbalance and poor parallel performance. From the data science perspective, machine learning from Artificial Intelligence (AI) shows promise for better CIT evaluation. This paper proposes a machine learning approach for predicting computational intensity, followed by an optimized domain decomposition, which divides the spatial domain into balanced subdivisions based on the predicted CIT to achieve better parallel performance. The approach provides a reference framework on how various machine learning methods including feature selection and model training can be used in predicting computational intensity and optimizing parallel geoprocessing against different cases. Some comparative experiments between the approach and traditional methods were performed using the two cases, DEM generation from point clouds and spatial intersection on vector data. The results not only demonstrate the advantage of the approach, but also provide hints on how traditional GIS computation can be improved by the AI machine learning.\n",
            "----\n",
            "Paper 294:\n",
            "Title: Libraries and Artificial Intelligence\n",
            "Abstract: Libraries are increasingly entering the digital age, and demands on them to offer more digital services are widening, with user expectations of “remote or distant access,” “distant learning,” and the use of other modern internet technologies. To this end, libraries must accelerate their use of technologies like AI, “data mining,” “machine-readable data,” “machine-generated classification,” “semantic ontologies,” and internet accessible catalogs and content because their aim should always be user benefit, user convenience, and user satisfaction. In this chapter, the author examines ways in which technologies and libraries are trying to fulfill their modern role and expectations of the modern user. Additionally, the author will examine how to strengthen data ethics in those particular fields of library use that most endanger the user's intellectual freedom on one side and his right to privacy on the other. One of the essential roles of modern libraries, in their new “informational” identity, will be as “guardians of data ethics and intellectual freedom.”\n",
            "----\n",
            "Paper 295:\n",
            "Title: How can Big Data and machine learning benefit environment and water management: a survey of methods, applications, and future directions\n",
            "Abstract: Big Data and machine learning (ML) technologies have the potential to impact many facets of environment and water management (EWM). Big Data are information assets characterized by high volume, velocity, variety, and veracity. Fast advances in high-resolution remote sensing techniques, smart information and communication technologies, and social media have contributed to the proliferation of Big Data in many EWM fields, such as weather forecasting, disaster management, smart water and energy management systems, and remote sensing. Big Data brings about new opportunities for data-driven discovery in EWM, but it also requires new forms of information processing, storage, retrieval, as well as analytics. ML, a subdomain of artificial intelligence (AI), refers broadly to computer algorithms that can automatically learn from data. ML may help unlock the power of Big Data if properly integrated with data analytics. Recent breakthroughs in AI and computing infrastructure have led to the fast development of powerful deep learning (DL) algorithms that can extract hierarchical features from data, with better predictive performance and less human intervention. Collectively Big Data and ML techniques have shown great potential for data-driven decision making, scientific discovery, and process optimization. These technological advances may greatly benefit EWM, especially because (1) many EWM applications (e.g. early flood warning) require the capability to extract useful information from a large amount of data in autonomous manner and in real time, (2) EWM researches have become highly multidisciplinary, and handling the ever increasing data volume/types using the traditional workflow is simply not an option, and last but not least, (3) the current theoretical knowledge about many EWM processes is still incomplete, but which may now be complemented through data-driven discovery. A large number of applications on Big Data and ML have already appeared in the EWM literature in recent years. The purposes of this survey are to (1) examine the potential and benefits of data-driven research in EWM, (2) give a synopsis of key concepts and approaches in Big Data and ML, (3) provide a systematic review of current applications, and finally (4) discuss major issues and challenges, and recommend future research directions. EWM includes a broad range of research topics. Instead of attempting to survey each individual area, this review focuses on areas of nexus in EWM, with an emphasis on elucidating the potential benefits of increased data availability and predictive analytics to improving the EWM research.\n",
            "----\n",
            "Paper 296:\n",
            "Title: Machine Learning and Digital Heritage: The CEPROQHA Project Perspective\n",
            "Abstract: None\n",
            "----\n",
            "Paper 297:\n",
            "Title: Machine learning for disease surveillance or outbreak monitoring: A review\n",
            "Abstract: Amidst the global pandemic, the methodical acquisition, analysis, and evaluation of health-related data is crucial to learn from experience and strengthen preparedness for future challenges of similar nature. The goal of this paper is to survey the recent approaches to disease surveillance or outbreak monitoring in the context of artificial intelligence or machine learning. Utilizing Elsevier's Scopus database, the keywords, “Disease Surveillance” or “Outbreak Monitoring” yielded 12,648 document results (with year duration starting 2016). Then, the documents were reduced to 367 after conjunction with the terms, namely, “Artificial Intelligence” or “Machine Learning (ML)” or “Data Science” and limiting the document type to article and review only. The documents were examined one-by-one leaving only the most recent and those papers which applied machine learning methods. The survey showed four major ML tasks in the recent literature, namely, topic modeling, time series forecasting & regression, classification, and clustering. It was also observed that many disease surveillance systems are still utilizing traditional (shallow) machine learning techniques. However, deep learning-based techniques was also demonstrated, in the literature, to perform better than the traditional models.\n",
            "----\n",
            "Paper 298:\n",
            "Title: Interpretability of Machine Learning Solutions in Public Healthcare: The CRISP-ML Approach\n",
            "Abstract: Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process.\n",
            "----\n",
            "Paper 299:\n",
            "Title: Performance analysis of machine learning based optimized feature selection approaches for breast cancer diagnosis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 300:\n",
            "Title: Interactive Visualizations to Introduce Data Science for High School Students\n",
            "Abstract: Artificial intelligence integrates with our daily life in more ways each day with the invention of new tools and applications. For example, businesses utilize machine learning, which is a subfield of artificial intelligence, to identify the ideal customer for targeted advertisement. Machine learning and data science scale previous manual problems to a wider customer base. With all the various applications for artificial intelligence, there is still a noticeable gap in the number of qualified job applicants who can solve some of the machine learning problems of the day. There is a movement to introduce computer science education earlier to K-12 students. One motivation behind an earlier introduction is to better prepare students to study computer science at universities and fulfill these coveted technology positions in the future. Given this motivation, there needs to be more computer science instructional activities and lesson plans to support K-12 teachers. Currently there are several new tools and curricula available for computer science. However, there is a need for beginner and K-12 accessible data science and machine learning instructional methods. These topics are advanced computer science electives but an earlier introduction can still benefit students. Interactive visual activities support high school students to learn challenging topics. In this paper, we consider how a visual analytic process can present difficult data science concepts. We present an interactive visual educational tool to teach data science to high school students. The first activity introduces the visual analytic pipeline. The next interactive activities present machine learning classification and regression topics. We select datasets and applications which are relatable to our main audience. This paper describes the design and implementation of the data science educational tool for high school students.\n",
            "----\n",
            "Paper 301:\n",
            "Title: A comprehensive study on artificial intelligence in oil and gas sector\n",
            "Abstract: None\n",
            "----\n",
            "Paper 302:\n",
            "Title: Using Artificial Intelligence to Improve Real-Time Decision-Making for High-Impact Weather\n",
            "Abstract: AbstractHigh-impact weather events, such as severe thunderstorms, tornadoes, and hurricanes, cause significant disruptions to infrastructure, property loss, and even fatalities. High-impact events can also positively impact society, such as the impact on savings through renewable energy. Prediction of these events has improved substantially with greater observational capabilities, increased computing power, and better model physics, but there is still significant room for improvement. Artificial intelligence (AI) and data science technologies, specifically machine learning and data mining, bridge the gap between numerical model prediction and real-time guidance by improving accuracy. AI techniques also extract otherwise unavailable information from forecast models by fusing model output with observations to provide additional decision support for forecasters and users. In this work, we demonstrate that applying AI techniques along with a physical understanding of the environment can significantly improve ...\n",
            "----\n",
            "Paper 303:\n",
            "Title: Hyperspectral imaging and artificial intelligence enhance remote phenotyping of grapevine rootstock influence on whole vine photosynthesis\n",
            "Abstract: Rootstocks are gaining importance in viticulture as a strategy to combat abiotic challenges, as well as enhancing scion physiology. Photosynthetic parameters such as maximum rate of carboxylation of RuBP (Vcmax) and the maximum rate of electron transport driving RuBP regeneration (Jmax) have been identified as ideal targets for potential influence by rootstock and breeding. However, leaf specific direct measurement of these photosynthetic parameters is time consuming, limiting the information scope and the number of individuals that can be screened. This study aims to overcome these limitations by employing hyperspectral imaging combined with artificial intelligence (AI) to predict these key photosynthetic traits at the canopy level. Hyperspectral imaging captures detailed optical properties across a broad range of wavelengths (400 to 1000 nm), enabling use of all wavelengths in a comprehensive analysis of the entire vine’s photosynthetic performance (Vcmax and Jmax). Artificial intelligence-based prediction models that blend the strength of deep learning and machine learning were developed using two growing seasons data measured post-solstice at 15 h, 14 h, 13 h and 12 h daylengths for Vitis hybrid ‘Marquette’ grafted to five commercial rootstocks and ‘Marquette’ grafted to ‘Marquette’. Significant differences in photosynthetic efficiency (Vcmax and Jmax) were noted for both direct and indirect measurements for the six rootstocks, indicating that rootstock genotype and daylength have a significant influence on scion photosynthesis. Evaluation of multiple feature-extraction algorithms indicated the proposed Vitis base model incorporating a 1D-Convolutional neural Network (CNN) had the best prediction performance with a R2 of 0.60 for Vcmax and Jmax. Inclusion of weather and chlorophyll parameters slightly improved model performance for both photosynthetic parameters. Integrating AI with hyperspectral remote phenotyping provides potential for high-throughput whole vine assessment of photosynthetic performance and selection of rootstock genotypes that confer improved photosynthetic performance potential in the scion.\n",
            "----\n",
            "Paper 304:\n",
            "Title: Artificial Intelligence Algorithms to Diagnose Glaucoma and Detect Glaucoma Progression: Translation to Clinical Practice\n",
            "Abstract: Purpose This concise review aims to explore the potential for the clinical implementation of artificial intelligence (AI) strategies for detecting glaucoma and monitoring glaucoma progression. Methods Nonsystematic literature review using the search combinations “Artificial Intelligence,” “Deep Learning,” “Machine Learning,” “Neural Networks,” “Bayesian Networks,” “Glaucoma Diagnosis,” and “Glaucoma Progression.” Information on sensitivity and specificity regarding glaucoma diagnosis and progression analysis as well as methodological details were extracted. Results Numerous AI strategies provide promising levels of specificity and sensitivity for structural (e.g. optical coherence tomography [OCT] imaging, fundus photography) and functional (visual field [VF] testing) test modalities used for the detection of glaucoma. Area under receiver operating curve (AROC) values of > 0.90 were achieved with every modality. Combining structural and functional inputs has been shown to even more improve the diagnostic ability. Regarding glaucoma progression, AI strategies can detect progression earlier than conventional methods or potentially from one single VF test. Conclusions AI algorithms applied to fundus photographs for screening purposes may provide good results using a simple and widely accessible test. However, for patients who are likely to have glaucoma more sophisticated methods should be used including data from OCT and perimetry. Outputs may serve as an adjunct to assist clinical decision making, whereas also enhancing the efficiency, productivity, and quality of the delivery of glaucoma care. Patients with diagnosed glaucoma may benefit from future algorithms to evaluate their risk of progression. Challenges are yet to be overcome, including the external validity of AI strategies, a move from a “black box” toward “explainable AI,” and likely regulatory hurdles. However, it is clear that AI can enhance the role of specialist clinicians and will inevitably shape the future of the delivery of glaucoma care to the next generation. Translational Relevance The promising levels of diagnostic accuracy reported by AI strategies across the modalities used in clinical practice for glaucoma detection can pave the way for the development of reliable models appropriate for their translation into clinical practice. Future incorporation of AI into healthcare models may help address the current limitations of access and timely management of patients with glaucoma across the world.\n",
            "----\n",
            "Paper 305:\n",
            "Title: Application of Information and Technology in Supply Chain Management: Case Study of Artificial Intelligence – A Mini Review\n",
            "Abstract: It is wide known that the world has been moving towards a digital future over the years, and Industry 4.0 technologies are considered to be the way of the future. One of the most prominent of these technologies (including Block Chain, Internet of Things, Cloud Computing, Big Data, etc.) is Artificial Intelligence (AI), was introduced to develop and create “thinking machines” that are capable of mimicking, learning, and replacing human intelligence. However, its widespread acceptance as a decision-aid tool, AI has seen limited application in supply chain management (SCM). The purpose of this work is to identify the contributions of AI to SCM through a brief review of the existing literature. Besides, this paper reviews the past record of success in AI applications to SCM and identifies the most subfields of SCM in which to apply AI.\n",
            "----\n",
            "Paper 306:\n",
            "Title: The Diagnosis of Chronic Liver Disease using Machine Learning Techniques\n",
            "Abstract: In the 21st-century, the issue of liver disease has been increasing all over the world. As per the latest survey report, liver disease death toll has been rise approximately 2 million per year worldwide. The overall percentage of death by liver disease is 3.5% worldwide. Chronic Liver disease is also considered to be one of the deadly diseases, so early detection and treatment can recover the disease easily. Due to rapid advancement in Artificial intelligence (AI), like various machine learning algorithms SVM, K-mean clustering, KNN, Random forest, Logistic regression, etc., This will improve the life span of a patient suffering from Chronic Liver Disease (CLD) in early stages. The data can be obtained in a large volume due to the broad exploitation of bar codes for supreme marketable products, the mechanization of various business and government dealings, and the development in the data collection tools. This research work is based on liver disease prediction using machine learning algorithms. Liver disease prediction has various levels of steps involved, pre-processing, feature extraction, and classification. In this s research work, a hybrid classification method is proposed for liver disease prediction. And Datasets are collected from the Kaggle database of Indian liver patient records. The proposed model achieved an accuracy of 77.58%. The proposed technique is implemented in Python with the Spyder tool and results are analyzed in terms of accuracy, precision, and recall. \n",
            " \n",
            "----\n",
            "Paper 307:\n",
            "Title: A machine learning approximation of the 2015 Portuguese high school student grades: A hybrid approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 308:\n",
            "Title: Mobile Data Science and Intelligent Apps: Concepts, AI-Based Modeling and Research Directions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 309:\n",
            "Title: The Role of Artificial Intelligence (AI) in Condition Monitoring and Diagnostic Engineering Management (COMADEM): A Literature Survey\n",
            "Abstract: Artificial Intelligence (AI) is playing a dominant role in the 21st century. Organizations have more data than ever, so it’s crucial to ensure that the analytics team should differentiate between Interesting Data and Useful Data. Amongst the important aspects in Machine Learning are “Feature Selection” and “Feature Extraction”. We are now witnessing the emerging fourth industrial revolution and a considerable number of evolutionary changes in machine learning methodologies to achieve operational excellence in operating and maintaining the industrial assets efficiently, reliably, safely and cost-effectively. AI techniques such as, knowledge based systems, expert systems, artificial neural networks, genetic algorithms, fuzzy logic, case-based reasoning and any combination of these techniques (hybrid systems), machine learning, biomimicry such as swarm intelligence and distributed intelligence. are widely used by multi-disciplinarians to solve a whole range of hitherto intractable problems associated with the proactive maintenance management of industrial assets. In this paper, an attempt is made to review the role of artificial intelligence in condition monitoring and diagnostic engineering management of modern engineering assets. The paper also highlights that unethical and immoral misuse of AI is dangerous.\n",
            "----\n",
            "Paper 310:\n",
            "Title: Fuzzy Sets in Data Analysis: From Statistical Foundations to Machine Learning\n",
            "Abstract: Basic ideas and formal concepts from fuzzy sets and fuzzy logic have been used successfully in various branches of science and engineering. This paper elaborates on the use of fuzzy sets in the broad field of data analysis and statistical sciences, including modern manifestations such as data mining and machine learning. In the fuzzy logic community, this branch of research has recently gained in importance, especially due to the emergence of data science as a new scientific discipline, and the increasing relevance of machine learning as a key methodology of modern artificial intelligence. This development has been accompanied by an internal shift from largely knowledge-based to strongly data-driven fuzzy modeling and systems design. Reflecting on the historical dimension and evolution of the area, we discuss the role of fuzzy logic in data analysis and related fields, highlight existing contributions of fuzzy sets in these fields, and outline interesting directions for future work.\n",
            "----\n",
            "Paper 311:\n",
            "Title: Towards the Integration of Artificial Intelligence in Higher Education, Challenges and Opportunities: The African Context, a Case of Zimbabwe.\n",
            "Abstract: This paper seeks to address the challenges of underdevelopment bedeviling the African continent in general and Zimbabwe in particular on the integration of Artificial Intelligence (AI) in higher education. Universities in Africa have over the years produced many graduates in Science, Technology, Engineering, and Mathematics (STEM) related learning areas but continue to grapple with underdevelopment both economically and intellectually. Part of the problem has been identified as a lack of quality in the education system that is not able to produce graduates with adequate 21st-century skills for the achievement of the Sustainable Development Goals (SDGs) to fulfil the Global Agenda 2030. Achieving this is possible if Africa moves a step forward and embraces modern technologies like the integration of AI in her education system. A technologically astute graduate would be able to apply their knowledge in achieving the much-needed sustainable economic development. This article highlights how the implementation of AI in higher education can foster a turnaround in Africa’s education systems and lead to industrialisation and innovation. Pivotal aspects of AI like the Turing machine test, cloud computing, big data, and machine learning are explored in an attempt to add value to the current education discourse. This study is situated in the interpretive paradigm, the qualitative approach was employed to gather data using several methods like e-questionnaires (open-ended), document analysis, and review of journals and other academic publications. Purposive sampling of STEM educators from a renowned university in Zimbabwe was undertaken to gather information on the ground and the data was thematically analysed. The article argues that Africa has the potential to be a leader in Industrialization and commerce should the continent fully embrace AI in higher education.\n",
            "----\n",
            "Paper 312:\n",
            "Title: Towards Machine Learning as an Enabler of Computational Creativity\n",
            "Abstract: Computational creativity composes a collection of activities that are capable of achieving or simulating behaviors, which can be deemed creative. A frequently articulated criticism for related systems is that the creative capability yet remains with the software designer rather than the computational creative system itself. The rise of machine learning (ML) enables new ways of combining, exploring, and transforming conceptual spaces to achieve creative results. This article demonstrates that the learning occurring within the computational machine through ML enables creative capabilities therein, allowing the computational creative system to be more creative on its own than ever before. Thus, we perceive ML as a key enabler of computational creativity. In this article, we consolidate research from the computer science, computational creativity, and information systems communities, which has been treated separately so far. We build on a framework of human creativity to examine the relationship between creative capabilities and ML mechanisms in ML-based computational creative systems. Specifically, we explicate, which creative capabilities are already established through ML mechanisms in computational creative systems as strengths. Furthermore, we explicate challenges pointing towards further potential of ML-based computational creative systems to enhance the inherent creative capabilities. Our results reveal that ML-based computational creative systems advance the previously static and explicit principles of non-ML-based computational creative systems, yielding creative capabilities on the machine's own, which yet have been in the realm of human actors. Impact Statement—Creativity is a core organizational skill as it enables innovation. With markets becoming increasingly volatile, constant innovation is essential for organizations to create and sustain their economic competitive advantage. Due to the undisputable relevance of creativity for individuals, organizations, and societies, artificial intelligence research has set out to enable creative behavior in computational systems. Especially, ML methods lately became a highly frequented means within computational creative systems. However, while oftentimes applied, ML is seldomly explicitly assessed for its potential to facilitate computational creativity. In this article, we analyze extant computational creative systems relying on ML. The findings serve as a guidance for the design of these systems in various contexts and as pointers for future research to advance the creative capabilities of such systems.\n",
            "----\n",
            "Paper 313:\n",
            "Title: Analyzing and Predicting Students’ Performance by Means of Machine Learning: A Review\n",
            "Abstract: Predicting students’ performance is one of the most important topics for learning contexts such as schools and universities, since it helps to design effective mechanisms that improve academic results and avoid dropout, among other things. These are benefited by the automation of many processes involved in usual students’ activities which handle massive volumes of data collected from software tools for technology-enhanced learning. Thus, analyzing and processing these data carefully can give us useful information about the students’ knowledge and the relationship between them and the academic tasks. This information is the source that feeds promising algorithms and methods able to predict students’ performance. In this study, almost 70 papers were analyzed to show different modern techniques widely applied for predicting students’ performance, together with the objectives they must reach in this field. These techniques and methods, which pertain to the area of Artificial Intelligence, are mainly Machine Learning, Collaborative Filtering, Recommender Systems, and Artificial Neural Networks, among others.\n",
            "----\n",
            "Paper 314:\n",
            "Title: Big data machine learning using apache spark MLlib\n",
            "Abstract: Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.\n",
            "----\n",
            "Paper 315:\n",
            "Title: Effectiveness of Artificial Intelligence Models for Cardiovascular Disease Prediction: Network Meta-Analysis\n",
            "Abstract: Heart failure is the most common cause of death in both males and females around the world. Cardiovascular diseases (CVDs), in particular, are the main cause of death worldwide, accounting for 30% of all fatalities in the United States and 45% in Europe. Artificial intelligence (AI) approaches such as machine learning (ML) and deep learning (DL) models are playing an important role in the advancement of heart failure therapy. The main objective of this study was to perform a network meta-analysis of patients with heart failure, stroke, hypertension, and diabetes by comparing the ML and DL models. A comprehensive search of five electronic databases was performed using ScienceDirect, EMBASE, PubMed, Web of Science, and IEEE Xplore. The search strategy was performed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) statement. The methodological quality of studies was assessed by following the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) guidelines. The random-effects network meta-analysis forest plot with categorical data was used, as were subgroups testing for all four types of treatments and calculating odds ratio (OR) with a 95% confidence interval (CI). Pooled network forest, funnel plots, and the league table, which show the best algorithms for each outcome, were analyzed. Seventeen studies, with a total of 285,213 patients with CVDs, were included in the network meta-analysis. The statistical evidence indicated that the DL algorithms performed well in the prediction of heart failure with AUC of 0.843 and CI [0.840–0.845], while in the ML algorithm, the gradient boosting machine (GBM) achieved an average accuracy of 91.10% in predicting heart failure. An artificial neural network (ANN) performed well in the prediction of diabetes with an OR and CI of 0.0905 [0.0489; 0.1673]. Support vector machine (SVM) performed better for the prediction of stroke with OR and CI of 25.0801 [11.4824; 54.7803]. Random forest (RF) results performed well in the prediction of hypertension with OR and CI of 10.8527 [4.7434; 24.8305]. The findings of this work suggest that the DL models can effectively advance the prediction of and knowledge about heart failure, but there is a lack of literature regarding DL methods in the field of CVDs. As a result, more DL models should be applied in this field. To confirm our findings, more meta-analysis (e.g., Bayesian network) and thorough research with a larger number of patients are encouraged.\n",
            "----\n",
            "Paper 316:\n",
            "Title: Artificial Intelligence and Law\n",
            "Abstract: None\n",
            "----\n",
            "Paper 317:\n",
            "Title: Machine Learning and Deep Learning\n",
            "Abstract: Now-a-days artificial intelligence has become an asset for engineering and experimental studies, just like statistics and calculus. Data science is a growing field for researchers and artificial intelligence, machine learning and deep learning are roots of it. This paper describes the relation between these roots of data science. There is a need of machine learning if any kind of analysis is to be performed. This study describes machine learning from the scratch. It also focuses on Deep Learning. Deep learning can also be known as new trend of machine learning. This paper gives a light on basic architecture of Deep learning. A comparative study of machine learning and deep learning is also given in the paper and allows researcher to have a broad view on these techniques so that they can understand which one will be preferable solution for a particular problem.\n",
            "----\n",
            "Paper 318:\n",
            "Title: Machine Learning in Agriculture: A Review\n",
            "Abstract: Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.\n",
            "----\n",
            "Paper 319:\n",
            "Title: An Artificial Intelligence Approach for Deploying Zero Trust Architecture (ZTA)\n",
            "Abstract: Cybersecurity is critical in preventing infractions, maintaining digital workplace discipline, and ensuring that laws and regulations are obeyed. Zero Trust Architecture (ZTA), often known as perimeter-less security, is a novel method for designing and implementing secured IT systems. Zero trust's basic notion is “never trust, always verify,” which indicates that devices should not be trusted by default. This means that each access from or to any asset must be assessed and follow the standard guidelines of the organization. Maintaining this type of control imposes a high burden on IT security and system administrators to be able to track and validate each control and manually sustain the configuration needed. With the power of Classification Algorithms in Machine Learning, we will explore in this paper an alternative solution to save time and effort and help maintain the same security posture with less human intervention. The proposed approach utilizes the information from available security feeds and statically configured policies to enforce and maintain zero-trust network policies. By analyzing the data, it will be feasible to identify the required policies to be configured and compare them against the traditional compliance rules to auto-configure the policies. This approach aims to enhance the existing security intelligence engines with more sophisticated rules and less time and effort.\n",
            "----\n",
            "Paper 320:\n",
            "Title: Three-dimensional vectorial holography based on machine learning inverse design\n",
            "Abstract: 3D vectorial holography reconstructs an arbitrary 3D vectorial field–carrying wavefront using machine learning inverse design. The three-dimensional (3D) vectorial nature of electromagnetic waves of light has not only played a fundamental role in science but also driven disruptive applications in optical display, microscopy, and manipulation. However, conventional optical holography can address only the amplitude and phase information of an optical beam, leaving the 3D vectorial feature of light completely inaccessible. We demonstrate 3D vectorial holography where an arbitrary 3D vectorial field distribution on a wavefront can be precisely reconstructed using the machine learning inverse design based on multilayer perceptron artificial neural networks. This 3D vectorial holography allows the lensless reconstruction of a 3D vectorial holographic image with an ultrawide viewing angle of 94° and a high diffraction efficiency of 78%, necessary for floating displays. The results provide an artificial intelligence–enabled holographic paradigm for harnessing the vectorial nature of light, enabling new machine learning strategies for holographic 3D vectorial fields multiplexing in display and encryption.\n",
            "----\n",
            "Paper 321:\n",
            "Title: Constructing a Software Tool for Detecting Face Mask-wearing by Machine Learning\n",
            "Abstract: In the pandemic era of COVID19, software engineering and artificial intelligence tools played a major role in monitoring, managing, and predicting the spread of the virus. According to reports released by the World Health Organization, all attempts to prevent any form of infection are highly recommended among people. One side of avoiding infection is requiring people to wear face masks. The problem is that some people do not incline to wear a face mask, and guiding them manually by police is not easy especially in a large or public area to avoid this infection. The purpose of this paper is to construct a software tool called Face Mask Detection (FMD) to detect any face that does not wear a mask in a specific public area by using CCTV (closed-circuit television). The problem also occurs in case the software tool is inaccurate. The technique of this notion is to use large data of face images, some faces are wearing masks, and others are not wearing masks. The methodology is by using machine learning, which is characterized by a HOG (histogram orientation gradient) for extraction of features, then an SVM(support vector machine) for classification, as it can contribute to the literature and enhance mask detection accuracy. Several public datasets for masked and unmasked face images have been used in the experiments. The findings for accuracy are as follows: 97.00%, 100.0%, 97.50%, 95.0% for RWMFD (Real-world Masked Face Dataset)& GENK14k, SMFDB (Simulated Masked Face Recognition Dataset), MFRD (Masked Face Recognition Dataset), and MAFA (MAsked FAces)& GENK14k for databases, respectively. The results are promising as a comparison of this work has been made with the state-of-the-art. The workstation of this research used a webcam programmed by Matlab for real-time testing. © 2022 University of Baghdad. All rights reserved.\n",
            "----\n",
            "Paper 322:\n",
            "Title: Skin Disease Classification System Based on Machine Learning Technique: A Survey\n",
            "Abstract: Skin diseases are a major and worrying problem in societies due to their physical and psychological effects on patients. Detecting skin diseases at an early stage has an important role in treatment. The process of diagnosing and treating skin injury is related to the skill and experience of the specialist doctor. The diagnostic process must be accurate and timely. Recently, artificial intelligence science has been used in the field of diagnosing skin diseases through the use of machine learning algorithms and the exploitation of the vast amount of data available in health centers and hospitals. In this paper, quite many previous studies related to methods of classification of skin diseases based on the principle of machine learning were collected. In a group of previous studies, the researchers used some systems, mechanisms, and algorithms. Several systems have been successful in classifying skin diseases and achieving varying diagnostic accuracy. Various systems have relied on methods of image processing and feature extraction that help predict and detect disease type. There are other systems designed to identify specific types of skin disease through clinical features and features obtained from tissue analyzes after a skin biopsy of the affected area. This survey shows that the diagnostic accuracy in image processing methods was relatively uneven, ranged between (50% to 100%). As for the methods of treating tissue features, the accuracy was of an excellent level of 94% or more. The results provide an overview of the actual relevant studies found in the literature and highlight most of which research gaps have emerged.\n",
            "----\n",
            "Paper 323:\n",
            "Title: Computational intelligence based machine learning methods for rule-based reasoning in computer vision applications\n",
            "Abstract: In robot control, rule discovery for understanding of data is of critical importance. Basically, understanding of data depends upon logical rules, similarity evaluation and graphical methods. The expert system collects training examples separately by exploring an anonymous environment by using machine learning techniques. In dynamic environments, future actions are determined by sequences of perceptions thus encoded as rule base. This paper is focused on demonstrating the extraction and application of logical rules for image understanding, using newly developed Synergistic Fibroblast Optimization (SFO) algorithm with well-known existing artificial learning methods. The SFO algorithm is tested in two modes: Michigan and Pittsburgh approach. Optimal rule discovery is evaluated by describing continuous data and verifying accuracy and error level at optimization phase. In this work, Monk's problem is solved by discovering optimal rules that enhance the generalization and comprehensibility of a robot classification system in classifying the objects from extracted attributes to effectively categorize its domain.\n",
            "----\n",
            "Paper 324:\n",
            "Title: Radiomics and Machine Learning in Oral Healthcare\n",
            "Abstract: The increasing storage of information, data, and forms of knowledge has led to the development of new technologies that can help to accomplish complex tasks in different areas, such as in dentistry. In this context, the role of computational methods, such as radiomics and Artificial Intelligence (AI) applications, has been progressing remarkably for dentomaxillofacial radiology (DMFR). These tools bring new perspectives for diagnosis, classification, and prediction of oral diseases, treatment planning, and for the evaluation and prediction of outcomes, minimizing the possibilities of human errors. A comprehensive review of the state‐of‐the‐art of using radiomics and machine learning (ML) for imaging in oral healthcare is presented in this paper. Although the number of published studies is still relatively low, the preliminary results are very promising and in a near future, an augmented dentomaxillofacial radiology (ADMFR) will combine the use of radiomics‐based and AI‐based analyses with the radiologist's evaluation. In addition to the opportunities and possibilities, some challenges and limitations have also been discussed for further investigations.\n",
            "----\n",
            "Paper 325:\n",
            "Title: Machine Learning and Psychological Research: The Unexplored Effect of Measurement\n",
            "Abstract: Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science.\n",
            "----\n",
            "Paper 326:\n",
            "Title: Sign language recognition using artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 327:\n",
            "Title: Supervised machine learning predictive analytics for alumni income\n",
            "Abstract: None\n",
            "----\n",
            "Paper 328:\n",
            "Title: The Role of Machine Learning in E-Learning Using the Web and AI-Enabled Mobile Applications\n",
            "Abstract: For over two decades, e-learning has been recognized as a flexible and faster method compared to the other established methods, especially in enhancing knowledge. Concurrently, the expansion of information technology applications, such as mobile applications and Artificial Intelligence (AI), has provided well-grounded foundations for e-learning to be more reachable. In particular, education can be seen as the most beneficial sector of advancements in e-learning. Machine learning is considered a form of personalized learning that could be used to give each student a specific personal experience through which students are directed to gain their own experience. Web and AI-enabled mobile applications can be recognized as one of the most broadly used platforms for e-learning where machine learning technology can be applied to measure many influences and predictions regarding the quality of e-learning, but we cannot ignore the complexities of use. This study shows the role of machine learning in the user’s ability to make use of the course and its contents to measure ease and clarity. Based on a former study shown previously, this paper attempts to pinpoint realities and complexities associated with web and AI-enabled mobile applications by evaluating user preferences. This paper forms the second phase using two user groups (21–30 years) where data were attained using a survey questionnaire to investigate the user preferences when using an application for e-learning. The analysis shows that the future of e-learning has greater potential in web-based applications, as they have more scope for development and improvements compared to mobile applications. The paper concludes with a conceptual framework that works as a machine that stimulates different information and uses e-learning applications that support artificial intelligence techniques. This research provides a solid underpinning for further research into the future of AI-enabled e-learning education and its implication with respect to cost, quality, and usability.\n",
            "----\n",
            "Paper 329:\n",
            "Title: Updating the Checklist for Artificial Intelligence in Medical Imaging (CLAIM) for reporting AI research\n",
            "Abstract: None\n",
            "----\n",
            "Paper 330:\n",
            "Title: Deploying the Big Data Science Center at the Shanghai Synchrotron Radiation Facility: the first superfacility platform in China\n",
            "Abstract: With recent technological advances, large-scale experimental facilities generate huge datasets, into the petabyte range, every year, thereby creating the Big Data deluge effect. Data management, including the collection, management, and curation of these large datasets, is a significantly intensive precursor step in relation to the data analysis that underpins scientific investigations. The rise of artificial intelligence (AI), machine learning (ML), and robotic automation has changed the landscape for experimental facilities, producing a paradigm shift in how different datasets are leveraged for improved intelligence, operation, and data analysis. Therefore, such facilities, known as superfacilities, which fully enable user science while addressing the challenges of the Big Data deluge, are critical for the scientific community. In this work, we discuss the process of setting up the Big Data Science Center within the Shanghai Synchrotron Radiation Facility (SSRF), China’s first superfacility. We provide details of our initiatives for enabling user science at SSRF, with particular consideration given to recent developments in AI, ML, and robotic automation.\n",
            "----\n",
            "Paper 331:\n",
            "Title: Material Classification via Machine Learning Techniques: Construction Projects Progress Monitoring\n",
            "Abstract: Nowadays, the construction industry is on a fast track to adopting digital processes under the Industrial Revolution (IR) 4.0. The desire to automate maximum construction processes with less human interference has led the industry and research community to inclined towards artificial intelligence. This chapter has been themed on automated construction monitoring practices by adopting material classification via machine learning (ML) techniques. The study has been conducted by following the structure review approach to gain an understanding of the applications of ML techniques for construction progress assessment. Data were collected from the Web of Science (WoS) and Scopus databases, concluding 14 relevant studies. The literature review depicted the support vector machine (SVM) and artificial neural network (ANN) techniques as more effective than other ML techniques for material classification. The last section of this chapter includes a python-based ANN model for material classification. This ANN model has been tested for construction items (brick, wood, concrete block, and asphalt) for training and prediction. Moreover, the predictive ANN model results have been shared for the readers, along with the resources and open-source web links.\n",
            "----\n",
            "Paper 332:\n",
            "Title: Role of biological Data Mining and Machine Learning Techniques in Detecting and Diagnosing the Novel Coronavirus (COVID-19): A Systematic Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 333:\n",
            "Title: Artificial intelligence in clinical and translational science: Successes, challenges and opportunities\n",
            "Abstract: Artificial intelligence (AI) is transforming many domains, including finance, agriculture, defense, and biomedicine. In this paper, we focus on the role of AI in clinical and translational research (CTR), including preclinical research (T1), clinical research (T2), clinical implementation (T3), and public (or population) health (T4). Given the rapid evolution of AI in CTR, we present three complementary perspectives: (1) scoping literature review, (2) survey, and (3) analysis of federally funded projects. For each CTR phase, we addressed challenges, successes, failures, and opportunities for AI. We surveyed Clinical and Translational Science Award (CTSA) hubs regarding AI projects at their institutions. Nineteen of 63 CTSA hubs (30%) responded to the survey. The most common funding source (48.5%) was the federal government. The most common translational phase was T2 (clinical research, 40.2%). Clinicians were the intended users in 44.6% of projects and researchers in 32.3% of projects. The most common computational approaches were supervised machine learning (38.6%) and deep learning (34.2%). The number of projects steadily increased from 2012 to 2020. Finally, we analyzed 2604 AI projects at CTSA hubs using the National Institutes of Health Research Portfolio Online Reporting Tools (RePORTER) database for 2011–2019. We mapped available abstracts to medical subject headings and found that nervous system (16.3%) and mental disorders (16.2) were the most common topics addressed. From a computational perspective, big data (32.3%) and deep learning (30.0%) were most common. This work represents a snapshot in time of the role of AI in the CTSA program.\n",
            "----\n",
            "Paper 334:\n",
            "Title: Big data and artificial intelligence application in energy field: a bibliometric analysis\n",
            "Abstract: None\n",
            "----\n",
            "Paper 335:\n",
            "Title: Forecasting of Heart Diseases in Early Stages Using Machine Learning Approaches\n",
            "Abstract: In the area of computer science, the terms “machine learning” as well as “data science” are no longer new buzzwords. People have begun to employ it in their apps. Machine Learning (ML) has become the most widely utilized method for web sites to classify visitors and provide relevant responses. ML is primarily an area of Artificial Intelligence(AI) that has been a crucial component of digitalization solutions that has gotten a lot of buzz in the digital world. In this work, ML is utilized to determine whether or not a person has cardiac disease. ML may be used to determine if a person has a cardiovascular illness based on particular characteristics such as chest discomfort, cholesterol levels, age, and other factors. Cardiovascular disease diagnosis can be simplified using ML classification algorithms that are based on supervised learning. To distinguish those with cardiac illness from those who do not, many ML algorithms are being used including K-Nearest Neighbor (KNN), Decision Tree classifier as well as Support Vector Classifiers. The dataset contains certain irrelevant features that are removed during the data cleaning stage, and the data is also standardized for better results. The results and analyses of the publicly available ML Heart Disease UCI dataset are being compared in this paper using different ML methods. The accuracy as well as confusion matrix are also being used to validate a good amount of promising outcomes.\n",
            "----\n",
            "Paper 336:\n",
            "Title: A Forecast of Coronary Heart Disease using Proficient Machine Learning Algorithms\n",
            "Abstract: Coronary Heart Diseases (CHDs) are a fundamental explanation of enormous deaths on earth in the last decades and are a dangerous disease in India and worldwide and Coronary Heart Disease has developed as one of the most unmistakable and uninformed reasons for death all around the globe. Thus, a dependable, precise & achievable framework for analyzing these maladies for appropriate therapy. Artificial Intelligence evaluations & systems are being used to restore data collections to robotize investigation within enormous & uneasy information. Numerous scientists, as of late, have been utilizing a few Artificial Intelligence methods to facilitate well-being for industry & professionals analysis of coronary-disease infections. This work intends to make use of chronological medical data to forecast CHD using Machine Learning. The work introduces machine learning techniques of different models dependent on calculations, procedures, and analyzes exhibition. Also, in this paper three supervised learning methods: Linear Regression using stochastic gradient descent and Decision Tree to find out the relationship in CHD data to improve prediction rate.\n",
            "----\n",
            "Paper 337:\n",
            "Title: Artificial Intelligence Based Predictive Analysis of Customer Churn\n",
            "Abstract: Deep learning has been evidenced to be a cutting-edge technology for big data scrutiny with a huge figure of effective cases in image processing, speech recognition, object detection, and so on. Lately, it has also been acquainted with in food science and business. In this paper, a fleeting overview of deep learning and detailly labelled the structure of some prevalent constructions of deep neural networks and the method for training a model is provided. Various techniques that used deep learning as the data analysis tool are analyzed to answer the complications and challenges in food sphere together with quality detection of fruits & vegetables. The precise difficulties, the datasets, the pre-processing approaches, the networks and frameworks used, the performance attained, and the evaluation with other prevalent explanations of each research are examined. We also analyzed the potential of deep learning to be used as a cutting-edge data mining tool in food sensory and consume explores. The outcome of our review specifies that deep learning outclasses other approaches such as physical feature extractors, orthodox machine learning algorithms, and deep learning as a capable tool in food quality and safety inspection. The cheering outcomes in classification and regression problems attained by deep learning will fascinate more research exertions to apply deep learning into the arena of food in the forthcoming. The main aim of this work is to facilitate our learning and implement that in real life. Food quality and food security are always issues which are always overlooked. In modern times, this has morphed into more significant concerns relating to optimization of on- demand supply chains and profitability of agri-businesses. But now with the advanced systems and technology, it is possible to resolve this issue efficiently using the power of AI.\n",
            "----\n",
            "Paper 338:\n",
            "Title: Artificial intelligence in radiation oncology\n",
            "Abstract: Artificial intelligence (AI) is a computer science that tries to mimic human-like intelligence in machines that use computer software and algorithms to perform specific tasks without direct human input. Machine learning (ML) is a subunit of AI that uses data-driven algorithms that learn to imitate human behavior based on a previous example or experience. Deep learning is an ML technique that uses deep neural networks to create a model. The growth and sharing of data, increasing computing power, and developments in AI have initiated a transformation in healthcare. Advances in radiation oncology have produced a significant amount of data that must be integrated with computed tomography imaging, dosimetry, and imaging performed before each fraction. Of the many algorithms used in radiation oncology, has advantages and limitations with different computational power requirements. The aim of this review is to summarize the radiotherapy (RT) process in workflow order by identifying specific areas in which quality and efficiency can be improved by ML. The RT stage is divided into seven stages: patient evaluation, simulation, contouring, planning, quality control, treatment application, and patient follow-up. A systematic evaluation of the applicability, limitations, and advantages of AI algorithms has been done for each stage.\n",
            "----\n",
            "Paper 339:\n",
            "Title: Data Science Education Based on ADDIE Model and the EDISON Framework\n",
            "Abstract: Data Science is a recent area that integrates data engineering and computing intelligence to support decision making. It is also a recent professional field which requires experts with knowledge in Statistics, Computer Science, Artificial Intelligence and Machine Learning, and specific Business Models. Organizations that pursue data intelligence consider Data Science professionals immensely relevant. However, universities and higher education institutions are starting to offer undergraduate programs and courses for Data Science education. In this sense, EDISON Data Science Framework (EDSF) is presented as a relevant framework to further the education of Data Science Professionals. Thus, this article addresses the integration of EDSF with ADDIE Model, an instructional design model, capable of enabling the construction of an Instructional System (IS). Based on the Learning Outcomes (LO) of the EDSF Model Curriculum Data Science (MC-DS), the article proposes an integrated approach of these LOs along with the phases of ADDIE Model supporting the construction of a Data Science Instructional System (DSIS). This integrated approach assists the understanding of the development of a DSIS and its learning objects, with its technological needs, such as: hardware environment, programming environments (IDEs, scripts, etc.), database models (SQL and NoSQL), massive data storage and processing tools, visualization tools, among others.\n",
            "----\n",
            "Paper 340:\n",
            "Title: Medical Information Mart for Intensive Care: A Foundation for the Fusion of Artificial Intelligence and Real-World Data\n",
            "Abstract: The Medical Information Mart for Intensive Care (MIMIC) is a database of de-identified electronic health records (EHR) associated with patients who stayed in intensive care units (ICU) at the Beth Israel Deaconess Medical Center in Boston, MA (Johnson et al., 2016). Currently on its fourth iteration, the database is maintained by the Massachusetts Institute of Technology’s Laboratory for Computational Physiology. With time frames and release dates shown in Table 1, all four MIMIC versions are accessible and available to the public, supporting the concept of reproducibility within ICU research (Johnson et al., 2018). Among these, MIMIC-III alone contains 53,423 distinct adult hospital admissions to one of the five different ICU departments at the Beth Israel DeaconessMedical Center between 2001 and 2012 (Johnson et al., 2016). These specialized ICU departments include Coronary Care, Cardiac Surgery Recovery, Medical Intensive Care, Surgical Intensive Care, and Trauma Surgical Intensive Care units. MIMIC is populated with data from hospital electronic health records, automated critical care information systems, and the Social Security Administration Death Master File. The two critical care information systems which provided time-stamped physiological measurements and progress notes were the Philips CareVue Clinical Information System and iMDsoft MetaVision ICU. Classes ofMIMIC data include patient demographics, billing information, unstructured text (notes), prescription medications, vital signs, laboratory results, and a plethora of physiological measurements. Due to the granularity of information, MIMIC serves as a real-world data (RWD) foundation for artificial intelligence (AI) and machine learning (ML) research applications. MIMIC places thousands of ICU records with millions of physiological measurements directly into scientists’ hands, which serves as a springboard for numerous projects in the development of AI/ML algorithms to support the work of ICU clinicians and staff. Here, we highlight a few of MIMIC’s contributions within the world of AI and ML, referring to this fusion as Artificial Intelligence for Real-World Data (AI4RWD). For the sake of brevity, a limited number of these efforts are mentioned in this short overview.\n",
            "----\n",
            "Paper 341:\n",
            "Title: Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 342:\n",
            "Title: A Machine Learning Approach to Detection of Critical Alerts from Imbalanced Multi-Appliance Threat Alert Logs\n",
            "Abstract: The extraordinary number of alerts generated by network intrusion detection systems (NIDS) can desensitize security analysts tasked with incident response. Security information and event management systems (SIEMs) perform some rudimentary automation but cannot replicate the decision-making process of a skilled analyst. Machine learning and artificial intelligence (AI) can detect patterns in data with appropriate training. In practice, the majority of the alert data comprises false alerts, and true alerts form only a small proportion. Consequently, a naive engine that classifies all security alerts into the majority class can yield a superficial high accuracy close to 100%. Without any correction for the class imbalance, the false alerts will dominate algorithmic predictions resulting in poor generalization performance. We propose a machine-learning approach to address the class imbalance problem in multi-appliance security alert data and automate the security alert analysis process performed in security operations centers (SOCs). We first used the neighborhood cleaning rule (NCR) to identify and remove ambiguous, noisy, and redundant false alerts. Then, we applied the support vector machine synthetic minority oversampling technique (SVMSMOTE) to generate synthetic training true alerts. Finally, we fit and evaluated the decision tree and random forest classifiers. In the experiments, using alert data from eight security appliances, we demonstrated that the proposed method can significantly reduce the need for manual auditing, decreasing the number of uninspected alerts and achieving a performance of 99.524% in recall.\n",
            "----\n",
            "Paper 343:\n",
            "Title: Cognitive computing and eScience in health and life science research: artificial intelligence and obesity intervention programs\n",
            "Abstract: None\n",
            "----\n",
            "Paper 344:\n",
            "Title: Theory and application of artificial intelligence in financial industry\n",
            "Abstract: : Artificial Intelligence (AI) is deemed to be the commanding point of science and technology in the next era. In recent years, with the enhancement of computer computing power, the improvement of the quantity and quality of big data, and the important breakthroughs in many research fields such as machine learning and speech recognition, AI technology has developed rapidly and has been widely used in all walks of life. In the financial industry, the application of AI technology in risk control, marketing, customer service, transaction, operation, and product optimization of financial institutions is becoming increasingly mature, and some new business models have been created. Starting from the application status and significance of AI in the international financial field, this paper expounds on the application, status quo, and development trend of AI in the financial industry. Then, in view of the risks and practical challenges existing in the development process of AI, based on the reality of international financial development, this paper summarizes the measures to promote the in-depth, healthy, and sustainable development of AI in the financial market. This paper aims to let readers understand the development status of AI in the financial field, and also provide theoretical reference for scholars in this field.\n",
            "----\n",
            "Paper 345:\n",
            "Title: Semantic relational machine learning model for sentiment analysis using cascade feature selection and heterogeneous classifier ensemble\n",
            "Abstract: The exponential rise in social media via microblogging sites like Twitter has sparked curiosity in sentiment analysis that exploits user feedback towards a targeted product or service. Considering its significance in business intelligence and decision-making, numerous efforts have been made in this area. However, lack of dictionaries, unannotated data, large-scale unstructured data, and low accuracies have plagued these approaches. Also, sentiment classification through classifier ensemble has been underexplored in literature. In this article, we propose a Semantic Relational Machine Learning (SRML) model that automatically classifies the sentiment of tweets by using classifier ensemble and optimal features. The model employs the Cascaded Feature Selection (CFS) strategy, a novel statistical assessment approach based on Wilcoxon rank sum test, univariate logistic regression assisted significant predictor test and cross-correlation test. It further uses the efficacy of word2vec-based continuous bag-of-words and n-gram feature extraction in conjunction with SentiWordNet for finding optimal features for classification. We experiment on six public Twitter sentiment datasets, the STS-Gold dataset, the Obama-McCain Debate (OMD) dataset, the healthcare reform (HCR) dataset and the SemEval2017 Task 4A, 4B and 4C on a heterogeneous classifier ensemble comprising fourteen individual classifiers from different paradigms. Results from the experimental study indicate that CFS supports in attaining a higher classification accuracy with up to 50% lesser features compared to count vectorizer approach. In Intra-model performance assessment, the Artificial Neural Network-Gradient Descent (ANN-GD) classifier performs comparatively better than other individual classifiers, but the Best Trained Ensemble (BTE) strategy outperforms on all metrics. In inter-model performance assessment with existing state-of-the-art systems, the proposed model achieved higher accuracy and outperforms more accomplished models employing quantum-inspired sentiment representation (QSR), transformer-based methods like BERT, BERTweet, RoBERTa and ensemble techniques. The research thus provides critical insights into implementing similar strategy into building more generic and robust expert system for sentiment analysis that can be leveraged across industries.\n",
            "----\n",
            "Paper 346:\n",
            "Title: Editorial: Machine Learning for Water Resources\n",
            "Abstract: The last years have seen a dramatic increase in the amount of data available to model Earth and environmental systems, thanks to new sensing technologies and open data policies. At the same time, innovative machine learning approaches are being developed, that are ideal tools to extract information from this large amount of data. This conjunction of more data and improved algorithms has a strong impact on research carried out in hydrology and hydrogeology, where non-linear processes are ubiquitous. This is reflected in the papers contained in this Research Topic on Machine Learning for Water Resources. These papers spread a wide range of domains, reflecting the richness in application domains, the wealth of data available, and the diversity of machine learning approaches. The papers in this Research Topic show the great interest and potential of future developments for artificial intelligence in hydrology. The result is that the contributions are very varied, and we will not attempt to summarize them all here; instead we encourage readers to delve into these papers themselves.\n",
            "----\n",
            "Paper 347:\n",
            "Title: Improving Machine Learning Performance Using Conceptual Modeling\n",
            "Abstract: Advances in machine learning (ML) make it possible to extract useful information from large and diverse datasets. ML methods aim to identify patterns in a dataset based on the values of features and their combinations. Recent research has proposed combining conceptual modeling, specifically data models, with artificial intelligence. In this paper, we employ conceptual modeling principles to develop a method for data preparation, which is comprised of six guidelines. We illustrate the method by applying it to a business case from a foster care organization; namely, predicting the length of stay of a child in the foster care system. The results show how conceptual modeling can improve ML model performance by imbuing explicit domain knowledge, instead of relying solely on data-driven rules.\n",
            "----\n",
            "Paper 348:\n",
            "Title: Machine Learning Techniques to Predict Academic Performance of Health Sciences Students\n",
            "Abstract: Prediction of academic performance of health sciences students prior to being fully engaged in academic studies will identify those students who may need early intervention. Machine learning (ML), a branch of artificial intelligence, can be used to predict the academic performance of such students and the factors that continue to impact their academic performance. Objective: To use a best fit model in ML to predict the academic performance of health science students and rank the most important factors affecting their performance. Method: The academic records of 3468 students were extracted from the student information system (SIS), which included preparatory year great point average (GPA), high school GPA, Achievement Test (AT), General Aptitude Test (GAT), and cumulative GPA upon graduation. Multiple machine learning algorithms were used to develop the best fit model to predict students' performance GPA and identify factors that contributed to GP A. Results: The best performing classifier based on area under the curve (AUC) is random forest (.773) followed by naïve bayes (.758), Support Vector Machine (.686), k-nearest neighbors (.684) and decision tree (.658), the three scoring methods showed preparatory year GPA, gender, and high school GPA were the top variables predicating student cumulative GPAs. Conclusion: Random forest model can assist college administrators and faculty in health colleges to predict which students are more likely to underperform during their undergraduate studies.\n",
            "----\n",
            "Paper 349:\n",
            "Title: Machine Learning and Deep Learning Technologies\n",
            "Abstract: In the information era, enormous amounts of data have become available on hand to decision makers. Big data refers to datasets that are not only big, but also high in variety and velocity, which makes them difficult to handle using traditional tools and techniques. Due to the rapid growth of such data, solutions need to be studied and provided in order to handle and extract value and knowledge from these datasets. Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Such minimal human intervention can be provided using machine learning, which is the application of advanced deep learning techniques on big data. This paper aims to analyse some of the different machine learning and deep learning algorithms and methods, aswell as the opportunities provided by the AI applications in various decision making domains.\n",
            "----\n",
            "Paper 350:\n",
            "Title: Artificial Intelligence Accelerates Human Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 351:\n",
            "Title: Causal Inference Meets Machine Learning\n",
            "Abstract: Causal inference has numerous real-world applications in many domains such as health care, marketing, political science and online advertising. Treatment effect estimation, a fundamental problem in causal inference, has been extensively studied in statistics for decades. However, traditional treatment effect estimation methods may not well handle large-scale and high-dimensional heterogeneous data. In recent years, an emerging research direction has attracted increasing attention in the broad artificial intelligence field, which combines the advantages of traditional treatment effect estimation approaches (e.g., matching estimators) and advanced representation learning approaches (e.g., deep neural networks). In this tutorial, we will introduce both traditional and state-of-the-art representation learning algorithms for treatment effect estimation. Background about causal inference, counterfactuals and matching estimators will be covered as well. We will also showcase promising applications of these methods in different application domains.\n",
            "----\n",
            "Paper 352:\n",
            "Title: Visualization Analysis of Artificial Intelligence Technology in Higher Education Based on SSCI and SCI Journals from 2009 to 2019\n",
            "Abstract: The application of artificial intelligence technology in higher education has study value and study space. The study mainly used core data collection of the Web of Science database based on SSCI and SCI journals and based on “artificial intelligence” and “higher education” these two subjects retrieval for 2009 to 2019. In addition, It used keyword co-occurrence function and cluster analysis function of CiteSpace Knowledge Graph software for visualization analysis and detailed analysis of study hotspots. The study elaborated on the application of artificial intelligence technology in higher education, and gained study hotspots such as machine learning, augmented reality, and learning management systems, and briefly discussed the impact of artificial intelligence on higher education.\n",
            "----\n",
            "Paper 353:\n",
            "Title: Disease Prediction by Machine Learning from Healthcare Communities\n",
            "Abstract: To promote sustainable improvement, the smart town implies a global imaginative and prescient that merges artificial intelligence, choice making, statistics and conversation era (ICT), and the net-of-things (IoT). in this mission, the subject of disease prediction and prognosis in clever healthcare is reviewed. due to records progress in biomedical and healthcare groups, correct have a look at of clinical data advantages early disorder recognition, patient care and network services. whilst the exceptional of medical information is incomplete the exactness of study is reduced. moreover, exclusive areas exhibit specific appearances of certain regional illnesses, which can also bring about weakening the prediction of sickness outbreaks. within the proposed system, it offers gadget gaining knowledge of algorithms for effective prediction of various disorder occurrences in ailment-frequent societies and predicts the waiting time for each treatment project for every patient as well as a hospital Queuing advice (HQR) system is advanced for recommending treatment mission sequence with appreciate to anticipated ready time. It experiments on a nearby chronic illness of cerebral infarction. using structured and unstructured facts from health centre it makes use of system studying selection Tree algorithm and KNN algorithm. To the first-rate of our knowledge inside the place of medical huge records analytics none of the existing paintings focused on each information types. in comparison to several normal estimate algorithms, the calculation exactness of our proposed set of rules reaches 94.8% with a convergence speed which is faster than that of the CNN-based totally uni-modal ailment threat prediction (CNN-UDRP) algorithm. similarly, challenges within the deployment of sickness diagnosis in healthcare had been mentioned.\n",
            "----\n",
            "Paper 354:\n",
            "Title: Machine Learning in Computer Vision: A Review\n",
            "Abstract: INTRODUCTION: Due to the advancement in the field of Artificial Intelligence (AI), the ability to tackle entire problems of machine intelligence. Nowadays, Machine learning (ML) is becoming a hot topic due to the direct training of machines with less interaction with a human. The scenario of manual feeding of the machine is changed in the modern era, it will learn automatically. Supervised and unsupervised ML techniques are used as a distinct purpose like feature extraction, pattern recognition, object detection, and classification. OBJECTIVES: In Computer Vision (CV), ML performs a significant role to extract crucial information from images. CV successfully contributes to multiple domains, surveillance system, optical character recognition, robotics, suspect detection, and many more. The direction of CV research is going toward healthcare realm, medical imaging (MI) is the emerging technology, play a vital role to enhance image quality and recognized critical features of binary medical image, covert original image into grayscale and set the threshold values for segmentation. CONTRIBUTION: This paper will address the importance of machine learning, state-of-the-art, and how ML is utilized in computer vision and image processing. This survey will provide details about the type of tools and applications, datasets, and techniques. Limitations of previous work and challenges of future work also discussed. Further, we identify and discuss a set of open issues yet to be addressed, for efficiently applying of ML in Computer vision and image process. METHODS, RESULTS, AND CONCLUSION: In this review paper, we have discussed the techniques and various types of supervised and unsupervised algorithms of ML, general overview of image processing and the results based on the impact; neural network enabled models, limitations, tools and application of CV, moreover, highlight the critical open research areas of ML in CV.\n",
            "----\n",
            "Paper 355:\n",
            "Title: Smart Agri Wine: An Artificial Intelligence Approach to Predict Wine Quality\n",
            "Abstract: : Lately the item quality has been one of the critical parts of each and every industry. The customary strategies for surveying the item quality are exceptionally tedious and furthermore not having the ideal outcome with the resultant in unique Technology development. Through the ideas of Artificial Intelligence (AI) and Information Science (IS) it is more productive to evaluate or to foresee any sort of thing effectively. In this study, the investigation of wine information is done on University of California Irvine (UCI) Machine Learning (ML) dataset. The fundamental motivation behind this examination is to foresee wine quality dependent on physicochemical information using AI.\n",
            "----\n",
            "Paper 356:\n",
            "Title: Review on the Application of Metalearning in Artificial Intelligence\n",
            "Abstract: In recent years, artificial intelligence supported by big data has gradually become more dependent on deep reinforcement learning. However, the application of deep reinforcement learning in artificial intelligence is limited by prior knowledge and model selection, which further affects the efficiency and accuracy of prediction, and also fails to realize the learning ability of autonomous learning and prediction. Metalearning came into being because of this. Through learning the information metaknowledge, the ability to autonomously judge and select the appropriate model can be formed, and the parameters can be adjusted independently to achieve further optimization. It is a novel method to solve big data problems in the current neural network model, and it adapts to the development trend of artificial intelligence. This article first briefly introduces the research process and basic theory of metalearning and discusses the differences between metalearning and machine learning and the research direction of metalearning in big data. Then, four typical applications of metalearning in the field of artificial intelligence are summarized: few-shot learning, robot learning, unsupervised learning, and intelligent medicine. Then, the challenges and solutions of metalearning are analyzed. Finally, a systematic summary of the full text is made, and the future development prospect of this field is assessed.\n",
            "----\n",
            "Paper 357:\n",
            "Title: Artificial Intelligence and Robots for the Library and Information Professions\n",
            "Abstract: We are very excited to present this year’s special issue of the Journal of the Australian Library and Information Association (JALIA) on Artificial Intelligence (AI) and Robots for the Library and Information Professions. This issue brings together five research papers and a research-in-practice paper on various aspects of AI and robots with specific implications for their use in Library and Information Science. The last few years have seen a wave of excitement and hype around AI and robots and their potential to create, according to some, a Fourth Industrial Revolution. This seems to have touched every sector of work from agriculture and retail, through to scientific research. The need for AI in health applications has been accelerated by COVID 19. Yet there are also strong public concerns about the ethics and responsible design and use of such automation (Jobin et al., 2019). It is also important to recognise that the application of AI in consumer technology creates an industrial complex based on intensive (perhaps unsustainable) use of energy, raw materials, and often low paid labour (Crawford, 2021). Libraries and information services, like every other sector, are likely to be changed, even transformed, by AI and robots (Cox, 2021). In many ways they are already being reshaped by AI in search and recommendation, with their implications for data and AI literacy. As user contexts change through AI, LIS roles supporting them will have to alter too. For example, new uses in scientific research will impact scholarly communication and how it is supported (Jones et al., 2019). Some library and information services are already using AI in their operations, specifically machine learning technologies for tasks such as automated clustering and classification of resources, discovering and linking collections, and weeding collections (Cordell, 2020), the use of robots in book retrieval (McCaffrey, 2021), the use of chatbots and voice assistants for university library services (Mckie & Narayan, 2019; Hopkins & Maccabee 2018), and in learning and teaching in schools and public libraries (Nguyen, 2020). Libraries are using AI to take advantage of big data and data analytics both in their operational and serviceoriented aspects (Garoufallou & Gaitanou, 2021). Wider societal concerns remain about the ethics of AI and robots around their potential impact on user privacy, freedom of expression, the risk of bias, and loss of transparency in decision making (Cordell, 2020; Padilla, 2019). There are fundamental questions about the impacts on human agency, as well as the direct impacts on work and jobs, including those of information professionals. We should also take pause to reflect critically about how discourses of technological solutionism such as those commonly found around AI and robots impact LIS thinking (Mirza & Seale, 2017). Developed responsibly AI does have the potential to increase access to knowledge and so it is vital that the profession develop a measured response to the opportunities and challenges AI and robots offer, including building educational opportunities for LIS students to learn about them within their curriculum. The papers collected in this special issue contribute to this discourse.\n",
            "----\n",
            "Paper 358:\n",
            "Title: Artificial intelligence, a possible solution for agriculture and animal husbandry sector?\n",
            "Abstract: This paper aims at making a review of the artificial intelligence concept, its global scope from the agro-livestock sector perspective and the understanding, approach and application of this concept Romania in early 2021. Artificial intelligence is a computer science sub-field that is materialized by algorithms developed starting from the logical or mathematical models of the cognition, perception and action processes. Globally, large agricultural companies are trying to grasp concepts such as big data, artificial intelligence (AI), machine learning and analysis. These areas have moved rapidly towards the agro-livestock sector too, but most companies have not been prepared to deal thoroughly with these new technologies. It really sounds interesting, but what does it take to take the next steps? The voice of the expert says: “If we really want to have a global impact on food sustainability, production and safety, we need to think about data standards, data sharing, benchmarking and analysis on aggregated data sets. Today, we see farmers who are reluctant to share data with agritech companies that have developed closed systems, which will hinder the evolution of things” (Claudia Roessler, IT specialist, Microsoft).\n",
            "----\n",
            "Paper 359:\n",
            "Title: Artificial Intelligence in Surgery: Promises and Perils\n",
            "Abstract: Objective: The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments. Summary Background Data: AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers. Methods: A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed. Results: Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed. Conclusions: Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.\n",
            "----\n",
            "Paper 360:\n",
            "Title: Application of Machine Learning to Analyse Biomedical Signals for Medical Diagnosis\n",
            "Abstract: Interest in research involving health-medical information analysis based on artificial intelligence has recently been increasing. Most of the research in this field has been focused on searching for new knowledge for predicting and diagnosing disease by revealing the relation between disease and various information features of data. However, still needed are more research and interest in applying the latest advanced artificial intelligence-based data analysis techniques to bio-signal data, which are continuous physiological records, such as EEG (electroencephalography) and ECG (electrocardiogram). This study presents a survey of ECG classification into arrhythmia types. Early and accurate detection of arrhythmia types is important in detecting heart diseases and choosing appropriate treatment for a patient.\n",
            "----\n",
            "Paper 361:\n",
            "Title: Artificial intelligence-based approaches for improving the diagnosis, triage, and prioritization of autism spectrum disorder: a systematic review of current trends and open issues\n",
            "Abstract: None\n",
            "----\n",
            "Paper 362:\n",
            "Title: Machine Learning and Deep Learning Approaches for Brain Disease Diagnosis: Principles and Recent Advances\n",
            "Abstract: Brain is the controlling center of our body. With the advent of time, newer and newer brain diseases are being discovered. Thus, because of the variability of brain diseases, existing diagnosis or detection systems are becoming challenging and are still an open problem for research. Detection of brain diseases at an early stage can make a huge difference in attempting to cure them. In recent years, the use of artificial intelligence (AI) is surging through all spheres of science, and no doubt, it is revolutionizing the field of neurology. Application of AI in medical science has made brain disease prediction and detection more accurate and precise. In this study, we present a review on recent machine learning and deep learning approaches in detecting four brain diseases such as Alzheimer’s disease (AD), brain tumor, epilepsy, and Parkinson’s disease. 147 recent articles on four brain diseases are reviewed considering diverse machine learning and deep learning approaches, modalities, datasets etc. Twenty-two datasets are discussed which are used most frequently in the reviewed articles as a primary source of brain disease data. Moreover, a brief overview of different feature extraction techniques that are used in diagnosing brain diseases is provided. Finally, key findings from the reviewed articles are summarized and a number of major issues related to machine learning/deep learning-based brain disease diagnostic approaches are discussed. Through this study, we aim at finding the most accurate technique for detecting different brain diseases which can be employed for future betterment.\n",
            "----\n",
            "Paper 363:\n",
            "Title: From hype to reality: data science enabling personalized medicine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 364:\n",
            "Title: Computational design of energy‐related materials: From first‐principles calculations to machine learning\n",
            "Abstract: Energy‐related materials are crucial for advancing energy technologies, improving efficiency, reducing environmental impacts, and supporting sustainable development. Designing and discovering these materials through computational techniques necessitates a comprehensive understanding of the material space, which is defined by the constituent atoms, composition, and structure. Depending on the search space involved in the investigation, the computational materials design can be categorized into four primary approaches: atomic substitution in fixed prototype structures, crystal structure prediction (CSP), variable‐composition CSP, and inverse design across the entire materials space. This review provides an overview of these paradigms, detailing the concepts, strategies, and applications pertinent to energy‐related materials. The progression from first‐principles calculations to machine learning techniques is emphasized, with the aim of enhancing understanding and elucidating new advancements in computationally design of energy‐related materials.This article is categorized under:\n",
            "Structure and Mechanism > Computational Materials Science\n",
            "Data Science > Artificial Intelligence/Machine Learning\n",
            "Electronic Structure Theory > Density Functional Theory\n",
            "\n",
            "----\n",
            "Paper 365:\n",
            "Title: Applications, Risks and Countermeasures of Artificial Intelligence in Education\n",
            "Abstract: The application of artificial intelligence in education has greatly affected school management, teachers' teaching and students' learning, and has an important influence on the mechanism and mechanism of learning process. The concept of artificial intelligence is to simulate human intelligence by machine, thus completing specific roles and tasks, and has the characteristics of convenience, intelligence and interaction. It is mainly applied to computer vision, natural language processing, biometric recognition, speech recognition, human-computer interaction and other technologies in the field of education, which brings space for the development of architecture, medicine and chemistry. However, AI also brings about the risk of decisionmaking mistakes, career substitution, privacy leakage, information cocoon house and data bias. In order to avoid these risks effectively, this paper puts forward solutions from five perspectives: the state, product developers, educational managers, teachers and students, so as to correctly understand the relationship between artificial intelligence and education, explore the application mode of artificial intelligence in the field of education, and ensure the deep integration of artificial intelligence and education.\n",
            "----\n",
            "Paper 366:\n",
            "Title: Understanding quality of analytics trade-offs in an end-to-end machine learning-based classification system for building information modeling\n",
            "Abstract: None\n",
            "----\n",
            "Paper 367:\n",
            "Title: Artificial intelligence in medical imaging: threat or opportunity? Radiologists again at the forefront of innovation in medicine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 368:\n",
            "Title: Data Science and AI: Trends Analysis\n",
            "Abstract: This study has the primary goal to analyze the growth of data science through the main search trends. This study was conducted by defining in high level the concept of data science as well as its main components. Supported in those elements, we identified the main trends. We used mainly data from google trends to determine the evolution of search by topics., research area, or simple expressions. It allowed us to reckon that artificial intelligence (AI)suffered a lack of interest until 2012. Then it became an increasingly popular field since 2014. This is due to the progression of machine learning and data science. Results show a cumulative search of data science since 2012.\n",
            "----\n",
            "Paper 369:\n",
            "Title: A Literature Review on Machine Learning Based Medical Information Retrieval Systems\n",
            "Abstract: As many fields progress with the assistance of cognitive computing, the field of health care is also adapting, providing many benefits to all users. However, advancements in this area are hindered by several challenges such as the void between user queries and the knowledge base, query mismatches, and range of domain knowledge in users. In this paper, we present existing methodologies as well as look into existing real-life applications that are used in the medical field today. We also look into specific challenges and techniques that can be used to overcome these barriers, specifically related to cognitive computing in the medical domain. Future information retrieval (IR) models that can be tailored specifically for medically intensive applications which can handle large amounts of data are explored as well. The purpose of this paper is to give the reader an in-depth understanding of artificial intelligence being used in the medical field today, as well as future possibilities in the domain.\n",
            "----\n",
            "Paper 370:\n",
            "Title: Predicting autism spectrum disorder from associative genetic markers of phenotypic groups using machine learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 371:\n",
            "Title: Emerging trends in geospatial artificial intelligence (geoAI): potential applications for environmental epidemiology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 372:\n",
            "Title: Predicting groundwater depth fluctuations using deep learning, extreme learning machine and Gaussian process: a comparative study\n",
            "Abstract: None\n",
            "----\n",
            "Paper 373:\n",
            "Title: Application of Machine Learning Techniques in Healthcare\n",
            "Abstract: Machine learning is an approach of artificial intelligence (AI) where the machine can automatically learn and improve its performance on experience. It is not explicitly programmed; the data is fed into the generic algorithm and it builds logic based on the data provided. Traditional algorithms have to define new rules or massive rules when the pattern varies or the number of patterns increases, which reduces the accuracy or efficiency of the algorithms. But the machine learning algorithms learn new input patterns capable of handling complex situations while maintaining accuracy and efficiency. Due to its effectual benefits, machine learning algorithms are used in various domains like healthcare, industries, travel, game development, social media services, robotics, and surveillance and information security. In this chapter, the application of machine learning technique in healthcare is discussed in detail.\n",
            "----\n",
            "Paper 374:\n",
            "Title: Multi-agent system for anomaly detection in Industry 4.0 using Machine Learning techniques\n",
            "Abstract: Industry 4.0 is the new industrial stage that is committed to greater automation, connectivity and globalization. The interrelation between the different areas has penetrated the industrial world thanks to the Internet of things and the world of Big Data. This amount of information available in plants is growing increasingly, also aided by the network computing services offered by cloud computing or edge computing. That is why it’s necessary to carry out complex fusion methods and data analysis using Machine Learning techniques to address specific industrial requirements and needs. The central challenge of industry 4.0 from the perspective of data science is to predict the history within the monitored processes, providing as much information as possible, avoiding them and stave off severe economic losses. This article will show a review of the application of Artificial Intelligence (AI) techniques such as Machine Learning (ML) immersed in multi-agent systems (MAS) in Industry 4.0. For this, a bibliographic search has been carried out in databases recognized as Science Direct, Google Scholar, Scopus or Springer, filtering the investigations from 2018 to the actually. The article concludes by pointing out the possible future lines and the importance of the transition towards the implementation of new technologies for the competitiveness of factories.\n",
            "----\n",
            "Paper 375:\n",
            "Title: Machine learning and medical education\n",
            "Abstract: None\n",
            "----\n",
            "Paper 376:\n",
            "Title: Personalized Explanation for Machine Learning: a Conceptualization\n",
            "Abstract: Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.\n",
            "----\n",
            "Paper 377:\n",
            "Title: Adaptive Web Sites: A Knowledge Extraction from Web Data Approach - Volume 170 Frontiers in Artificial Intelligence and Applications\n",
            "Abstract: This book can be presented in two different ways; introducing a particular methodology to build adaptive Web sites and; presenting the main concepts behind Web mining and then applying them to adaptive Web sites. In this case, adaptive Web sites is the case study to exemplify the tools introduced in the text. The authors start by introducing the Web and motivating the need for adaptive Web sites. The second chapter introduces the main concepts behind a Web site: its operation, its associated data and structure, user sessions, etc. Chapter three explains the Web mining process and the tools to analyze Web data, mainly focused in machine learning. The fourth chapter looks at how to store and manage data. Chapter five looks at the three main and different mining tasks: content, links and usage. The following chapter covers Web personalization, a crucial topic if we want to adapt our site to specific groups of people. Chapter seven shows how to use information extraction techniques to find user behavior patterns. The subsequent chapter explains how to acquire and maintain knowledge extracted from the previous phase. Finally, chapter nine contains the case study where all the previous concepts are applied to present a framework to build adaptive Web sites. In other words, the authors have taken care of writing a self-contained book for people that want to learn and apply personalization and adaptation in Web sites. This is commendable considering the large and increasing bibliography in these and related topics. The writing is easy to follow and although the coverage is not exhaustive, the main concepts and topics are all covered.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences\n",
            "----\n",
            "Paper 378:\n",
            "Title: Privacy and Data Protection in ChatGPT and Other AI Chatbots: Strategies for Securing User Information\n",
            "Abstract: The evolution of artificial intelligence (AI) and machine learning (ML) has led to the development of sophisticated large language models (LLMs) that are used extensively in applications such as chatbots. This research investigates the critical issues of data protection and privacy enhancement in the context of LLM-based chatbots, with a focus on OpenAI's ChatGPT. It explores the dual challenges of safeguarding sensitive user information while ensuring the efficiency of machine learning models. It assesses existing privacy-enhancing technologies (PETs) and proposes innovative methods, such as differential privacy, federated learning, and data minimization techniques. The study also includes a survey of Chatbot users to measure their concerns related to data privacy with the use of these LLM-based applications. This study is meant to serve as a comprehensive guide for developers, policymakers, and researchers, contributing to the discourse on data protection in artificial intelligence.\n",
            "----\n",
            "Paper 379:\n",
            "Title: Seven Pitfalls of Using Data Science in Cybersecurity\n",
            "Abstract: None\n",
            "----\n",
            "Paper 380:\n",
            "Title: Clinical applications of machine learning in cardiovascular disease and its relevance to cardiac imaging.\n",
            "Abstract: Artificial intelligence (AI) has transformed key aspects of human life. Machine learning (ML), which is a subset of AI wherein machines autonomously acquire information by extracting patterns from large databases, has been increasingly used within the medical community, and specifically within the domain of cardiovascular diseases. In this review, we present a brief overview of ML methodologies that are used for the construction of inferential and predictive data-driven models. We highlight several domains of ML application such as echocardiography, electrocardiography, and recently developed non-invasive imaging modalities such as coronary artery calcium scoring and coronary computed tomography angiography. We conclude by reviewing the limitations associated with contemporary application of ML algorithms within the cardiovascular disease field.\n",
            "----\n",
            "Paper 381:\n",
            "Title: Curriculum for Hands-on Artificial Intelligence Cybersecurity\n",
            "Abstract: Interest and awareness of Artificial Intelligence (AI) grows at such a rate that academia and higher education struggle keeping up with the accelerating demand of industry. It is forecast that 75% of enterprise applications will use AI, Machine Learning or Deep Learning technology by 2021, yet university programs commonly place the burden on students to obtain their data science educations through elective coursework spanning multiple disparate departments. Data science requires specific mathematics preparation especially for cybersecurity students whose programs have reduced the requirements for advance mathematics to a bare minimum. To facilitate curricula preparation, and hands-on usage of AI tools, a notebook (“A Trellis For Novice AI Practitioners”) was prepared in the R programming language as a first step in introducing computer science and cybersecurity students to the concepts and capabilities of AI. A focus on an intrusion detection data set to mitigate nine common cyber vulnerabilities is used in this analysis. Trellis bridges the theoretical and practical chasm for students by building an ANN network intrusion predictive model from scratch. It serves as a template but also encourages heavy contextual modification, and may be relied upon in the beginning stages of a cybersecurity practitioner's data science activities on a wide variety of data sets in all areas of the discipline.\n",
            "----\n",
            "Paper 382:\n",
            "Title: A Guided Tour of Artificial Intelligence Research: Volume I: Knowledge Representation, Reasoning and Learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 383:\n",
            "Title: A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI\n",
            "Abstract: Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.\n",
            "----\n",
            "Paper 384:\n",
            "Title: Artificial Intelligence in Disaster Risk Communication: A Systematic Literature Review\n",
            "Abstract: Effective communication of disaster risks is crucial to provoking appropriate responses from citizens and emergency operators. With recent advancement in Artificial Intelligence (AI), several researchers have begun exploring machine learning techniques in improving disaster risk communication. This paper adopts a systematic literature approach to report on the various research activities involving the application of AI in disaster risk communication. The study found that research activities focus on two broad areas: (1) prediction and monitoring for early warning, and (2) information extraction and classification for situational awareness. These broad areas are discussed, including background information to help establish future applications of AI in disaster risk communication. The paper concludes with recommendations of several ways in which AI applications can have a broader role in disaster risk communication.\n",
            "----\n",
            "Paper 385:\n",
            "Title: A review on the self and dual interactions between machine learning and optimisation\n",
            "Abstract: None\n",
            "----\n",
            "Paper 386:\n",
            "Title: Artificial intelligence in drug design\n",
            "Abstract: None\n",
            "----\n",
            "Paper 387:\n",
            "Title: Comparison of medical image classification accuracy among three machine learning methods.\n",
            "Abstract: BACKGROUND\n",
            "Low-quality medical images may influence the accuracy of the machine learning process.\n",
            "\n",
            "\n",
            "OBJECTIVE\n",
            "This study was undertaken to compare accuracy of medical image classification among machine learning methods, as classification is a basic aspect of clinical image inspection.\n",
            "\n",
            "\n",
            "METHODS\n",
            "Three types of machine learning methods were used, which include Support Vector Machine (SVM), Artificial Neural Network (ANN), and Convolution Neural Network (CNN). To investigate changes in accuracy related to image quality, we constructed a single dataset using two different file formats of DICOM (Digital Imaging and Communications in Medicine) and JPEG (Joint Photographic Experts Group).\n",
            "\n",
            "\n",
            "RESULTS\n",
            "The JPEG format contains less color information and data capacity than the DICOM format. CNN classification was accurate for both datasets, whereas SVM and ANN accuracy decreased with the loss of data from DICOM to JPEG formats.\n",
            "\n",
            "\n",
            "CONCLUSIONS\n",
            "CNN is more accurate than conventional machine learning methods that utilize the manual feature extraction.\n",
            "----\n",
            "Paper 388:\n",
            "Title: Artificial Intelligence, Big Data, and mHealth: The Frontiers of the Prevention of Violence Against Children\n",
            "Abstract: Violence against children is a global public health threat of considerable concern. At least half of all children worldwide experience violence every year; globally, the total number of children between the ages of 2 and 17 years who have experienced violence in any given year is one billion. Based on a review of the literature, we argue that there is substantial potential for AI (and associated machine learning and big data), and mHealth approaches to be utilized to prevent and address violence at a large scale. This potential is particularly marked in low- and middle-income countries (LMIC), although whether it could translate into effective solutions at scale remains unclear. We discuss possible entry points for Artificial Intelligence (AI), big data, and mHealth approaches to violence prevention, linking these to the World Health Organization's seven INSPIRE strategies. However, such work should be approached with caution. We highlight clear directions for future work in technology-based and technology-enabled violence prevention. We argue that there is a need for good agent-based models at the level of entire cities where and when violence can occur, where local response systems are. Yet, there is a need to develop common, reliable, and valid population- and individual/family-level data on predictors of violence. These indicators could be integrated into routine health or other information systems and become the basis of Al algorithms for violence prevention and response systems. Further, data on individual help-seeking behavior, risk factors for child maltreatment, and other information which could help us to identify the parameters required to understand what happens to cause, and in response to violence, are needed. To respond to ethical issues engendered by these kinds of interventions, there must be concerted, meaningful efforts to develop participatory and user-led work in the AI space, to ensure that the privacy and profiling concerns outlined above are addressed explicitly going forward. Finally, we make the case that developing AI and other technological infrastructure will require substantial investment, particularly in LMIC.\n",
            "----\n",
            "Paper 389:\n",
            "Title: A novel machine learning approach for rice yield estimation\n",
            "Abstract: ABSTRACT Artificial Intelligence is quickly emerging as a technological solution for the agriculture industry to surmount its classical challenges. Artificial Intelligence is facilitating farmers to refine their products and alleviate unfavourable impacts due to the environment. The central concern of this paper is predictive analytics to develop a machine learning model to identify and predict crop yield based on multiple environmental factors. In this paper, a hybrid learner ‘RaNN’ is proposed that combines the feature sampling and majority voting technique of Random Forest in-combination with the multilayer Feedforward Neural Network to predict the crop yield. Research has also ascertained the essential features responsible for accurate yield prediction. The proposed model works for rice yield prediction, one of the chief grains of India. The region chosen for the work is Punjab, which is among the largest producer states of India for rice. The dataset consists of 15 attributes comprising the weather and agriculture data collected from the Indian Meteorological Department Pune, and Punjab Environment Information System (ENVIS) Center, Government of India. The study has also made a comparative assessment of ‘RaNN’ with machine learning methods like Multiple Linear Regression, Random Forest, Decision Tree, Boosting Regression, Support Vector Machine Regression, Ensemble Learner, and Artificial Neural Network. Our model RaNN has listed a better prediction accuracy with minimal error among the other techniques providing a 98% correlation between the actual and the predicted yield. Abbreviations: AI – Artificial Intelligence; ANN – Artificial Neural Network; BR – Boosting Regression; Chem Fert – chemical fertilisers; DT – Decision Tree; EL – Ensemble Learner; ENVIS – Punjab Environment Information System; GBM – Stochastic Gradient Boosting Method; GPS – Global Positioning System; HMAX – highest maximum temperature in degrees C; IMD – Indian Meteorological Department; L1 – Lasso regression; L2 – Ridge regression; LMIN – lowest minimum temperature; ML – Machine Learning; MAE – Mean Absolute Error; MEVP – mean evaporation in mm; MLR – Multiple Linear Regression; MMAX – mean maximum temperature in degrees C; MMIN – mean minimum temperature in degrees C; MSSH – Mean sunshine duration in hours; MWS – mean wind speed in km/h; P1 – number of days with precipitation (0.1–0.2 mm); P2 – number of days with precipitation (greater than or equal to 0.3 mm); RaNN – Hybrid RF-ANN model; RMSE – Root Mean Squared Error; $${R^2}$$R2 – Coefficient of determination; RD – number of rainy days; RF – Random Forest; SVM Reg – Support Vector Machine Regression; TMRF – total rainfall per month in mm\n",
            "----\n",
            "Paper 390:\n",
            "Title: Artificial Intelligence with the Internet of Things on Healthcare systems: A Survey\n",
            "Abstract: The technologies of Artificial intelligence (AI) and the Internet of Things (IoT) perfectly complement each other for future enlargement. IoT connects all the things in the world over the internet. The number of connected devices contains a large number of data, which means IoT devices have endless information in chips and sensors for empowering people in various aspects of their lives. Even human and computer software can’t handle and processes the considerable number of data which are generated by IoT devices. So, Artificial intelligence and machine learning algorithms help to control them. AI provides the functional solution to managing the various connected IoT elements. The critical issue is learning abilities and unlimited data processing that is generated by IoT devices. To rectify this issue, the companies are using the powerful subset of AI, called machine learning (ML). For applying ML to IoT data, the smart systems provide an accurate prediction. AI applications for IoT, enables the organizations to avoid unplanned downtime, spawn new services or products, increase operating efficiency, and enhance risk management. This combination is mainly used in the applications of healthcare, smart homes, autonomous vehicles, agriculture, and marketing. In these various applications, this survey paper describes healthcare applications based on Artificial intelligence (AI) and IoT. It also introduces the survey of different AI-based IoT techniques to predict the diseases earlier in the medical filed.\n",
            "----\n",
            "Paper 391:\n",
            "Title: Research on Artificial Intelligence Visualization Application under Internet of Things Big Data\n",
            "Abstract: Machine learning is an inevitable outcome of the development of artificial intelligence research to a certain stage. It has become the most popular technology in the fields of computer vision and natural language processing. Learning techniques such as inductive logic programming, neural network-based connectionist learning technology, and statistical learning theory are constantly evolving. The shortcomings of various learning techniques in data representation and result processing have become a concern of many scholars. As one of the cutting-edge science and technology in the 21st century, artificial intelligence has a profound impact on education. This research is based on the artificial intelligence in Visualization Application for nearly 12 years. It has carried out visual analysis of research hotspots and frontiers. In addition, this study also discusses the research content through cluster analysis, discusses the influence of artificial intelligence on Chinese education field, and reflects on it.\n",
            "----\n",
            "Paper 392:\n",
            "Title: An evaluation of machine learning techniques to predict the outcome of children treated for Hodgkin-Lymphoma on the AHOD0031 trial\n",
            "Abstract: ABSTRACT In this manuscript, we analyze a data set containing information on children with Hodgkin Lymphoma (HL) enrolled on a clinical trial. Treatments received and survival status were collected together with other covariates such as demographics and clinical measurements. Our main task is to explore the potential of machine learning (ML) algorithms in a survival analysis context in order to improve over the Cox Proportional Hazard (CoxPH) model. We discuss the weaknesses of the CoxPH model we would like to improve upon and then we introduce multiple algorithms, from well-established ones to state-of-the-art models, that solve these issues. We then compare every model according to the concordance index and the Brier score. Finally, we produce a series of recommendations, based on our experience, for practitioners that would like to benefit from the recent advances in artificial intelligence.\n",
            "----\n",
            "Paper 393:\n",
            "Title: Application and existing problems of computer network technology in the field of artificial intelligence\n",
            "Abstract: With the development of science and technology, computer technology is more and more widely used in people's life. As a branch of computer technology, artificial intelligence technology is also gradually developing and expanding, affecting people's life. Nowadays, the processing ability shown by simple computer technology has gradually lagged behind the times. If you want more convenient data processing, artificial intelligence is an excellent solution. Agent technology in artificial intelligence makes artificial intelligence have stronger data processing ability and learning ability than computer technology, and can make decisions quickly according to data information. Artificial intelligence technology can promote automation and intelligence in various industries and improve the production efficiency of enterprises. Starting from the elaboration of artificial intelligence, this paper deeply explores the relationship between artificial intelligence and computer, analyzes the development status of artificial intelligence and computer, and puts forward the technical problems between artificial intelligence and computer, so as to provide reference for the further development of artificial intelligence.\n",
            "----\n",
            "Paper 394:\n",
            "Title: Customer Churn Prediction in the Banking Sector Using Machine Learning-Based Classification Models\n",
            "Abstract: Aim/Purpose: Previous research has generally concentrated on identifying the variables that most significantly influence customer churn or has used customer segmentation to identify a subset of potential consumers, excluding its effects on forecast accuracy. Consequently, there are two primary research goals in this work. The initial goal was to examine the impact of customer segmentation on the accuracy of customer churn prediction in the banking sector using machine learning models. The second objective is to experiment, contrast, and assess which machine learning approaches are most effective in predicting customer churn.\n",
            "\n",
            "Background: This paper reviews the theoretical basis of customer churn, and customer segmentation, and suggests using supervised machine-learning techniques for customer attrition prediction.\n",
            "\n",
            "Methodology: In this study, we use different machine learning models such as k-means clustering to segment customers, k-nearest neighbors, logistic regression, decision tree, random forest, and support vector machine to apply to the dataset to predict customer churn.\n",
            "\n",
            "Contribution: The results demonstrate that the dataset performs well with the random forest model, with an accuracy of about 97%, and that, following customer segmentation, the mean accuracy of each model performed well, with logistic regression having the lowest accuracy (87.27%) and random forest having the best (97.25%).\n",
            "\n",
            "Findings: Customer segmentation does not have much impact on the precision of predictions. It is dependent on the dataset and the models we choose.\n",
            "\n",
            "Recommendations for Practitioners: The practitioners can apply the proposed solutions to build a predictive system or apply them in other fields such as education, tourism, marketing, and human resources.\n",
            "\n",
            "Recommendation for Researchers: The research paradigm is also applicable in other areas such as artificial intelligence, machine learning, and churn prediction. \n",
            "\n",
            "Impact on Society: Customer churn will cause the value flowing from customers to enterprises to decrease. If customer churn continues to occur, the enterprise will gradually lose its competitive advantage.\n",
            "\n",
            "Future Research: Build a real-time or near real-time application to provide close information to make good decisions. Furthermore, handle the imbalanced data using new techniques.\n",
            "\n",
            "\n",
            "----\n",
            "Paper 395:\n",
            "Title: Information Theoretic Learning-Enhanced Dual-Generative Adversarial Networks With Causal Representation for Robust OOD Generalization\n",
            "Abstract: Recently, machine/deep learning techniques are achieving remarkable success in a variety of intelligent control and management systems, promising to change the future of artificial intelligence (AI) scenarios. However, they still suffer from some intractable difficulty or limitations for model training, such as the out-of-distribution (OOD) issue, in modern smart manufacturing or intelligent transportation systems (ITSs). In this study, we newly design and introduce a deep generative model framework, which seamlessly incorporates the information theoretic learning (ITL) and causal representation learning (CRL) in a dual-generative adversarial network (Dual-GAN) architecture, aiming to enhance the robust OOD generalization in modern machine learning (ML) paradigms. In particular, an ITL- and CRL-enhanced Dual-GAN (ITCRL-DGAN) model is presented, which includes an autoencoder with CRL (AE-CRL) structure to aid the dual-adversarial training with causality-inspired feature representations and a Dual-GAN structure to improve the data augmentation in both feature and data levels. Following a newly designed feature separation strategy, a causal graph is built and improved based on the information theory, which can enhance the causally related factors among the separated core features and further enrich the feature representation with the counterfactual features via interventions based on the refined causal relationships. The ITL is incorporated to improve the extraction of low-dimensional feature representations and learn the optimized causal representations based on the idea of “information flow.” A dual-adversarial training mechanism is then developed, which not only enables the generator to expand the boundary of feature distribution in accordance with the optimized feature representation from AE-CRL, but also allows the discriminator to further verify and improve the quality of the augmented data for OOD generalization. Experiment and evaluation results based on an open-source dataset demonstrate the outstanding learning efficiency and classification performance of our proposed model for robust OOD generalization in modern smart applications compared with three baseline methods.\n",
            "----\n",
            "Paper 396:\n",
            "Title: Machine Learning of Medical Applications Involving Complicated Proteins and Genetic Measurements\n",
            "Abstract: Motivations. Breast cancer is the second greatest cause of cancer mortality among women, according to the World Health Organization (WHO), and one of the most frequent illnesses among all women today. The influence is not confined to industrialized nations but also includes emerging countries since the authors believe that increased urbanization and adoption of Western lifestyles will lead to a rise in illness prevalence. Problem Statement. The breast cancer has become one of the deadliest diseases that women are presently facing. However, the causes of this disease are numerous and cannot be properly established. However, there is a huge difficulty in not accurately recognizing breast cancer in its early stages or prolonging the detection process. Methodology. In this research, machine learning is a field of artificial intelligence that employs a variety of probabilistic, optimization, and statistical approaches to enable computers to learn from past data and find and recognize patterns from large or complicated groups. The advantage is particularly well suited to medical applications, particularly those involving complicated proteins and genetic measurements. Result and Implications. However, when using the PCA method to reduce the features, the detection accuracy dropped to 89.9%. IG-ANFIS gave us detection accuracy (98.24%) by reducing the number of variables using the “information gain” method. While the ANFIS algorithm had a detection accuracy of 59.9% without utilizing features, J48, which is one of the decision tree approaches, had a detection accuracy of 92.86% without using features extraction methods. When applying PCA techniques to minimize features, the detection accuracy was lowered to the same way (91.1%) as the Naive Bayes detection algorithm (96.4%).\n",
            "----\n",
            "Paper 397:\n",
            "Title: Artificial Intelligence for Automatic Text Summarization\n",
            "Abstract: Automatic text summarization has played a critical role in helping people obtain key information from increasing huge data with the advantaged development of technology. In the past, few literatures are related to solve the problem of generating titles (short summaries) by using artificial intelligence (AI). The purpose of this study is that we proposed an AI approach for automatic text summarization. We developed an AI text summarization system architecture with three models, namely, statistical model, machine learning model, and deep learning model as well as evaluating the performance of three models. Essay titles and essay abstracts are used to train artificial intelligence deep learning model to generate the candidate titles and evaluated by ROUGE for performance evaluation. The contribution of this paper is that we proposed an AI automatic text summarization system by applying deep learning to generate short summaries from the titles and abstracts of the Web of Science (WOS) database.\n",
            "----\n",
            "Paper 398:\n",
            "Title: Ubiquitous Computing and Distributed Machine Learning in Smart Cities\n",
            "Abstract: The article is devoted to the analysis of the use of ubiquitous computing and distributed machine learning in smart cities. Smart city is characterized by the introduction of high-tech infrastructure, digital services, integrated information monitoring systems that allow to optimize the environment and processes of urban management. The most promising direction of smart cities development is the implementation of ubiquitous computing systems. Ubiquitous computing involves the introduction of a significant number of technologies, including sensors, artificial intelligence, Internet of Things, network robots. Since ubiquitous computing is based on the processing of data generated by different devices, the new solutions are needed to structure and ensure data compatibility. Such solutions are the distributed machine learning methods: stochastic gradient descent and K-means method. The work separately considers the use of federated training, which has advantages in data privacy and mobile computing. The article deals with the main provisions of the concept of smart city, technologies of ubiquitous computing, features of methods of distributed machine learning and their introduction into urban systems management.\n",
            "----\n",
            "Paper 399:\n",
            "Title: Machine learning architecture and framework\n",
            "Abstract: Abstract Machine Learning (ML) is a branch of Artificial Intelligence that enables computer systems to learn from past experiences and improve accordingly without the direct intervention of the programmer. ML enables machines to behave very similarly to human beings. In order to extract the required information from the huge amount of data, ML can be used to design algorithms based on the trends of data and relationships among the data. ML can be applied in various fields such as intrusion detection, bioinformatics, health care, marketing, game playing, and so on. It enables the computers or the machines to make data-driven decisions rather than being explicitly programmed for carrying out a certain task. These programs or algorithms are designed in a way that they learn and improve over time when they are exposed to new or unseen data. Due to the huge amount of data, the significance of ML can be seen in various sections of the society. Especially in industries, ML is assisting exploration of the hidden patterns of the data, and through this the overall performance of the business can be improved. It is cost-effective, affordable, and simple computing techniques allow the analysis and handling of a huge amount of complex data. ML is not only helping to understand and identify the hidden patterns of a diverse set of data but also encourages automation in analysis in place of humans. Also, ML is helping industries to avail of the opportunities and make it profitable in future endeavors. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning, and types of classification. Then, details of the ML architecture and framework are discussed.\n",
            "----\n",
            "Paper 400:\n",
            "Title: A Fast Machine Learning Workflow for Rapid Phenotype Prediction from Whole Shotgun Metagenomes\n",
            "Abstract: Research on the microbiome is an emerging and crucial science that finds many applications in healthcare, food safety, precision agriculture and environmental studies. Huge amounts of DNA from microbial communities are being sequenced and analyzed by scientists interested in extracting meaningful biological information from this big data. Analyzing massive microbiome sequencing datasets, which embed the functions and interactions of thousands of different bacterial, fungal and viral species, is a significant computational challenge. Artificial intelligence has the potential for building predictive models that can provide insights for specific cutting edge applications such as guiding diagnostics and developing personalised treatments, as well as maintaining soil health and fertility. Current machine learning workflows that predict traits of host organisms from their commensal microbiome do not take into account the whole genetic material constituting the microbiome, instead basing the analysis on specific marker genes. In this paper, to the best of our knowledge, we introduce the first machine learning workflow that efficiently performs host phenotype prediction from whole shotgun metagenomes by computing similaritypreserving compact representations of the genetic material. Our workflow enables prediction tasks, such as classification and regression, from Terabytes of raw sequencing data that do not necessitate any pre-prossessing through expensive bioinformatics pipelines. We compare the performance in terms of time, accuracy and uncertainty of predictions for four different classifiers. More precisely, we demonstrate that our ML workflow can efficiently classify real data with high accuracy, using examples from dog and human metagenomic studies, representing a step forward towards real time diagnostics and a potential for cloud applications.\n",
            "----\n",
            "Rate limit exceeded. Retrying in 5 seconds... (Attempt 1/5)\n",
            "Rate limit exceeded. Retrying in 10 seconds... (Attempt 2/5)\n",
            "Rate limit exceeded. Retrying in 20 seconds... (Attempt 3/5)\n",
            "Paper 401:\n",
            "Title: Extracting Landmark and Trait Information from Segmented Digital Specimen Images Generated by Artificial Neural Networks\n",
            "Abstract: We have been successfully developing Artificial Intelligence (AI) models for automatically classifying fish species using neural networks over the last three years during the “Biology Guided Neural Network” (BGNN) project*1. We continue our efforts in another broader project, “Imageomics: A New Frontier of Biological Information Powered by Knowledge-Guided Machine Learning”*2. One of the main topics in the Imageomics Project is “Morphological Barcoding”. Within the Morphological Barcoding study, we are trying to build a gold standard method to identify species in different taxonomic groups based on their external morphology. This list of characters will contain, but not be limited to, landmarks, quantitative traits such as measurements of distances, areas, angles, proportions, colors, histograms, patterns, shapes, and outlines. The taxonomic groups will be limited by the data available, and we will be using fish as the topic of interest in this preliminary study.\n",
            " In this current study, we have focused on extracting morphological characters that are relying on anatomical features of fish, such as location of the eye, body length, and area of the head. We developed a schematic workflow to describe how we processed the data and extract the information (Fig. 1). We performed our analysis on the segmented images produced by Karpatne Team within the BGNN project (Bart et al. 2021). Segmentation analysis was performed using Artificial Neural Networks - Semantic Segmentation (Long et al. 2015); the list of segments to be detected were given as eye, head, trunk, caudal fin, pectoral fin, dorsal fin, anal fin, pelvic fin for fish.\n",
            " Segmented images, metadata and species lists were given as input to the workflow. During the cleaning and filtering subroutines, a subset of data was created by filtering down to the desired segmented images with corresponding metadata. In the validation step, segmented images were checked by comparing the number of specimens in the original image to the separate bounding-boxed specimen images, noting: violations in the segmentations, counts of segments, comparisons of relative positions of the segments among one another, traces of batch effect; comparisons according to their size and shape and finally based on these validation criteria each segmented image was assigned a score from 1 to 5 similar to Adobe XMP Basic namespace.\n",
            " The landmarks and the traits to be used in the study were extracted from the current literature, while mindful that some of the features may not be extracted successfully computationally. By using the landmark list, landmarks have been extracted by adapting the descriptions from the literature on to the segments, such as picking the left most point on the head as the tip of snout and top left point on the pelvic fin as base of the pelvic fin. These 2D vectors (coordinates), are then fine tuned by adjusting their positions to be on the outline of the fish, since most of the landmarks are located on the outline. Procrustes analysis*3 was performed to scale all of the measurements together and point clouds were generated. These vectors were stored as landmark data. Segment centroids were also treated as landmarks. Extracted landmarks were validated by comparing their relative position among each other, and then if available, compared with their manually captured position. A score was assigned based on these comparisons, similar to the segmentation validation score. Based on the trait list definitions, traits were extracted by measuring the distances between two landmarks, angles between three landmarks, areas between three or more landmarks, areas of the segments, ratios between two distances or areas and between a distance and a square rooted area and then stored as trait data. Finally these values were compared within their own species clusters for errors and whether the values were still within the bounds. Trait scores were calculated from these error calculations similar to segmentation scores aiming selecting good quality scores for further analysis such as Principal Component Analysis.\n",
            " Our work on extraction of features from segmented digital specimen images has shown that the accuracy of the traits such as measurements, areas, and angles depends on the accuracy of the landmarks. Accuracy of the landmarks is highly dependent on segmentation of the parts of the specimen. The landmarks that are located on the outline of the body (combination of head and trunk segments of the fish) are found to be more accurate comparing to the landmarks that represents inner features such as mouth and pectoral fin in some taxonomic groups. However, eye location is almost always accurate, since it is based on the centroid of the eye segment. In the remaining part of this study we will improve the score calculation for segments, images, landmarks and traits and calculate the accuracy of the scores by comparing the statistical results obtained by analysis of the landmark and trait data.\n",
            "----\n",
            "Paper 402:\n",
            "Title: Application Research Based on Machine Learning in Network Privacy Security\n",
            "Abstract: As the hottest frontier technology in the field of artificial intelligence, machine learning is subverting various industries step by step. In the future, it will penetrate all aspects of our lives and become an indispensable technology around us. Among them, network security is an area where machine learning can show off its strengths. Among many network security problems, privacy protection is a more difficult problem, so it needs more introduction of new technologies, new methods and new ideas such as machine learning to help solve some problems. The research contents for this include four parts: an overview of machine learning, the significance of machine learning in network security, the application process of machine learning in network security research, and the application of machine learning in privacy protection. It focuses on the issues related to privacy protection and proposes to combine the most advanced matching algorithm in deep learning methods with information theory data protection technology, so as to introduce it into biometric authentication. While ensuring that the loss of matching accuracy is minimal, a high-standard privacy protection algorithm is concluded, which enables businesses, government entities, and end users to more widely accept privacy protection technology.\n",
            "----\n",
            "Paper 403:\n",
            "Title: The Technological Elements of Artificial Intelligence\n",
            "Abstract: We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.\n",
            "----\n",
            "Paper 404:\n",
            "Title: State-of-the-art of artificial intelligence in medicinal chemistry\n",
            "Abstract: As in many areas of science and technology, artificial intelligence (AI) is currently promoted with high expectations in drug discovery and medicinal chemistry. Here, AI mostly refers to machine learning (ML), which is only a part of the methodological AI spectrum. The high level of interest in AI essentially originates from deep learning using multi-layered neural network (NN) architectures. Other AI approaches entering medicinal chemistry include expert systems and (laboratory) robotics. However, deep learning is clearly predominant. Of note, ML already has a long history in chemoinformatics and medicinal chemistry. For more than two decades, ML methods have been extensively applied for compound property predictions. In medicinal chemistry, properties of interest for computational studies include, first and foremost, biological activities of small molecules, but also physiochemical (e.g., solubility) or in vivo properties (such as metabolic stability or toxicity). Predictions of such properties aim to support the key task in the practice of medicinal chemistry: deciding which compound(s) to synthesize next. Over the years, NNs – which were popular early on for property predictions – were for the most part replaced by other ML methods such as support vector machines, random forests or Bayesian modeling. This was largely due to the tendency of NNs to overfit models to training data and also to the black box character of their predictions (the black box also applies to other – but not all – ML methods). In medicinal chemistry, chemical intuition continues to play a major role and black box predictions that cannot be explained in chemical terms work against the acceptance of ML for practical applications. Recently, NNs have been experiencing a renaissance in medicinal chemistry with the advent of deep neural networks (DNNs) and high expectations associated with deep ML. These expectations have primarily originated from other fields such as computer vision (image analysis), natural language processing or network science (including social networks).\n",
            "----\n",
            "Paper 405:\n",
            "Title: Machine learning for data-driven discovery in solid Earth geoscience\n",
            "Abstract: Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.\n",
            "----\n",
            "Paper 406:\n",
            "Title: Explainable Machine Learning and Mining of Influential Patterns from Sparse Web\n",
            "Abstract: With advancements in modern technology in the current era, very large volumes of big data have been generated and collected in numerous real-life applications. These have formed a connected world comprising webs of agents, data, people, things and trust. Some of these webs have also emerged in health and smart living. As valuable information and knowledge is embedded in these rich sets of webs, web intelligence is in demand. In this paper, we focus on a data science task of web usage mining. In particular, we present a web intelligent solution to conduct explainable machine learning and mining of influential patterns from sparse web. It provides a compressed representation of sparse web, discovers influential websites and/or web pages that are frequently browsed or surfed by web surfers, and recommends these influential websites and/or web pages to other web surfers. Evaluation results show the effectiveness (especially, in data compression), interpretability and practicality of our solution.\n",
            "----\n",
            "Paper 407:\n",
            "Title: Machine Learning-based Flu Forecasting Study Using the Official Data from the Centers for Disease Control and Prevention and Twitter Data\n",
            "Abstract: Aim/Purpose: In the United States, the Centers for Disease Control and Prevention (CDC) tracks the disease activity using data collected from medical practice's on a weekly basis. Collection of data by CDC from medical practices on a weekly basis leads to a lag time of approximately 2 weeks before any viable action can be planned. The 2-week delay problem was addressed in the study by creating machine learning models to predict flu outbreak.\n",
            "\n",
            "Background: The 2-week delay problem was addressed in the study by correlation of the flu trends identified from Twitter data and official flu data from the Centers for Disease Control and Prevention (CDC) in combination with creating a machine learning model using both data sources to predict flu outbreak.\n",
            "\n",
            "Methodology: A quantitative correlational study was performed using a quasi-experimental design. Flu trends from the CDC portal and tweets with mention of flu and influenza from the state of Georgia were used over a period of 22 weeks from December 29, 2019 to May 30, 2020 for this study.\n",
            "\n",
            "Contribution: This research contributed to the body of knowledge by using a simple bag-of-word method for sentiment analysis followed by the combination of CDC and Twitter data to generate a flu prediction model with higher accuracy than using CDC data only.\n",
            "\n",
            "Findings: The study found that (a) there is no correlation between official flu data from CDC and tweets with mention of flu and (b) there is an improvement in the performance of a flu forecasting model based on a machine learning algorithm using both official flu data from CDC and tweets with mention of flu.\n",
            "\n",
            "Recommendations for Practitioners: In this study, it was found that there was no correlation between the official flu data from the CDC and the count of tweets with mention of flu, which is why tweets alone should be used with caution to predict a flu out-break. Based on the findings of this study, social media data can be used as an additional variable to improve the accuracy of flu prediction models. It is also found that fourth order polynomial and support vector regression models offered the best accuracy of flu prediction models.\n",
            "\n",
            "Recommendations for Researchers: Open-source data, such as Twitter feed, can be mined for useful intelligence benefiting society. Machine learning-based prediction models can be improved by adding open-source data to the primary data set.\n",
            "\n",
            "Impact on Society: Key implication of this study for practitioners in the field were to use social media postings to identify neighborhoods and geographic locations affected by seasonal outbreak, such as influenza, which would help reduce the spread of the disease and ultimately lead to containment. Based on the findings of this study, social media data will help health authorities in detecting seasonal outbreaks earlier than just using official CDC channels of disease and illness reporting from physicians and labs thus, empowering health officials to plan their responses swiftly and allocate their resources optimally for the most affected areas.\n",
            "\n",
            "Future Research: A future researcher could use more complex deep learning algorithms, such as Artificial Neural Networks and Recurrent Neural Networks, to evaluate the accuracy of flu outbreak prediction models as compared to the regression models used in this study. A future researcher could apply other sentiment analysis techniques, such as natural language processing and deep learning techniques, to identify context-sensitive emotion, concept extraction, and sarcasm detection for the identification of self-reporting flu tweets. A future researcher could expand the scope by continuously collecting tweets on a public cloud and applying big data applications, such as Hadoop and MapReduce, to perform predictions using several months of historical data or even years for a larger geographical area.\n",
            "----\n",
            "Paper 408:\n",
            "Title: Machine learning for tracking illegal wildlife trade on social media\n",
            "Abstract: None\n",
            "----\n",
            "Paper 409:\n",
            "Title: A Study on Machine Learning Approaches for Named Entity Recognition\n",
            "Abstract: Natural Language Processing (NLP) is one of the sub-parts of Artificial Intelligence which normally focuses on empowering computers to understand and operate on human languages and to get computers closer towards understanding a language as a human does. Named Entity Recognition (NER) is core of NLP systems. NER is a process of automatic identification of named entities in a given text or document. Named entities are real world objects or in general named entities are proper nouns like name of person, location, date and time expression etc. The recognition and extraction of real named entity is very important for solving difficulties in many research areas like Question Answering and Summarization Systems, Information Extraction, Machine Learning, Semantic Web Search and Bio-informatics, Video Annotation and many more. In this paper the major focus is given on comprehending different types of NER and approaches applied for NER especially different machine learning models used for identification of Named Entities.\n",
            "----\n",
            "Paper 410:\n",
            "Title: IoT with Big Data Framework using Machine Learning Approach\n",
            "Abstract: In future IoT (Internet of Things), big-data administration & machine learning disclosure for expansive scale modern robotization application, the significance of mechanical internet is expanding step by step. The interconnection by means of the Internet of computing gadgets installed in ordinary items, empowering them to send and get information. BD is informational collections that are so voluminous and complex that customary information preparing application programming are insufficient to manage them. ML is a subset of artificial intelligence that regularly utilizes measurable procedures to enable PCs to \"learn\" with information, without being expressly modified. A few differentiated advancements, for example, IoT, computational intelligence, machine type communication, BD, & sensor technology can be fused together to enhance the data administration & information revelation effectiveness of expansive scale robotization applications. An expanding measure of significant data sources, propels in IoT & Big Data (BD) advances & also the accessibility of an extensive variety of machine learning (ML) calculations offers new potential to convey logical administrations to nationals & urban chiefs. In any case, there is as yet a hole in joining the present best in class in an incorporated system that would help lessening improvement costs & empower new sort of administrations. Voluminous measures of data have been created, since the previous decade as the scaling down of IoT gadgets increments. Be that as it may, such data are not valuable without scientific power. Various BD, IoT, & investigation arrangements have empowered individuals to acquire profitable knowledge into extensive information created by IoT gadgets. However, these arrangements are still in their earliest stages, & the domain does not have a thorough review on this. Here we endeavored to give a reasonable more profound understanding about the IoT in BD structure alongside its different issues & challenges & concentrated on giving conceivable arrangement by ML strategy.\n",
            "----\n",
            "Paper 411:\n",
            "Title: Design of Knowledge Base and Curriculum Planning Based on Artificial Intelligence Teaching System\n",
            "Abstract: The application of artificial intelligence in early teaching resulted in the intelligent teaching System. It is a comprehensive subject involving computer science, artificial intelligence, psychology, education and behavior. Its main purpose is to make the computer system the guidance and assistant of learning, to endure the intelligence to the computer system, and to replace the human teacher to some extent, to achieve the best teaching effect. From the perspective of results, it changes the traditional teaching mode and teaching environment, can complete the teaching task in the traditional teaching mode, while playing an irreplaceable good teaching effect. However, in the development of computer network technology, people's demand has gradually increased, intelligent teaching system also began to show its own shortcomings, such as: low degree of intelligence and personalization, waste of a lot of student information resources, lack of knowledge base and other shortcomings. In this paper, the data mining technology is applied to the development of intelligent teaching system, so as to mine more and more important information in the student information database, and the knowledge base is fully applied or reorganized or enriched. By using the data mining technology for reference, the intelligent and personalized service of the intelligent teaching system is strengthened.\n",
            "----\n",
            "Paper 412:\n",
            "Title: Machine learning in biomedical engineering\n",
            "Abstract: None\n",
            "----\n",
            "Paper 413:\n",
            "Title: Artificial intelligence and physicians in the future of medicine: a meeting of minds?\n",
            "Abstract: Artificial intelligence (AI) is one of today's most powerful technologies. Having already transformed the business world, AI may be poised to transform healthcare next. Current AI systems demonstrate impressive competency in certain tasks of clinical medicine. Machine learning approaches to creating AI are of particular relevance in healthcare, given the ability of modern machine learning algorithms to work with large amounts of complex data and generate intelligent predictions therefrom. Here we propose that much of what physicians do can be modelled as information processing and thus can be performed by AI. We further propose that whereas certain AI systems may adopt approaches based on novel pattern extraction and interpretation, and thus diverge from human physician cognition, AI is well-positioned to assist physicians by operating in parallel alongside them. Navigating the intersection of physician and AI competence will be a tremendous and complex challenge, but may return high rewards in improving patient outcomes and lead to transformative gains in medical knowledge. Advances in AI will have tremendous and complex impact on the future of medicine. \n",
            "Keywords: arti cial intelligence, machine learning, information theory, computer science, healthcare, medicine\n",
            "----\n",
            "Paper 414:\n",
            "Title: Applying Internet of Things and Machine-Learning for Personalized Healthcare: Issues and Challenges\n",
            "Abstract: Personalized Healthcare (PH) is a new patientoriented healthcare approach which expects to improve the traditional healthcare system. The focus of this new advancement is the patient data collected from patient Electronic health records (EHR), Internet of Things (IoT) sensor devices, wearables and mobile devices, web-based information and social media. PH applies Artificial Intelligence (AI) techniques to the collected dataset to improve disease progression technique, disease prediction, patient selfmanagement and clinical intervention. Machine learning techniques are widely used in this regard to develop analytic models. These models are integrated into different healthcare service applications and clinical decision support systems. These models mainly analyse the collected data from sensor devices and other sources to identify behavioral patterns and clinical conditions of the patient. For example, these models analyse the collected data to identify the patient's improvements, habits and anomaly in daily routine, changes in sleeping and mobility, eating, drinking and digestive pattern. Based on those patterns the healthcare applications and the clinical decision support systems recommend lifestyle advice, special treatment and care plans for the patient. The doctors and caregivers can also be engaged in the care plan process to validate lifestyle advice. However, there are many uncertainties and a grey area when it comes to applying machine learning in this context. Clinical, behaviour and lifestyle data in nature are very sensitive. There could be different types of biased involved in the process of data collection and interpretation. The training data model could have an older version of the dataset. All these could lead to an incorrect decision from the system without the user's knowledge. In this paper, some of the standards of the ML models reported in the recent research trends, identify the reliability issues and propose improvements.\n",
            "----\n",
            "Paper 415:\n",
            "Title: A Review on Machine Learning Styles in Computer Vision—Techniques and Future Directions\n",
            "Abstract: Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.\n",
            "----\n",
            "Paper 416:\n",
            "Title: Artificial intelligence for ocean science data integration: current state, gaps, and way forward\n",
            "Abstract: Oceanographic research is a multidisciplinary endeavor that involves the acquisition of an increasing amount of in-situ and remotely sensed data. A large and growing number of studies and data repositories are now available on-line. However, manually integrating different datasets is a tedious and grueling process leading to a rising need for automated integration tools. A key challenge in oceanographic data integration is to map between data sources that have no common schema and that were collected, processed, and analyzed using different methodologies. Concurrently, artificial agents are becoming increasingly adept at extracting knowledge from text and using domain ontologies to integrate and align data. Here, we deconstruct the process of ocean science data integration, providing a detailed description of its three phases: discover, merge, and evaluate/correct. In addition, we identify the key missing tools and underutilized information sources currently limiting the automation of the integration process. The efforts to address these limitations should focus on (i) development of artificial intelligence-based tools for assisting ocean scientists in aligning their schema with existing ontologies when organizing their measurements in datasets; (ii) extension and refinement of conceptual coverage of – and conceptual alignment between – existing ontologies, to better fit the diverse and multidisciplinary nature of ocean science; (iii) creation of ocean-science-specific entity resolution benchmarks to accelerate the development of tools utilizing ocean science terminology and nomenclature; (iv) creation of ocean-science-specific schema matching and mapping benchmarks to accelerate the development of matching and mapping tools utilizing semantics encoded in existing vocabularies and ontologies; (v) annotation of datasets, and development of tools and benchmarks for the extraction and categorization of data quality and preprocessing descriptions from scientific text; and (vi) creation of large-scale word embeddings trained upon ocean science literature to accelerate the development of information extraction and matching tools based on artificial intelligence.\n",
            "----\n",
            "Paper 417:\n",
            "Title: The Impact of Artificial Intelligence on Rules, Standards, and Judicial Discretion\n",
            "Abstract: Artificial intelligence (AI), and machine learning in particular, promises lawmakers greater specificity and fewer errors. Algorithmic lawmaking and judging will leverage models built from large stores of data that permit the creation and application of finely tuned rules. AI is therefore regarded as something that will bring about a movement from standards to rules. Drawing on contemporary data science, this Article shows that machine learning is less impressive when the past is unlike the future, as it is whenever new variables appear over time. In the absence of regularities, machine learning loses its advantage and, as a result, looser standards can become superior to rules. We apply this insight to bail and sentencing decisions, as well as familiar corporate and contract law rules. More generally, we show that a Human-AI combination can be superior to AI acting alone. Just as today’s judges overrule errors and outmoded precedent, tomorrow’s lawmakers will sensibly overrule AI in legal domains where the challenges of measurement are present. When measurement is straightforward and prediction is accurate, rules will prevail. When empirical limitations such as overfit, Simpson’s Paradox, and omitted variables make measurement difficult, AI should be trusted less and law should give way to standards.\n",
            "----\n",
            "Paper 418:\n",
            "Title: Artificial intelligence techniques and their application in oil and gas industry\n",
            "Abstract: None\n",
            "----\n",
            "Paper 419:\n",
            "Title: Predicting COVID19 Spread in Saudi Arabia Using Artificial Intelligence Techniques—Proposing a Shift Towards a Sustainable Healthcare Approach\n",
            "Abstract: None\n",
            "----\n",
            "Paper 420:\n",
            "Title: Digital applications and artificial intelligence in agriculture toward next-generation plant phenotyping\n",
            "Abstract: ABSTRACT In the upcoming years, global changes in agricultural and environmental systems will require innovative approaches in crop research to ensure more efficient use of natural resources and food security. Cutting-edge technologies for precision agriculture are fundamental to improve in a non-invasive manner, the efficiency of detection of environmental parameters, and to assess complex traits in plants with high accuracy. The application of sensing devices and the implementation of strategies of artificial intelligence for the acquisition and management of high-dimensional data will play a key role to address the needs of next-generation agriculture and boosting breeding in crops. To that end, closing the gap with the knowledge from the other ‘omics’ sciences is the primary objective to relieve the bottleneck that still hinders the potential of thousands of accessions existing for each crop. Although it is an emerging discipline, phenomics does not rely only on technological advances but embraces several other scientific fields including biology, statistics and bioinformatics. Therefore, establishing synergies among research groups and transnational efforts able to facilitate access to new computational methodologies and related information to the community, are needed. In this review, we illustrate the main concepts of plant phenotyping along with sensing devices and mechanisms underpinning imaging analysis in both controlled environments and open fields. We then describe the role of artificial intelligence and machine learning for data analysis and their implication for next-generation breeding, highlighting the ongoing efforts toward big-data management.\n",
            "----\n",
            "Paper 421:\n",
            "Title: Data Science for Local Government\n",
            "Abstract: The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.\n",
            "----\n",
            "Paper 422:\n",
            "Title: A Research on Machine Learning Methods and Its Applications\n",
            "Abstract: Machine learning is a science which was found and developed as a subfield of artificial intelligence in the 1950s. The first steps of machine learning goes back to the 1950s but there were no significant researches and developments on this science. However, in the 1990s, the researches on this field restarted, developed and have reached to this day. It is a science that will improve more in the future. The reason behind this development is the difficulty of analysing and processing the rapidly increasing data. Machine learning is based on the principle of finding the best model for the new data among the previous data thanks to this increasing data. Therefore, machine learning researches will go on in parallel with the increasing data. This research includes the history of machine learning, the methods used in machine learning, its application fields, and the researches on this field. The aim of this study is to transmit the knowledge on machine learning, which has become very popular nowadays, and its applications to the researchers.\n",
            "----\n",
            "Paper 423:\n",
            "Title: Collaborative SQL-injections detection system with machine learning\n",
            "Abstract: Data mining and information extraction from data is a field that has gained relevance in recent years thanks to techniques based on artificial intelligence and use of machine and deep learning. The main aim of the present work is the development of a tool based on a previous behaviour study of security audit tools (oriented to SQL pentesting) with the purpose of creating testing sets capable of performing an accurate detection of a SQL attack. The study is based on the information collected through the generated web server logs in a pentesting laboratory environment. Then, making use of the common extracted patterns from the logs, each attack vector has been classified in risk levels (dangerous attack, normal attack, non-attack, etc.). Finally, a training with the generated data was performed in order to obtain a classifier system that has a variable performance between 97 and 99 percent in positive attack detection. The training data is shared to other servers in order to create a distributed network capable of deciding if a query is an attack or is a real petition and inform to connected clients in order to block the petitions from the attacker's IP.\n",
            "----\n",
            "Paper 424:\n",
            "Title: Artificial Intelligence for COVID-19 Drug Discovery and Vaccine Development\n",
            "Abstract: SARS-COV-2 has roused the scientific community with a call to action to combat the growing pandemic. At the time of this writing, there are as yet no novel antiviral agents or approved vaccines available for deployment as a frontline defense. Understanding the pathobiology of COVID-19 could aid scientists in their discovery of potent antivirals by elucidating unexplored viral pathways. One method for accomplishing this is the leveraging of computational methods to discover new candidate drugs and vaccines in silico. In the last decade, machine learning-based models, trained on specific biomolecules, have offered inexpensive and rapid implementation methods for the discovery of effective viral therapies. Given a target biomolecule, these models are capable of predicting inhibitor candidates in a structural-based manner. If enough data are presented to a model, it can aid the search for a drug or vaccine candidate by identifying patterns within the data. In this review, we focus on the recent advances of COVID-19 drug and vaccine development using artificial intelligence and the potential of intelligent training for the discovery of COVID-19 therapeutics. To facilitate applications of deep learning for SARS-COV-2, we highlight multiple molecular targets of COVID-19, inhibition of which may increase patient survival. Moreover, we present CoronaDB-AI, a dataset of compounds, peptides, and epitopes discovered either in silico or in vitro that can be potentially used for training models in order to extract COVID-19 treatment. The information and datasets provided in this review can be used to train deep learning-based models and accelerate the discovery of effective viral therapies.\n",
            "----\n",
            "Paper 425:\n",
            "Title: Machine Learning of Mineralization-Related Geochemical Anomalies: A Review of Potential Methods\n",
            "Abstract: None\n",
            "----\n",
            "Paper 426:\n",
            "Title: On to the next chapter for crop breeding: Convergence with data science\n",
            "Abstract: Crop breeding is as ancient as the invention of cultivation.  In essence, the objective of crop breeding is to improve plant fitness under human cultivation conditions, making crops more productive while maintaining consistency in life cycle and quality. The applications of predictive breeding has been gaining momentum in agricultural industry and public breeding programs for the last decade, in the aftermath of genomic selection being recognized and widely applied for accelerating genetic gain in breeding programs. The massive amounts of data that has been generated by industry and farmers year after year through several decades has finally been recognized as an asset. A wide range of analytical methods such as machine learning, deep learning and artificial intelligence that were initially developed for diverse quantitative disciplines are now being adopted to crop breeding decision making processes. New technologies are currently being developed that would enable integration of data from various domains such as geospatial variables and a multitude of phenotypic responses as well as genetic information, in order to identify, develop and improve crop faster via partial or full automation of the decisions that pertain to variety development. Here we will discuss and summarize efforts from public and private domains for predictive analytics, and its applications to crop breeding and agricultural product development, and provide suggestions for future research.\n",
            "----\n",
            "Paper 427:\n",
            "Title: Machine Learning and Natural Language Processing in Mental Health: Systematic Review\n",
            "Abstract: Background Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining. Objective The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice Methods This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed. Results A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform. Conclusions Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients’ daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.\n",
            "----\n",
            "Paper 428:\n",
            "Title: 2018 special issue on artificial intelligence 2.0: theories and applications\n",
            "Abstract: In July 2017, the Chinese government issued a guideline on developing artificial intelligence (AI), namely, the ‘New-Generation Artificial Intelligence Development Plan’, through 2030 to the public, setting a goal of becoming a global innovation center in this field by 2030. According to the development plan, breakthroughs should be made in basic theories of AI in terms of big data intelligence, cross-media computing, human-machine hybrid intelligence, collective intelligence, autonomous unmanned decisionmaking, brain-like computing, and quantum intelligent computing. The next-generation AI would be never-ending (self) learning from data and experience, intuitive reasoning and adaptation (Pan, 2016, 2017). From the perspective of overcoming the limitation of existing AI, it is generally recognized that the crossdisciplinary collaboration is a key for AI having real impact on the world. Thanks for the efforts from researchers in computer science, statistics, robotics, and psychiatry, the topics in this special issue consist mainly of five subjects: (1) fundamental issues in AI such as interpretable deep learning and unsupervised learning (i.e., domain adaptation and generative adversarial learning); (2) brain-like learning such as spiking neural network and memory-augmented reasoning; (3) human-in-the-loop learning such as crowdsourcing design and digital brain with crowd power; (4) creative applications such as social chatbots (i.e., XiaoICe) and automatic speech recognition; (5) Dr. Raj Reddy from CMU shared his view on the new-generation AI, Prof. Bin Yu from UC Berkeley advocated that AI should use statistical concepts through humanmachine collaboration, and researchers from the Chinese Academy of Sciences surveyed the acceleration of deep neural networks. All of interview, perspective, survey, and research papers target rethinking the appropriate ways for a general scenario or a specific application. In an interview, Dr. Raj Reddy shared his views on the new-generation AI and detailed the idea of cognition amplifiers and guardian angles (FITEE editorial staff, 2018). Yu and Kumbier (2018) discussed how humanmachine collaboration can be approached in AI through the statistical concepts of population, question of interest, representativeness of training data, and scrutiny of results (PQRS). The PQRS workflow provides a conceptual framework for integrating statistical ideas with human input into AI products and research. Shum et al. (2018) discussed the issue of social chatbots. The design of social chatbots must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Using XiaoIce as an illustrative example, authors introduced key technologies in building social chatbots from core chat to visual sense to skills. Zhang and Zhu (2018) reviewed recent studies in emerging directions of understanding neuralnetwork representations and learning interpretable neural networks. They revisited visualization of convolutional neural network (CNN) representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling CNN representations, learning of Editorial: Frontiers of Information Technology & Electronic Engineering www.jzus.zju.edu.cn; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn\n",
            "----\n",
            "Paper 429:\n",
            "Title: Open source intelligence extraction for terrorism‐related information: A review\n",
            "Abstract: In this contemporary era, where a large part of the world population is deluged by extensive use of the internet and social media, terrorists have found it a potential opportunity to execute their vicious plans. They have got a befitting medium to reach out to their targets to spread propaganda, disseminate training content, operate virtually, and further their goals. To restrain such activities, information over the internet in context of terrorism needs to be analyzed to channel it to appropriate measures in combating terrorism. Open Source Intelligence (OSINT) accounts for a felicitous solution to this problem, which is an emerging discipline of leveraging publicly accessible sources of information over the internet by effectively utilizing it to extract intelligence. The process of OSINT extraction is broadly observed to be in three phases (i) Data Acquisition, (ii) Data Enrichment, and (iii) Knowledge Inference. In the context of terrorism, researchers have given noticeable contributions in compliance with these three phases. However, a comprehensive review that delineates these research contributions into an integrated workflow of intelligence extraction has not been found. The paper presents the most current review in OSINT, reflecting how the various state‐of‐the‐art tools and techniques can be applied in extracting terrorism‐related textual information from publicly accessible sources. Various data mining and text analysis‐based techniques, that is, natural language processing, machine learning, and deep learning have been reviewed to extract and evaluate textual data. Additionally, towards the end of the paper, we discuss challenges and gaps observed in different phases of OSINT extraction.\n",
            "----\n",
            "Paper 430:\n",
            "Title: Machine learning application in growth and health prediction of broiler chickens\n",
            "Abstract: Artificial intelligence (AI) already represents a factor for increasing efficiency and productivity in many sectors, and there is a need for expanding its implementation in animal science. There is a growing demand for the development and use of smart devices at the farm level, which would generate enough data, which increases the potential for AI using machine learning algorithms and real-time analysis. Machine learning (ML) is a category of algorithm that allows software to become accurate in predicting outcomes without being explicitly programmed. The essential principle of machine learning is to construct algorithms that can receive input data and use statistical analysis to predict an output. Exploitation of machine learning approaches, by using different training inputs, derived the prediction accuracy of growth and body weight in broiler chickens that ranged from 98 to 99%. Furthermore, a neural network with an accuracy of 100% identified the presence or absence of ascites in broiler chickens, while the support vector machine (SVM) model obtained an accuracy rate of 99.5% in combination with machine vision for the recognition of healthy and bird flu-challenged chickens. Consequently, machine learning algorithms, besides accurate growth prediction of broiler chickens, can successfully contribute to health disorders prediction. It is obvious that machine learning has a great potential for application in the future. This paper analyses machine learning applications in broiler growth and health prediction, and its ability to cope with high inputs of data and non-linearity can successfully replace common methodology.\n",
            "----\n",
            "Paper 431:\n",
            "Title: Hybrid Artificial Intelligence and IoT in Health care for Cardiovascular Patient in Decision-Making System\n",
            "Abstract: None\n",
            "----\n",
            "Paper 432:\n",
            "Title: High-Dimensional Similarity Query Processing for Data Science\n",
            "Abstract: Similarity query (a.k.a. nearest neighbor query) processing has been an active research topic for several decades. It is an essential procedure in a wide range of applications (e.g., classification & regression, deduplication, image retrieval, and recommender systems). Recently, representation learning and auto-encoding methods as well as pre-trained models have gained popularity. They basically deal with dense high-dimensional data, and this trend brings new opportunities and challenges to similarity query processing. Meanwhile, new techniques have emerged to tackle this long-standing problem theoretically and empirically. This tutorial aims to provide a comprehensive review of high-dimensional similarity query processing for data science. It introduces solutions from a variety of research communities, including data mining (DM), database (DB), machine learning (ML), computer vision (CV), natural language processing (NLP), and theoretical computer science (TCS), thereby highlighting the interplay between modern computer science and artificial intelligence technologies. We first discuss the importance of high-dimensional similarity query processing in data science applications, and then review query processing algorithms such as cover tree, locality sensitive hashing, product quantization, proximity graphs, as well as recent advancements such as learned indexes. We analyze their strengths and weaknesses and discuss the selection of algorithms in various application scenarios. Moreover, we consider the selectivity estimation of high-dimensional similarity queries, and show how researchers are bringing in state-of-the-art ML techniques to address this problem. We expect that this tutorial will provide an impetus towards new technologies for data science.\n",
            "----\n",
            "Paper 433:\n",
            "Title: Introductory Chapter: Artificial Intelligence - Challenges and Applications\n",
            "Abstract: Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:\n",
            "----\n",
            "Paper 434:\n",
            "Title: Machine Learning for Environmental Toxicology: A Call for Integration and Innovation.\n",
            "Abstract: Recent advances in computing power have enabled the application of machine learning (ML) across all areas of science. A step change from a data-rich landscape to one where new hypotheses, relationships, and knowledge is emerging as a result. While ML is related to artificial intelligence (AI), they are not the same. ML is a branch of AI involving the application of statistical algorithms to enable a system to learn. Learning can involve data interpretation, identification of patterns and decision making. However, application and acceptance of ML within environmental toxicology, and more specifically for our viewpoint, environmental risk assessment (ERA), remains low. ML is an example of a disruptive research technology, which is urgently needed to cope with the complexity and scale of work required.\n",
            "----\n",
            "Paper 435:\n",
            "Title: IFLBC: On the Edge Intelligence Using Federated Learning Blockchain Network\n",
            "Abstract: Lately there has been an increase in the number of Machine Learning (ML) and Artificial Intelligence (AI) applications ranging from recommendation systems to face to speech recognition. At the helm of the advent of deep learning is the proliferation of data from diverse data sources ranging from Internet-of-Things (IoT) devices to self-driving automobiles. Tapping into this unlimited reservoir of information presents the problem of finding quality data out of a myriad of irrelevant ones, which to this day, has been a significant issue in data science with a direct ramification of this being the inability to generate quality ML models for useful predictive analysis. Edge computing has been deemed a solution to some of issues such as privacy, security, data silos and latency, as it ventures to bring cloud computing services closer to end-nodes. A new form of edge computing known as edge-AI attempts to bring ML, AI, and predictive analytics services closer to the data source (end devices). In this paper, we investigate an approach to bring edge-AI to end-nodes through a shared machine learning model powered by the blockchain technology and a federated learning framework called iFLBC edge. Our approach addresses the issue of the scarcity of relevant data by devising a mechanism known as the Proof of Common Interest (PoCI) to sieve out relevant data from irrelevant ones. The relevant data is trained on a model, which is then aggregated along with other models to generate a shared model that is stored on the blockchain. The aggregated model is downloaded by members of the network which they can utilize for the provision of edge intelligence to end-users. This way, AI can be more ubiquitous as members of the iFLBC network can provide intelligence services to end-users.\n",
            "----\n",
            "Paper 436:\n",
            "Title: Lifelong machine learning: a paradigm for continuous learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 437:\n",
            "Title: Artificial Intelligence Based Sentiment Analysis for Health Crisis Management in Smart Cities\n",
            "Abstract: Smart city promotes the unification of conventional urban infrastructure and information technology (IT) to improve the quality of living and sustainable urban services in the city. To accomplish this, smart cities necessitate collaboration among the public as well as private sectors to install IT platforms to collect and examinemassive quantities of data. At the same time, it is essential to design effective artificial intelligence (AI) based tools to handle healthcare crisis situations in smart cities. To offer proficient services to people during healthcare crisis time, the authorities need to look closer towards them. Sentiment analysis (SA) in social networking can provide valuable information regarding public opinion towards government actions. With this motivation, this paper presents a new AI based SA tool for healthcare crisis management (AISA-HCM) in smart cities. The AISA-HCM technique aims to determine the emotions of the people during the healthcare crisis time, such as COVID-19. The proposed AISA-HCM technique involves distinct operations such as pre-processing, feature extraction, and classification. Besides, brain stormoptimization (BSO) with deep belief network (DBN), called BSODBN model is employed for feature extraction. Moreover, beetle antenna search with extreme learning machine (BAS-ELM) method was utilized for classifying the sentiments as to various classes. The use of BSO and BAS algorithms helps to effectively modify the parameters involved in the DBN andELMmodels respectively. The performance validation of the AISA-HCM technique takes place using Twitter data and the outcomes are examined with respect to various measures. The experimental outcomes highlighted the enhanced performance of the AISA-HCM technique over the recent state of art SA approaches with the maximum precision of 0.89, recall of 0.88, Fmeasure of 0.89, and accuracy of 0.94. © 2022 Tech Science Press. All rights reserved.\n",
            "----\n",
            "Paper 438:\n",
            "Title: Machine learning and its applications: A review\n",
            "Abstract: Nowadays, large amount of data is available everywhere. Therefore, it is very important to analyze this data in order to extract some useful information and to develop an algorithm based on this analysis. This can be achieved through data mining and machine learning. Machine learning is an integral part of artificial intelligence, which is used to design algorithms based on the data trends and historical relationships between data. Machine learning is used in various fields such as bioinformatics, intrusion detection, Information retrieval, game playing, marketing, malware detection, image deconvolution and so on. This paper presents the work done by various authors in the field of machine learning in various application areas.\n",
            "----\n",
            "Paper 439:\n",
            "Title: An Information-theoretic approach to dimensionality reduction in data science\n",
            "Abstract: None\n",
            "----\n",
            "Paper 440:\n",
            "Title: The NeurIPS '18 Competition: From Machine Learning to Intelligent Conversations\n",
            "Abstract: None\n",
            "----\n",
            "Paper 441:\n",
            "Title: Artificial Intelligence in Education: A Bibliometric Study\n",
            "Abstract: The aim of this study is to examine the studies in the literature on the use of artificial intelligence in education in terms of its bibliometric properties. The Web of Science (WoS) database was used to collect the data. Various keywords were used to search the literature, and a total of 2,686 publications on the subject published between 2001-2021 were found. The inquiry revealed that most of the studies were carried out in the USA. According to the results, it was seen that the most frequently published journals were Computers Education and International Journal of Emerging Technologies in Learning. The study showed that the institutions of the authors were in the first place as Carnegie Mellon University, University of Memphis and Arizona State University as the most productive organizations due to the number of their publications, while Vanlehn, K. and Chen, C. –M. were the most effective and productive researchers. As a result of the analysis, it was determined that the co-authorship network structure was predominantly USA, Taiwan and United Kingdom. In addition, when the keywords mentioned together were mapped, it was seen that the words artificial intelligence, intelligent tutoring systems, machine learning, deep learning and higher education were used more frequently.\n",
            "----\n",
            "Paper 442:\n",
            "Title: A Comprehensive Artificial Intelligence Based User Intention Assessment Model from Online Reviews and Social Media\n",
            "Abstract: ABSTRACT Predictive analytics is being increasingly used to predict various aspects of applications and users. It offers vast opportunities in the growth of the modern era’s business transformation by enabling automated decision-making processes. Being able to determine the intention of users in an automated way is one of the important factors in enabling automated decision-making for applications and businesses using such applications. In this paper, we utilize and build upon the existing works, and propose a comprehensive intention assessment model that detects different possible intents of users by analyzing their text-based reviews on online forums, retail market websites, or on social media. If the information about a product or service experience is present somewhere in a review or post, our technique can accurately segregate different possible purchase intention labels (i.e., positive, negative, and unknown). Our proposed comprehensive model for intention assessment includes extensive data pre-processing, extended feature selection model, utilization of artificial intelligence (machine learning and deep learning) techniques, and customized cost and loss functions. We built a comprehensive testbed and carried out evaluations and comparisons. Our solution demonstrates high accuracy, precision, and F1 score. The proposed solution helps in mining and gaining deeper insights into behavior of consumers and market tendencies and can help in making informed decisions.\n",
            "----\n",
            "Paper 443:\n",
            "Title: Where are we? Using Scopus to map the literature at the intersection between artificial intelligence and research on crime\n",
            "Abstract: None\n",
            "----\n",
            "Paper 444:\n",
            "Title: Retrofitting Word Embeddings with the UMLS Metathesaurus for Clinical Information Extraction\n",
            "Abstract: Deep learning has surged in popularity and proven to be effective for various artificial intelligence applications including information extraction from cancer pathology reports. Since word representation is a core unit that enables deep learning algorithms to understand words and be able to perform NLP, this representation must include as much information as possible to help these algorithms achieve high classification performance. Therefore, in this work in addition to the distributional information of words in large sized corpora, we use UMLS vocabulary resources to enrich the vector space representation of words with the semantic relations between words. These resources provide many terminologies pertaining to cancer. The refined word embeddings are used with a convolutional neural (CNN) model to extract four data elements from cancer pathology reports; ICD-O-3 tumor topography codes, tumor laterality, behavior, and histological grade. We observed that using UMLS vocabulary resources to enrich word embeddings of CNN models consistently outperformed CNN models without pre-training word embeddings and even with pre-trained word embeddings on a domain specific corpus across all four tasks. The results show marginal improvement on the laterality task, but a significant improvement on the other tasks, especially for the macro-f score. Specifically, the improvements are 3%, 13%, and 15% for tumor site, histological grade, and behavior tasks, respectively. This approach is encouraging to enrich word embeddings with more clinical data resources to be used for information abstraction tasks from clinical pathology reports.\n",
            "----\n",
            "Paper 445:\n",
            "Title: Introductory Chapter: Machine Learning and Biometrics\n",
            "Abstract: We are entering the era of artificial intelligence and big data, and thus, systems are becoming more intelligent with performance even to a human level in limited applications. We also connect every part of the globe with ultrahigh-speed Internet to share information in almost real time, and innovatively make changes on the life style of people. At the core of artificial intelligence, machine learning algorithms contribute to semiautomatically or automatically develop highly intelligent systems by overcoming existing difficulties for various fields including applications on engineering, business, science, and pure art.\n",
            "----\n",
            "Paper 446:\n",
            "Title: Artificial intelligence (AI) for medical imaging to combat coronavirus disease (COVID-19): a detailed review with direction for future research\n",
            "Abstract: None\n",
            "----\n",
            "Paper 447:\n",
            "Title: Artificial intelligence in network intrusion detection\n",
            "Abstract: In past, detection of network attacks has been almost solely done by human operators. They anticipated network anomalies in front of consoles, where based on their expert knowledge applied necessary security measures. With the exponential growth of network bandwidth, this task slowly demanded substantial improvements in both speed and accuracy. One proposed way how to achieve this is the usage of artificial intelligence (AI), progressive and promising computer science branch, particularly one of its sub-fields - machine learning (ML) - where main idea is learning from data. In this paper authors will try to give a general overview of AI algorithms, with main focus on their usage for network intrusion detection.\n",
            "----\n",
            "Paper 448:\n",
            "Title: Research on the Application of Artificial Intelligence and Distributed Parallel Computing in Archives Classification\n",
            "Abstract: In the traditional mode, the classification of archives often has large workload, long time consumption and high labor cost. With the development of archives management towards informationization and paperlessness, it has become an important research topic to classify archives by using artificial intelligence classification method based on machine learning. In this paper, an archives classification method based on XGBoost and Spark distributed parallel computing is proposed. XGBoost algorithm can continuously improve the classification accuracy of small class samples during training rounds, which makes XGBoost algorithm has certain advantage in the classification of archives data with large samples and unbalanced class distribution. Spark distributed parallel computing system can greatly improve the computational efficiency of XGBoost algorithm and reduce the training time of the archives classification model. The simulation results show that our method has the advantages of faster model training and higher classification accuracy than the traditional method, and can satisfy the complex and huge archives classification task.\n",
            "----\n",
            "Paper 449:\n",
            "Title: Artificial Intelligence for Supply Chain Success in the Era of Data Analytics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 450:\n",
            "Title: Future Trends of the Healthcare Data Predictive Analytics using Soft Computing Techniques in Data Science\n",
            "Abstract: Predictive Analytics, Soft Computing (SC) and Optimization, Data Mining and Data Science are rapidly becoming some of the most-discussed, perhaps utmost glorified topics in healthcare business. Artificial Intelligence, Machine Learning, Artificial Neural Networks, Fuzzy Logic, Expert  Systems, etc., is well-studied disciplines with a long history of success in many industries. Healthcare can acquire treasured sessions from this prior achievement to startup the efficacy of predictive analytics for refining patient care, chronic disease management, hospital administration and supply chain efficiencies. The prospects that presently occurs for healthcare systems is to state what “predictive analytics” stands for to them and how can it be cast off furthermost excellently to cause further enhancements. In all industries including healthcare, prediction plays a best worthwhile role when that data is passed on as accomplishments. The inclinations to mediate the vital data is in harnessing the power of historical and real-time data with visions from forecasting those data based on the times ahead. Importantly, to best gauge efficacy and value, both the predictor and the intervention must be integrated within the same system and workflow where the trend occurs. A valuable report of the organized publicity and expectation of predictive analytics in healthcare through a blend of psychology, digital technology, and entrepreneurship is available for real-time implementation for the good of the public. Review and evaluation on these disciplines pave ways to open up new arenas envisaging the future trends of Predictive analytics, Data Mining and Science and Soft Computing (SC) in healthcare, stepping strongly into pervasive computing, ambient intelligence, ubiquitous computing and many more automated technical concepts and computing’s ahead.\n",
            "----\n",
            "Paper 451:\n",
            "Title: Human intelligence-based metaverse for co-learning of students and smart machines\n",
            "Abstract: None\n",
            "----\n",
            "Paper 452:\n",
            "Title: Feature Extraction and Machine Learning on Symbolic Music using the music21 Toolkit\n",
            "Abstract: Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the “feature” capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system’s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper’s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.\n",
            "----\n",
            "Paper 453:\n",
            "Title: The Use of Artificial Intelligence in Disaster Management - A Systematic Literature Review\n",
            "Abstract: Whenever a disaster occurs, users in social media, sensors, cameras, satellites, and the like generate vast amounts of data. Emergency responders and victims use this data for situational awareness, decision-making, and safe evacuations. However, making sense of the generated information under time-bound situations is a challenging task as the amount of data can be significant, and there is a need for intelligent systems to analyze, process, and visualize it. With recent advancements in Artificial Intelligence (AI), numerous researchers have begun exploring AI, machine learning (ML), and deep learning (DL) techniques for big data analytics in managing disasters efficiently. This paper adopts a systematic literature approach to report on the application of AI, ML, and DL in disaster management. Through a systematic review process, we identified one relevant hundred publications. After that, we analyzed all the identified papers and concluded that most of the reviewed articles used AI, ML, and DL methods on social media data, satellite data, sensor data, and historical data for classification and prediction. The most common algorithms are support vector machines (SVM), Naïve Bayes (NB), Random Forest (RF), Convolutional Neural Networks (CNN), Artificial neural networks (ANN), Natural language processing techniques (NLP), Latent Dirichlet Allocation (LDA), K-nearest neighbor (KNN), and Logistic Regression (LR).\n",
            "----\n",
            "Paper 454:\n",
            "Title: Combining Domain Knowledge & Machine Learning: Making Predictions using Boosting Techniques\n",
            "Abstract: The latest hit on technology is the information and telecommunication novelties. Internet and big data are important source of information in order to understand this vast majority of the upcoming knowledge. Websites are progressively expanding and making it available to everyone who has access. Modern economic systems are built on data or knowledge. Thus, tech companies gather huge data and exercise their powers to digitalize the information to capture and utilize the knowledge within their reach. Information management enables firms to improve customer satisfaction, increase revenue, understand customer behavior, mitigate risk assessment, making a multidisciplinary approach. Another approach is to be able to identify business strategies. Information management researches (using machine learning algorithm) can help the field to discover what is significant on a customer behavior and reduce costs to its clients. Over the recent years, information management and machine learning algorithms are getting more and more close and on topic. Since, information management and machine learning intensely concerns with domain knowledge. Artificial Intelligence allows the machines to accumulate knowledge and adapts it. Information management and machine learning are affected by several factors such as business strategies, customer behavior, effectively using knowledge. Therefore, pulling only a single topic will not be enough to capture meaningful information from a huge chunk. Machine learning, and information systems also have the potential to help organizations in financial aspects. XG-Boost is one of the algorithms under the Decision-Trees equipped with boosting techniques similar to Microsoft's Light GBM algorithm. In recent years, XG-Boost algorithm has gained huge popularity due to its easy to use, speed, and performance. The aim of this paper is to show the use of the data in order to predict house prices XG-Boost algorithm and Neural Network model. The main goal here is to estimate the house prices using domain knowledge and machine learning algorithms. RMSE criteria was preferred. This paper suggests a generic way to gather knowledge on a very specific domain. Following the achieved result, the house price was estimated by processing the domain info through the machine learning algorithm here presented.\n",
            "----\n",
            "Paper 455:\n",
            "Title: Empowering Consumer Research with Data Science\n",
            "Abstract: The consumer is ultimately the key determinant of success of an organization. The need for an in-depth and objective understanding of the consumers, therefore, in terms of what runs in their minds and hearts for the way they behave and act when they go about making complex purchase decisions cannot be over-emphasized. Borrowed from the disciplines of psychology, economics and sociology, the study of consumer behavior isan interdisciplinary and continually-evolving science since long. Other data-science related areas, such as, Big Data, Artificial Intelligence, Machine Learning, Neural Networks and Internet of Things are also being increasingly adopted to analyze and predict consumer behavior. This paper attempts to explore the significance of Big Data and Artificial Intelligence in empowering consumer research.\n",
            "----\n",
            "Paper 456:\n",
            "Title: Explainable Machine Learning for Scientific Insights and Discoveries\n",
            "Abstract: Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.\n",
            "----\n",
            "Paper 457:\n",
            "Title: A Review on the Significance of Machine Learning for Data Analysis in Big Data\n",
            "Abstract: Big data revolution is changing the lifestyle in terms of working and thinking environments through facilitating improvement in vision finding and decision-making. But, big data science's technical dilemma is that there is no knowledge that can administer and analyze large amounts of actively increasing data and pull out valuable information. As data around the world grows rapidly and its distribution with real-time processing continues, traditional tools for automated machine learning have become inadequate. However, conventional machine learning (ML) approaches have been extended to meet the needs of other applications, but with increased information or large data knowledge bases, there are significant challenges for ML algorithms for big data analysis. This paper aims to facilitate understanding the importance of ML in the analysis of large data. It contributes to understanding the implications and challenges in big data computational complexity, classification imperfection and data heterogeneity. It discusses the capability to mine value from large-scale data for decision-making and predictive analysis through data transformation and knowledge extraction. It will suggest the impact of big data on real-time data analysis and discuss the extent to which machine learning can be used to analyze large data through machine learning in big data analysis. It will also suggest the meaning and opportunity from the point of view of encouraging feature research development in the field of ML using big data.\n",
            "----\n",
            "Paper 458:\n",
            "Title: Face Liveness Detection Using Artificial Intelligence Techniques: A Systematic Literature Review and Future Directions\n",
            "Abstract: Biometrics has been evolving as an exciting yet challenging area in the last decade. Though face recognition is one of the most promising biometrics techniques, it is vulnerable to spoofing threats. Many researchers focus on face liveness detection to protect biometric authentication systems from spoofing attacks with printed photos, video replays, etc. As a result, it is critical to investigate the current research concerning face liveness detection, to address whether recent advancements can give solutions to mitigate the rising challenges. This research performed a systematic review using the PRISMA approach by exploring the most relevant electronic databases. The article selection process follows preset inclusion and exclusion criteria. The conceptual analysis examines the data retrieved from the selected papers. To the author, this is one of the foremost systematic literature reviews dedicated to face-liveness detection that evaluates existing academic material published in the last decade. The research discusses face spoofing attacks, various feature extraction strategies, and Artificial Intelligence approaches in face liveness detection. Artificial intelligence-based methods, including Machine Learning and Deep Learning algorithms used for face liveness detection, have been discussed in the research. New research areas such as Explainable Artificial Intelligence, Federated Learning, Transfer learning, and Meta-Learning in face liveness detection, are also considered. A list of datasets, evaluation metrics, challenges, and future directions are discussed. Despite the recent and substantial achievements in this field, the challenges make the research in face liveness detection fascinating.\n",
            "----\n",
            "Paper 459:\n",
            "Title: Modelling of total dissolved solids in water supply systems using regression and supervised machine learning approaches\n",
            "Abstract: None\n",
            "----\n",
            "Paper 460:\n",
            "Title: A review of Artificial Intelligence approach for credit risk assessment\n",
            "Abstract: Every day, each bank around the world has to analyze many credit applications from its customers and prospects, individuals, professionals, or companies. Banks develop their rating system based on different parameters but most of them do not take benefit of the tremendous set of Big Data available and gathered continuously. To extract valuable information, Big Data analysis (BDA) and artificial intelligence (AI) lead to interesting applications for the banking industry such as segmentation, customized service, customer relationship management, fraud detection, credit risk assessment, and in all back, middle, and front office missions. This article presents the benefit of artificial intelligence for credit risk assessment. A state of art for the actual research advance is discussed concerning this specific item. To handle this review, we first focused on the keywords to capture and analyze the available articles of experts. We limited the period from 2016 to 2021 to skim the recent advances. Researchers have explored different methods with feature selection, classification, and prediction. Algorithms of Data mining, machine learning (supervised and unsupervised), and deep learning (artificial neural networks) are very different and tackle various aspects to be explored. With these advances, banks can become smart and propose a better and quicker service while preserving themselves from losses due to credit defaulters. Support vector machine, Catboost, decision tree, and logistic regression have delivered interesting results according to the studied researches.\n",
            "----\n",
            "Paper 461:\n",
            "Title: Use of Artificial Intelligence in the Identification and Diagnosis of Frailty Syndrome in Older Adults: Scoping Review\n",
            "Abstract: Background Frailty syndrome (FS) is one of the most common noncommunicable diseases, which is associated with lower physical and mental capacities in older adults. FS diagnosis is mostly focused on biological variables; however, it is likely that this diagnosis could fail owing to the high biological variability in this syndrome. Therefore, artificial intelligence (AI) could be a potential strategy to identify and diagnose this complex and multifactorial geriatric syndrome. Objective The objective of this scoping review was to analyze the existing scientific evidence on the use of AI for the identification and diagnosis of FS in older adults, as well as to identify which model provides enhanced accuracy, sensitivity, specificity, and area under the curve (AUC). Methods A search was conducted using PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines on various databases: PubMed, Web of Science, Scopus, and Google Scholar. The search strategy followed Population/Problem, Intervention, Comparison, and Outcome (PICO) criteria with the population being older adults; intervention being AI; comparison being compared or not to other diagnostic methods; and outcome being FS with reported sensitivity, specificity, accuracy, or AUC values. The results were synthesized through information extraction and are presented in tables. Results We identified 26 studies that met the inclusion criteria, 6 of which had a data set over 2000 and 3 with data sets below 100. Machine learning was the most widely used type of AI, employed in 18 studies. Moreover, of the 26 included studies, 9 used clinical data, with clinical histories being the most frequently used data type in this category. The remaining 17 studies used nonclinical data, most frequently involving activity monitoring using an inertial sensor in clinical and nonclinical contexts. Regarding the performance of each AI model, 10 studies achieved a value of precision, sensitivity, specificity, or AUC ≥90. Conclusions The findings of this scoping review clarify the overall status of recent studies using AI to identify and diagnose FS. Moreover, the findings show that the combined use of AI using clinical data along with nonclinical information such as the kinematics of inertial sensors that monitor activities in a nonclinical context could be an appropriate tool for the identification and diagnosis of FS. Nevertheless, some possible limitations of the evidence included in the review could be small sample sizes, heterogeneity of study designs, and lack of standardization in the AI models and diagnostic criteria used across studies. Future research is needed to validate AI systems with diverse data sources for diagnosing FS. AI should be used as a decision support tool for identifying FS, with data quality and privacy addressed, and the tool should be regularly monitored for performance after being integrated in clinical practice.\n",
            "----\n",
            "Paper 462:\n",
            "Title: Machine Learning in Educational Technology\n",
            "Abstract: Machine learning is a subset of artificial intelligence (AI) that helps computers or teaching machines learn from all previous data and make intelligent decisions. The machine-learning framework entails capturing and maintaining a rich set of information and transforming it into a structured knowledge base for different uses in various fields. In the field of education, teachers can save time in their non-classroom activities by adopting machine learning. For example, teachers can use virtual assistants who work remotely from the home for their students. This kind of assistance helps to enhance students’ learning experience and can improve progression and student achievement. Machine learning fosters personalized learning in the context of disseminating education. Advances in AI are enabling teachers to gain a better understanding of how their students are progressing with learning. This enables teachers to create customized curriculum that suits the specific needs of the learners. When employed in the context of education, AI can foster intelligence moderation. It is through this platform that the analysis of data by human tutors and moderators is made possible.\n",
            "----\n",
            "Paper 463:\n",
            "Title: Artificial Intelligence Approaches\n",
            "Abstract: Artificial Intelligence (AI) has received tremendous attention from academia, industry, and the general public in recent years. The integration of geography and AI, or GeoAI, provides novel approaches for addressing a variety of problems in the natural environment and our human society. This entry briefly reviews the recent development of AI with a focus on machine learning and deep learning approaches. We discuss the integration of AI with geography and particularly geographic information science, and present a number of GeoAI applications and possible future directions.\n",
            "----\n",
            "Paper 464:\n",
            "Title: Information Extraction for Freight-Related Natural Language Queries\n",
            "Abstract: The ability to retrieve accurate information from databases without an extensive knowledge of the contents and organization of each database is extremely beneficial to the dissemination and utilization of freight data. Advances in the artificial intelligence and information sciences provide an opportunity to develop query capturing algorithms to retrieve relevant keywords from freight-related natural language queries. The challenge is correctly identifying and classifying these keywords. On their own, current natural language processing algorithms are insufficient in performing this task for freight-related queries. High performance machine learning algorithms also require an annotated corpus of named entities which currently does not exist in the freight domain. This paper proposes a hybrid named entity recognition approach which draws on the individual strengths of models to correctly identify entities. The hybrid approach resulted in a greater precision for named entity recognition of freight entities-a key requirement for accurate information retrieval from freight data sources.\n",
            "----\n",
            "Paper 465:\n",
            "Title: Implementation of Machine Learning Models for the Prevention of Kidney Diseases (CKD) or Their Derivatives\n",
            "Abstract: Chronic kidney disease (CKD) is a global health issue with a high rate of morbidity and mortality and a high rate of disease progression. Because there are no visible symptoms in the early stages of CKD, patients frequently go unnoticed. The early detection of CKD allows patients to receive timely treatment, slowing the disease's progression. Due to its rapid recognition performance and accuracy, machine learning models can effectively assist physicians in achieving this goal. We propose a machine learning methodology for the CKD diagnosis in this paper. This information was completely anonymized. As a reference, the CRISP-DM® model (Cross industry standard process for data mining) was used. The data were processed in its entirety in the cloud on the Azure platform, where the sample data was unbalanced. Then the processes for exploration and analysis were carried out. According to what we have learned, the data were balanced using the SMOTE technique. Four matching algorithms were used after the data balancing was completed successfully. Artificial intelligence (AI) (logistic regression, decision forest, neural network, and jungle of decisions). The decision forest outperformed the other machine learning models with a score of 92%, indicating that the approach used in this study provides a good baseline for solutions in the production.\n",
            "----\n",
            "Paper 466:\n",
            "Title: Convolutional Neural Network Inception-v3: A Machine Learning Approach for Leveling Short-Range Rainfall Forecast Model from Satellite Image\n",
            "Abstract: None\n",
            "----\n",
            "Paper 467:\n",
            "Title: Cloud Cognitive Services Based on Machine Learning Methods in Architecture of Modern Knowledge Management Solutions\n",
            "Abstract: None\n",
            "----\n",
            "Paper 468:\n",
            "Title: Hemp Disease Detection and Classification Using Machine Learning and Deep Learning\n",
            "Abstract: Hemp is a multipurpose plant that has industrial as well as medicinal value. The plant is easy to grow, maintain, and suitable under any climate. However, just like other plants, Hemp diseases affect plant growth and cause a significant economic loss in hemp production. With the rapid advancement of artificial intelligence and machine learning technology, researchers have started using data-driven machine learning approaches in smart agriculture and farming. Plant disease detection and classification is an application of the smart agriculture technique. This paper focuses on hemp disease detection and classification by proposing one SVM-based machine learning model and three deep learning ensemble models. The focused hemp diseases include Hemp Powdery Mildew, Hemp Leaf Spot, Hemp Bud Rot, and Hemp Nutrient Deficiency. The paper uses pre-trained deep learning ensemble models with transfer learning. It reports comparative evaluation results of the three deep learning ensemble models with an SVM-based model with manual feature extraction. The evaluation results from different models show as high as 98% accuracy with strong application potential.\n",
            "----\n",
            "Paper 469:\n",
            "Title: Artificial intelligence-based scholarship and credit pre-assessment system\n",
            "Abstract: Scholarships are given to students who are studied in university by public institutions and organizations such as Yüksek Öğrenim Kredi ve Yurtlar Kurumu by taking different criterions into account. The students who deserve to have scholarship are detected by examining information in the forms that are given to student to detect the person who needs the scholarship. This process is causing time loose and also quite exhausting. On the other hand, it becomes too difficult to make objective decisions in some situations. Using artificial intelligence and machine learning in the process of detecting students who will be given scholarship is important to evaluate in both objective and subjective ways to students who apply. The data which are used in this study are collected from the internet and the purpose of this study is to detect the students who are studying in the university and given scholarship by answering the questions, which are prepared by Kredi ve Yurtlar Kurumu, with artificial intelligence and machine learning methods objective and subjectively in a correct way.\n",
            "----\n",
            "Paper 470:\n",
            "Title: Meta-Analysis of Artificial Intelligence Works in Ubiquitous Learning Environments and Technologies\n",
            "Abstract: Ubiquitous learning (u-learning) refers to anytime and anywhere learning. U-learning has progressed to be considered a conventional teaching and learning approach in schools and is adopted to continue with the school curriculum when learners cannot attend schools for face-to-face lessons. Computer Science, namely the field of Artificial Intelligence (AI) presents tools and techniques to support the growth of u-learning and provide recommendations and insights to academic practitioners and AI researchers. Aim: The aim of this study was to conduct a meta-analysis of Artificial Intelligence works in ubiquitous learning environments and technologies to present state from the plethora of research. Method: The mining of related articles was devised according to the technique of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). The complement of included research articles was sourced from the broadly used databases, namely, Science Direct, Springer Link, Semantic Scholar, Academia, and IEEE. Results: A total of 16 scientific research publications were shortlisted for this study from 330 articles identified through database searching. Using random-effects model, the estimated pooled estimate of artificial intelligence works in ubiquitous learning environments and technologies reported was 10% (95% CI: 3%, 22%; I2 = 99.46%, P = 0.00) which indicates the presence of considerable heterogeneity. Conclusion: It can be concluded based on the experimental results from the sub group analysis that machine learning studies [18% (95% CI: 11%, 25%), I2 = 99.83%] was considerably more heterogeneous (I2 = 99.83%) than intelligent decision support systems, intelligent systems and educational data mining. However, this does not mean that intelligent decision support systems, intelligent systems and educational data mining is not efficient.\n",
            "----\n",
            "Paper 471:\n",
            "Title: An Interpretable Machine Learning Model for Accurate Prediction of Sepsis in the ICU\n",
            "Abstract: Objectives: Sepsis is among the leading causes of morbidity, mortality, and cost overruns in critically ill patients. Early intervention with antibiotics improves survival in septic patients. However, no clinically validated system exists for real-time prediction of sepsis onset. We aimed to develop and validate an Artificial Intelligence Sepsis Expert algorithm for early prediction of sepsis. Design: Observational cohort study. Setting: Academic medical center from January 2013 to December 2015. Patients: Over 31,000 admissions to the ICUs at two Emory University hospitals (development cohort), in addition to over 52,000 ICU patients from the publicly available Medical Information Mart for Intensive Care-III ICU database (validation cohort). Patients who met the Third International Consensus Definitions for Sepsis (Sepsis-3) prior to or within 4 hours of their ICU admission were excluded, resulting in roughly 27,000 and 42,000 patients within our development and validation cohorts, respectively. Interventions: None. Measurements and Main Results: High-resolution vital signs time series and electronic medical record data were extracted. A set of 65 features (variables) were calculated on hourly basis and passed to the Artificial Intelligence Sepsis Expert algorithm to predict onset of sepsis in the proceeding T hours (where T = 12, 8, 6, or 4). Artificial Intelligence Sepsis Expert was used to predict onset of sepsis in the proceeding T hours and to produce a list of the most significant contributing factors. For the 12-, 8-, 6-, and 4-hour ahead prediction of sepsis, Artificial Intelligence Sepsis Expert achieved area under the receiver operating characteristic in the range of 0.83–0.85. Performance of the Artificial Intelligence Sepsis Expert on the development and validation cohorts was indistinguishable. Conclusions: Using data available in the ICU in real-time, Artificial Intelligence Sepsis Expert can accurately predict the onset of sepsis in an ICU patient 4–12 hours prior to clinical recognition. A prospective study is necessary to determine the clinical utility of the proposed sepsis prediction model.\n",
            "----\n",
            "Paper 472:\n",
            "Title: Artificial Intuition for Automated Decision-Making\n",
            "Abstract: ABSTRACT Automated decision-making techniques play a crucial role in data science, AI, and general machine learning. However, such techniques need to balance accuracy with computational complexity, as their solution requirements are likely to need exhaustive analysis of the potentially numerous events combinations, which constitute the corresponding scenarios. Intuition is an essential tool in the identification of solutions to problems. More specifically, it can be used to identify, combine and discover knowledge in a “parallel” manner, and therefore more efficiently. As a consequence, the embedding of artificial intuition within data science is likely to provide novel ways to identify and process information. There is extensive research on this topic mainly based on qualitative approaches. However, due to the complexity of this field, limited quantitative models and implementations are available. In this article, the authors have extended the evaluation to include a real-world, multi-disciplinary area in order to provide a more comprehensive assessment. The results demonstrate the value of artificial intuition, when embedded in decision-making and information extraction models and frameworks. In fact, the output produced by the approach discussed in their article was compared with a similar task carried out by a group of experts in the field. This demonstrates comparable results further showing the potential of this framework, as well as artificial intuition as a tool for decision-making and information extraction.\n",
            "----\n",
            "Paper 473:\n",
            "Title: Artificial Intelligence in Education\n",
            "Abstract: None\n",
            "----\n",
            "Paper 474:\n",
            "Title: Combining Machine Learning, Patient Reported Outcomes and Value Based Healthcare: A Protocol for Scoping Reviews (Preprint)\n",
            "Abstract: \n",
            " BACKGROUND\n",
            " Objective measures such as vital signs and lab values only provide a partial view of a patient’s condition. Patient reported outcome measures (PROMs)1 and Patient reported experience measures (PREMs)1 are subjective reports shared by patients that can help complete this view by filling in gaps that other methods are incapable of assessing such as pain levels, patient experience, motivation, human factors, patient related outcomes and health priorities. Machine learning, the use of computer algorithms that improve automatically through experience, is a powerful tool in healthcare that often does not utilize subjective information shared by patients.2 Furthermore, earlier implementations of machine learning in medicine were developed without patient or public input and may be missing priorities and measures that matter to patients. Public and patient involvement can bring these measures together by defining end-user experience, meaning, patient priorities and implementation thus providing enriched data for machine learning and more functional PROMS and PREMs.\n",
            " Patient reported outcome measures (PROMs) are questionnaires measuring the patients’ views of their health status. Patient reported experience measures (PREMs) refer to data collected from patients on their experience within the health. These questionnaires can help understand the patient perspective to identify goals for care and evaluate the impact of care.\n",
            " Machine learning is an application of Artificial Intelligence (AI) that trains systems to automatically learn and improve from experience. In the past decade, machine learning has given us practical speech and speech to text recognition, algorithms for medical diagnosis, improvements in predictive epidemiology and public health, and prognostic treatment models. While this is a powerful tool, these algorithms are only as reliable and free from bias as the data that is used to build and train them.3\n",
            " This review of reviews looks at ways to integrate machine learning with patient reported outcomes for the development of improved public and patient partnership in research and health care.\n",
            " \n",
            " \n",
            " OBJECTIVE\n",
            " What can we learn from existing systematic reviews about the best methods for combining machine learning and patient reported outcomes?\n",
            "1. How are the public engaged as involved partners in the development of Artificial Intelligence (AI) in medicine? \n",
            "2. What examples of good practice can we identify for the integration of Patient Reported Outcome Measures (PROMs) into machine learning algorithms?\n",
            "3. How has value-based healthcare influenced the development of artificial intelligence in healthcare?\n",
            " \n",
            " \n",
            " METHODS\n",
            " Searches\n",
            " This review covers a broad range of interrelated topics where we will assess the overall data by conducting three separate scoping reviews. The first review will focus on the intersection of AI and PROMS. The second scoping review will focus on AI and public involvement. The third will focus on AI and value-based healthcare. We have chosen to do three separate scoping reviews instead of one or multiple systematic reviews in order to more efficiently identify knowledge gaps and investigate the way the research was conducted.4,5 Preliminary searches have indicated that large bodies of knowledge have been published concerning the integration of PROMs into statistical methods6,7,8, but few have indicated frameworks for public and patient involvement in the development of artificial intelligence. Search strategies for each review were developed with the team and reviewed by our information specialist (CS). Our search strategies utilize controlled terms and a range of techniques to optimise sensitivity. No language restrictions will be applied. Each review will include relevant date restrictions to further isolate informative and innovative research. \n",
            " The MEDLINE database will be used to identify initial search results. Initial search results will be reviewed to confirm there are no significant exclusions. Once the final search strategy has been identified, we will expand our search to the following information sources: Ovid MEDLINE(R), EMBASE, PsycINFO, Science Citation Index, Cochrane Library, Database of Abstracts of Reviews of Effects, PROSPERO. For search strategies see appendix-1\n",
            "Types of Study to be Included\n",
            " We will include systematic reviews and overviews published in any language. Reviews will be included if they searched a minimum two databases, appraised the included studies, provided synthesis of the data and information retrieved. All findings will be reviewed and discussed by members of the author team until consensus is reached. Once a preliminary set of eligible studies has been identified for each review based on outcome measures and broad inclusion criteria, we will progress to the next stage of evaluation. Each eligible study will be further evaluated based on more narrow inclusion criteria in order to select the most relevant and informative research for each review. \n",
            "Types of Study to be Excluded\n",
            " Upon initial screening of title and abstract, we will exclude articles meeting any of the following criteria:\n",
            "• Papers not dealing with any form of or related forms of Artificial Intelligence\n",
            "• Papers in which no relevant outcomes are reported\n",
            "• Papers describing protocols for future studies\n",
            "• Papers dealing with animal models\n",
            "• Papers in which the full paper is not accessible\n",
            "\n",
            "Condition or Domain being studied\n",
            " We are investigating three domains. In the first scoping review, we will study examples of how the general public has been involved in Artificial Intelligence development where the outcomes include aspects of the trial and the experiences and perspectives of the public, participants, or researchers. The second review will focus on machine learning algorithms that have utilized Patient Reported Outcomes Measures (PROMs) to improve their performance on a healthcare related task. This will include any research study that is investigating the use of PROMs to improve diagnostic or treatment approaches. The third review will investigate artificial intelligence research that has focused on value-based care. Studies that have utilized artificial intelligence to investigate, evaluate, or design value-based care systems will be included. \n",
            "\n",
            "Public and Patient Involvement\n",
            "Patients and members of the public will be involved in the review and will be trained to screen titles and abstracts as well as risk or bias and quality assessment. They will be named as authors at that time if they have met the standards for authorship. Funding constraints and COVID-19 restrictions prevented us from involving them more actively in protocol building.\n",
            " \n",
            "Dissemination\n",
            "The research will be disseminated via social media and presented by the authors at conferences and convenings. The lessons learned and the findings will be used to teach our teens and young adult learners at the Stanford Anesthesia Summer Institute.\n",
            " \n",
            " \n",
            " RESULTS\n",
            " Main Outcomes\n",
            "The following outcomes will be considered:\n",
            "• Public involvement in artificial intelligence research planning, conduct, or management\n",
            "• Public Involvement in research analysis\n",
            "• Research recruitment, enrolment, and retention\n",
            "• Factors that affect cooperation and participation\n",
            "• Patient reported outcome measures (PROMs)\n",
            "• Patient reported experience measures (PREMs)\n",
            "• Ethics related to the inclusion of patient reported information in AI\n",
            "• Factors relating to participant interaction with AI\n",
            "• Barriers to acquiring PROMs and PREMs for use in AI research\n",
            "• Cost-effectiveness outcomes relating to inclusion of PROMs and PREMs in AI research\n",
            "\n",
            "Measures of effect\n",
            " Quantitative, qualitative, and mixed methods studies will be included in our reviews. If sufficient quantitative studies relating to the inclusion of PROMs in AI warrant a meta-analysis, we will perform it and calculate a weighted effect across the studies using a random effects model. After utilizing a random effects model, it may still be desirable to identify sources of heterogeneity. If this is the case, we will utilize a subgroup analysis approach to investigate the reasons for heterogeneity. \n",
            "Data Extraction\n",
            " The flow of information through the different stages of our review will be guided by the PRISMA flow chart.9 First, we will identify records through database searching and other sources as described in appendix 1. Relevant results from each database and source of information will then be downloaded into Zotero, a management software for managing research materials. Results will then be uploaded into Covidence, an online tool for screening references, for screening and analysis. After uploading into Covidence, we will remove duplicate records. Titles and abstract of potentially relevant articles will then be screened independently by at least two reviewers against the relevant inclusion criteria. Discrepancies will be resolved through discussion with the entire group when necessary. Individuals recruited from the Cochrane Task Exchange, Stanford Medicine X and Stanford Science Technology and Medicine Summer Internships will co-produce the study design and will be active in screening, data extraction, analysis, prioritizing what to report, and editing and authoring tasks. \n",
            " After excluding initial search results that do not meet our inclusion criteria, we will begin to review the full text of included records. Full text review will be conducted by at least two authors with an additional author reserved to mediate areas where agreement is uncertain. Authors will then come to agreement through discussion. The full paper review will result in the final set of included records. The authors will provide tables to show the characteristics of the included studies as seen in Table 1, and an additional table to show author, year, exclusion reason for excluded full studies as seen in Table 2. \n",
            "Table 1: Table of Included Studies\n",
            "Study Name Intervention Enablers Barriers Outcomes Results\n",
            "Example Stud\n",
            "----\n",
            "Paper 475:\n",
            "Title: Artificial Neural Network Classification of High Dimensional Data with Novel Optimization Approach of Dimension Reduction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 476:\n",
            "Title: In Search of the Future Technologies: Fusion of Machine Learning, Fog and Edge Computing in the Internet of Things\n",
            "Abstract: None\n",
            "----\n",
            "Paper 477:\n",
            "Title: Remodeling Hospitality Industry through Artificial Intelligence\n",
            "Abstract: Remodeling the hospitality industry through artificial intelligence (AI) – that uses big data analytics and complex machine learning – is a concept that will help the industry to leapfrog to the next level. The notion put forward in this paper is to develop a framework to utilize machine learning analyses the multi-channel user data for efficient decision making to enrich the customer experience and to provide the maximum revenue to the vendor. We propose strategies to infer customer behaviors by capturing otherwise salient information – e.g. through the various digital footprints. Feeding such analytics to a suitably trained collection of machine learning algorithms called the “Digital Operations Manager” helps to automate complex decision making, removing human error and bias. The proposed AI system, if appropriately deployed within a hospitality industry environment, is thought to bring out a significant gain in the user choice and experience as well as efficiency in resource management and revenue optimization.\n",
            "----\n",
            "Paper 478:\n",
            "Title: The Myth of Artificial Intelligence\n",
            "Abstract: THE MYTH OF ARTIFICIAL INTELLIGENCE: Why Computers Can't Think the Way We Do by Erik J. Larson. Cambridge, MA: Belknap Press, 2021. 312 pages. Hardcover; $29.95. ISBN: 9780674983519. *The Myth of Artificial Intelligence (AI) offers a technical and philosophical introduction to AI with an emphasis on AI's limitations. Larson, a computer scientist and tech entrepreneur, keeps his central claim modest: true general AI is neither inevitable nor imminent, and if it is possible, it will require fundamentally new approaches. It is an easy read, combining references to fiction, history, and science. It lays out a bird's eye view of the origins and ideas behind current AI methods, focusing on general AI, a category of AI that would need to learn and engage with a wide variety of problems. *Separated into three parts, The Myth of AI begins with the history and algorithmic logic of AI, largely through the lens of the Turing test. Larson argues that we are not near the singularity (superintelligent computers able to create ever more intelligent machines) and that, in fact, the basic premise of the singularity is flawed. *The second part discusses inference. AI falls short of human intelligence because it can work with hard rules, but cannot make the guesses necessary to formulate new ones or handle uncertain rules. In attempts at the Turing test, AI can throw data at the problem but will always lack understanding. Achieving the understanding necessary for true intelligence will require an approach fundamentally different from recent advances made in AI, which are only effective for narrow AI (a category of AI for solving specialized problems) and not general AI. *The final, and relatively brief, part examines AI in science. According to Larson's assessment, new scientific research relies heavily on newly available computation power and big data in order to use narrow AI to its full extent. Larson claims that this approach will hinder development of new theories. He also claims that this leads to treating scientists as if they were computers as well, which causes overvaluing the system of science above people. He criticizes \"swarm science,\" which he describes as a large group of scientists approaching one problem with a variety of projects, emphasizing this collaboration over the individuals. Instead, he claims, we need our culture to continue to emphasize individual discovery and intelligence, as it is the key to innovation. *Through the discussions of the history, philosophy, and logic of AI in the first two parts of the book, Larson disentangles the hype of AI from what is actually possible with current technology. Even as he sheds light on the gap between the singularity prediction and what machine learning is truly capable of, he emphasizes the significance of the myth. \"The myth is an emotional lighthouse by which we navigate the AI topic\" (p. 76). The stories we tell through predictions and science fiction define AI in the public eye and set the goals for AI research. *Our underlying philosophy matters as much as the current state of AI research, when we consider the social role of AI and what we predict for our future. In the development of AI, we must define intelligence and explore what it means to be human. While this is not a book with overtly religious claims, it does acknowledge the spiritual claims inherent in discussions of personhood. It also frames technoscience as replacing philosophy and religion and as the oversimplified understanding of humanity and the precursor to expectations of the singularity. *Beyond the stated goal of disenchanting the reader of the inevitability of AI, the book highlights the significance of stories to both society and science and emphasizes the importance of understanding for both humans and AI. We need to understand not only the technical aspects of the technology we build but also the philosophy that defines our goals. *While I found the first two sections of the book to be an engaging and accurate discussion of the tension between the science and hopes of AI, I had concerns about the warnings of \"swarm science\" in the third. Larson is placing a strong emphasis on individual genius in science; however, science has never been a truly independent endeavor. Many times in history, from evolution to DNA, multiple teams of scientists independently made the same discoveries at nearly the same time, based on previously published work. Though these discoveries were not inevitable, they built upon other research and relied on collaboration at least as much as individual genius. Larson focuses on a particular neuroscience project and makes some valid criticisms, but then he generalizes his observations to all of science in ways that I do not believe to be accurate. His argument that all of science is moving away from theory toward shallow observations is not as obvious as he claims, nor is it supported by the evidence offered in the book. *As a counterexample, the research that resulted in the COVID-19 vaccine could be considered \"swarm science\" and was effective. Large amounts of funding were very suddenly directed to many scientists for one goal: understand and prevent the coronavirus. Due to both new funding and established research, we developed and approved multiple vaccines in one year. I was not convinced of several of Larson's generalizations in this third section. Tension between celebrating collaboration and individual genius will persist. However, it appears that there is more collaboration in science today. This is likely due to a variety of reasons, including a scientific community connected by the internet and more contributors receiving appropriate credit for their work. *The Myth of AI is a broad view of AI that should prove valuable and comprehensible to readers with or without a technical background. The first two sections offer a clear explanation and history of AI, and the third offers food for thought on how the process of science has been shaped by advances in AI and computer technology. The first sections would be a good introduction to someone not familiar with AI or looking to think about the philosophy of AI and I¬†would recommend the book for these sections. *While the book avoids religious claims, the philosophical discussions of what it means to \"understand\" and the level of trust we place in AI are essential questions for Christians working in technology-related disciplines. The Myth of AI presents a jumping-off point for much deeper reflection about using AI responsibly and what it means to be human. *Reviewed by Elizabeth Koning, graduate student in the Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801.\n",
            "----\n",
            "Paper 479:\n",
            "Title: Introduction to the special issue on Artificial Intelligence for Justice (AI4J)\n",
            "Abstract: None\n",
            "----\n",
            "Paper 480:\n",
            "Title: The hybrid of BERT and deep learning models for Indonesian sentiment analysis\n",
            "Abstract: Artificial intelligence (AI) is one example of how data science innovation has advanced quickly in recent years and has greatly improved human existence. Neural networks, which are a type of machine learning model, are a fundamental component of deep learning in the field of AI. Deep learning models can carry out feature extraction and classification tasks in a single design because of their numerous neural network layers. Modern machine learning algorithms have been shown to perform worse than this model on tasks including text classification, audio recognition, imaginary, and pattern recognition. Deep learning models have outperformed AI-based methods in sentiment analysis and other text categorization tasks. Text data can originate from a number of places, including social media. Sentiment analysis is the computational examination of textual expressions of ideas and feelings. This study employs the convolutional neural network (CNN), long-short term memory (LSTM), CNN-LSTM, and LSTM-CNN models in a deep learning framework using bidirectional encoder representations from transformers (BERT) data representation to assess the performance of machine learning. The implementation of the model utilises YouTube discussion data pertaining to political films associated with the Indonesian presidential election of 2024. Confusion metrics, including as accuracy, precision, and recall, are then used to analyse the model’s performance.\n",
            "----\n",
            "Paper 481:\n",
            "Title: Evolving EEG signal processing techniques in the age of artificial intelligence\n",
            "Abstract: 1 CAS Key Laboratory of Mental Health, Institute of Psychology, Chinese Academy of Sciences, Beijing 100101, China 2 Department of Psychology, University of Chinese Academy of Sciences, Beijing 100049, China 3 School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen 518000, Guangdong, China  Electroencephalogram (EEG) is an important technique for measuring population‐level electrical activity arising from the human brain. Due to its exquisite temporal sensitivity and implementation simplicity, EEG has been widely applied to dynamically evaluate the function of the brain. Being responded to a specific sensory, cognitive, or motor event, the changes of EEG signals give rise to evoked potentials (EPs) and event‐related potentials (ERPs), which are highly associated with different brain functions, e.g., perception, emotion, and cognition. These advances make the EEG technique popularly used in various basic and clinical applications. To make full use of the EEG technique, signal processing and machine learning methods are crucial in the extraction of information for better understan‐ ding the cerebral functioning. Particularly, in this age of artificial intelligence (AI), rapidly developed AI methods, such as convolutional neural networks and recurrent neural networks, have been applied to EEG signals and have achieved promising performance in many real applications. As a consequence, the field of EEG signal processing has undergone significant growth in the last few years, and the scope and range of practical applications of EEG, such as brain–computer interface (BCI), are steadily increasing. For this reason, the special issue aims to provide a collection of papers discussing the conceptual and methodological innovations as well as practical applications of the EEG techniques. This special session has included seven review papers contributed by experts in this interdisciplinary field, and all authors have worked in the fields of EEG processing methods and applications for many years. First of all, Li [1] shared his insightful and constructive thoughts on EEG signal analysis and classification. Specifically, he focused on several important and emerging topics in EEG processing, such as brain connectivity, tensor decomposition, multi‐modality, deep learning, big data, and naturalistic experiments. These topics, particularly those AI‐related topics, are both crucial and promising for the future advancement of EEG signal analysis and classification. Next, this special issue presented several papers concerning the applications of EEG in psychology, emotion recognition, and BCI. One important and conventional application field of EEG is psychology, in which EEG has been extensively used to decode the psychological  Address correspondence to Li Hu, huli@psych.ac.cn; and Zhiguo Zhang, zgzhang@szu.edu.cn\n",
            "----\n",
            "Paper 482:\n",
            "Title: Cloud AI: A comparison of specimen image data extraction processes\n",
            "Abstract: The Natural History Museum (NHM) of London has embarked on an ambitious programme to digitise the 80 million specimens in its collection, releasing them through the NHM data portal and the global biodiversity research community. As part of the digitisation process, data is transcribed from specimen labels to capture the vital taxonomic and collection event data. Accurate human transcription is slow and the NHM, like many institutions, has been exploring machine learning (ML) for automated specimen analysis and label data capture. This process requires many different models, chained in series: semantic segmentation to identify specimen and label regions of interest; optical character recognition to identify text on labels; natural language processing to extract entities from the text.\n",
            " As part of SYNTHESYS+, the NHM has been building the Specimen Data Refinery (SDR) (Smith et al. 2019) - a workflow engine for chaining ML models, each performing one atomic task in the data extraction process. The SDR is now in public beta, and we present evaluation metrics from our initial testing. Alongside the SDR project, the NHM has been exploring cloud-based artificial intelligence tools for specimen digitisation, using Google and Amazon technologies. We present an analysis of these different approaches, comparing the results from third-party AI services with models developed specifically for the biodiversity and natural history collection domains. With large corporates providing comparatively low-cost access to AI compute resources and models transferrable to many specimen image digitisation tasks, is developing bespoke solutions still required?\n",
            "----\n",
            "Paper 483:\n",
            "Title: Artificial Neural Networks for Space and Safety-Critical Applications: Reliability Issues and Potential Solutions\n",
            "Abstract: Machine learning is among the greatest advancements in computer science and engineering and is today used to classify or detect objects, a key feature in autonomous vehicles. Since neural networks are heavily used in safety-critical applications, such as automotive and aerospace, their reliability must be paramount. However, the reliability evaluation of neural network systems is extremely challenging due to the complexity of the software, which is composed of hundreds of layers, and the underlying hardware, typically a parallel device or an embedded accelerator. This article reviews fundamental concepts of artificial intelligence, deep neural networks, and parallel computing device reliability. Then, the reliability studies that consider the radiation effects in the hardware, their propagation through the computing architecture, and their final impact on the software output are summarized. A detailed survey of the available strategies to measure the sensitivity of neural network frameworks and observe fault propagation is given, together with a summary of the data obtained so far. Finally, a discussion on how to use the experimental evaluation to design effective and efficient hardening solutions for artificial neural networks is provided. The available hardening solutions are critically reviewed, highlighting their benefits and drawbacks.\n",
            "----\n",
            "Paper 484:\n",
            "Title: Data Science in the Business Environment: Skills Analytics for Curriculum Development\n",
            "Abstract: None\n",
            "----\n",
            "Paper 485:\n",
            "Title: Key Management and Governance Challenges when Executing Data Science / Analytics Projects\n",
            "Abstract: Big data, data science and analytics have become increasingly important strategic assets because they can help organizations \n",
            "make better decisions, discover new insights, competitively differentiate, and they enable the embedding of intelligence into \n",
            "automated processes so organizations can efficiently respond at the speed of business. Effective organizational management \n",
            "and governance of data science practices are necessary in order to mitigate risks associated with analytics deployment. For \n",
            "example, organizations need to capture and manage critical meta-information detailing modeling and environmental \n",
            "assumptions underlying the analytics solutions, they also need to establish policies and a culture designed to ensure \n",
            "adherence to the highest ethical standards of data management and predictive model deployment. At a higher level, \n",
            "unleashing machine learning algorithms may require safeguards and risk mitigation monitoring to address these types of \n",
            "socio-technical challenges. This panel will foster a debate with respect to what are the most important concerns or potential \n",
            "issues that an organization should focus on while executing a data science/analytics project. Via a debate, the panel, along \n",
            "with the audience, will explore the field of data science and predictive analytics, and what are the key project risks that need \n",
            "to be mitigated.\n",
            "----\n",
            "Paper 486:\n",
            "Title: Machine Learning as a Service for DiSSCo’s Digital  Specimen Architecture\n",
            "Abstract: International mass digitization efforts through infrastructures like the European Distributed System of Scientific Collections (DiSSCo), the US resource for Digitization of Biodiversity Collections (iDigBio), the National Specimen Information Infrastructure (NSII) of China, and Australia’s digitization of National Research Collections (NRCA Digital) make geo- and biodiversity specimen data freely, fully and directly accessible. \n",
            " Complementary, overarching infrastructure initiatives like the European Open Science Cloud (EOSC) were established to enable mutual integration, interoperability and reusability of multidisciplinary data streams including biodiversity, Earth system and life sciences (De Smedt et al. 2020). \n",
            " Natural Science Collections (NSC) are of particular importance for such multidisciplinary and internationally linked infrastructures, since they provide hard scientific evidence by allowing direct traceability of derived data (e.g., images, sequences, measurements) to physical specimens and material samples in NSC. \n",
            " To open up the large amounts of trait and habitat data and to link these data to digital resources like sequence databases (e.g., ENA), taxonomic infrastructures (e.g., GBIF) or environmental repositories (e.g., PANGAEA), proper annotation of specimen data with rich (meta)data early in the digitization process is required, next to bridging technologies to facilitate the reuse of these data. \n",
            " This was addressed in recent studies (Younis et al. 2018, Younis et al. 2020), where we employed computational image processing and artificial intelligence technologies (Deep Learning) for the classification and extraction of features like organs and morphological traits from digitized collection data (with a focus on herbarium sheets).\n",
            " However, such applications of artificial intelligence are rarely—this applies both for (sub-symbolic) machine learning and (symbolic) ontology-based annotations—integrated in the workflows of NSC’s management systems, which are the essential repositories for the aforementioned integration of data streams.\n",
            " This was the motivation for the development of a Deep Learning-based trait extraction and coherent Digital Specimen (DS) annotation service providing “Machine learning as a Service” (MLaaS) with a special focus on interoperability with the core services of DiSSCo, notably the DS Repository (nsidr.org) and the Specimen Data Refinery (Walton et al. 2020), as well as reusability within the data fabric of EOSC. \n",
            " Taking up the use case to detect and classify regions of interest (ROI) on herbarium scans, we demonstrate a MLaaS prototype for DiSSCo involving the digital object framework, Cordra, for the management of DS as well as instant annotation of digital objects with extracted trait features (and ROIs) based on the DS specification openDS (Islam et al. 2020).\n",
            " Source code available at: https://github.com/jgrieb/plant-detection-service\n",
            "----\n",
            "Paper 487:\n",
            "Title: 2022 Review of Data-Driven Plasma Science\n",
            "Abstract: Data-driven science and technology offer transformative tools and methods to science. This review article highlights the latest development and progress in the interdisciplinary field of data-driven plasma science (DDPS), i.e., plasma science whose progress is driven strongly by data and data analyses. Plasma is considered to be the most ubiquitous form of observable matter in the universe. Data associated with plasmas can, therefore, cover extremely large spatial and temporal scales, and often provide essential information for other scientific disciplines. Thanks to the latest technological developments, plasma experiments, observations, and computation now produce a large amount of data that can no longer be analyzed or interpreted manually. This trend now necessitates a highly sophisticated use of high-performance computers for data analyses, making artificial intelligence and machine learning vital components of DDPS. This article contains seven primary sections, in addition to the introduction and summary. Following an overview of fundamental data-driven science, five other sections cover widely studied topics of plasma science and technologies, i.e., basic plasma physics and laboratory experiments, magnetic confinement fusion, inertial confinement fusion and high-energy-density physics, space and astronomical plasmas, and plasma technologies for industrial and other applications. The final Section before the summary discusses plasma-related databases that could significantly contribute to DDPS. Each primary Section starts with a brief introduction to the topic, discusses the state-of-the-art developments in the use of data and/or data-scientific approaches, and presents the summary and outlook. Despite the recent impressive signs of progress, the DDPS is still in its infancy. This article attempts to offer a broad perspective on the development of this field and identify where further innovations are required.\n",
            "----\n",
            "Paper 488:\n",
            "Title: DASentimental: Detecting depression, anxiety and stress in texts via emotional recall, cognitive networks and machine learning\n",
            "Abstract: Most current affect scales and sentiment analysis on written text focus on quantifying valence/sentiment, the primary dimension of emotion. Distinguishing broader, more complex negative emotions of similar valence is key to evaluating mental health. We propose a semi-supervised machine learning model, DASentimental, to extract depression, anxiety, and stress from written text. We trained DASentimental to identify how N = 200 sequences of recalled emotional words correlate with recallers’ depression, anxiety, and stress from the Depression Anxiety Stress Scale (DASS-21). Using cognitive network science, we modeled every recall list as a bag-of-words (BOW) vector and as a walk over a network representation of semantic memory—in this case, free associations. This weights BOW entries according to their centrality (degree) in semantic memory and informs recalls using semantic network distances, thus embedding recalls in a cognitive representation. This embedding translated into state-of-the-art, cross-validated predictions for depression (R = 0.7), anxiety (R = 0.44), and stress (R = 0.52), equivalent to previous results employing additional human data. Powered by a multilayer perceptron neural network, DASentimental opens the door to probing the semantic organizations of emotional distress. We found that semantic distances between recalls (i.e., walk coverage), was key for estimating depression levels but redundant for anxiety and stress levels. Semantic distances from “fear” boosted anxiety predictions but were redundant when the “sad–happy” dyad was considered. We applied DASentimental to a clinical dataset of 142 suicide notes and found that the predicted depression and anxiety levels (high/low) corresponded to differences in valence and arousal as expected from a circumplex model of affect. We discuss key directions for future research enabled by artificial intelligence detecting stress, anxiety, and depression in texts.\n",
            "----\n",
            "Paper 489:\n",
            "Title: AI-Based Feature Extraction Approaches for Dual Modalities of Autism Spectrum Disorder Neuroimages\n",
            "Abstract: : High-dimensional data, lower detection accuracy, susceptibility to manual errors, and the requirement of clinical experts are some drawbacks of conventional classification models available for Autism Spectrum Disorder (ASD) detection. To address these challenges and explore the affiliated information from advanced imaging modalities such as Magnetic Resonance Imaging (MRI) in structural MRI (sMRI) and resting state-functional MRI (rs-fMRI), the study applied an Artificial Intelligence (AI) approach. In this context, AI is used to automate the feature extraction process, which is crucial in the interpretation of medical images for diagnosis. The work aims to apply AI-based techniques to extract the features and identify the impact of each feature in the Autism diagnosis. The morphometric features were extracted using sMRI images and rs-fMRI scans were employed to fetch functional connectivity features. Surface-based, region-based, and seed-based analyses are performed for the whole brain, followed by feature selection techniques such as Recursive Feature Elimination (RFE) with correlation, Principal Component Analysis (PCA), Independent Component Analysis (ICA), and graph theory are implemented to extract and distinguish features. The effectiveness of the extracted features was measured as classification accuracy. Support Vector Machine (SVM) with RFE is the best classification model, with 88.67% accuracy for high-dimensional data. SVM is a supervised learning model that outperforms other classification models due to its capability to handle high-dimensional data with a larger feature set. Medical imaging modalities provide detailed insights and visual differences related to various cognitive conditions that must be recognized accurately for efficient diagnosis. The study presented an empirical analysis of various Feature extraction approaches and the significance of the extracted features in high-dimensional data scenarios for Autism classification\n",
            "----\n",
            "Paper 490:\n",
            "Title: A Review of the Application of Machine Learning and Data Mining Approaches in Continuum Materials Mechanics\n",
            "Abstract: Machine learning tools represent key enablers for empowering material scientists and engineers to accelerate the development of novel materials, processes and techniques. One of the aims of using such approaches in the field of materials science is to achieve high-throughput identification and quantification of essential features along the process-structure-property-performance chain. In this contribution, machine learning and statistical learning approaches are reviewed in terms of their successful application to specific problems in the field of continuum materials mechanics. They are categorized with respect to their type of task designated to be either descriptive, predictive or prescriptive; thus to ultimately achieve identification, prediction or even optimization of essential characteristics. The respective choice of the most appropriate machine learning approach highly depends on the specific use-case, type of material, kind of data involved, spatial and temporal scales, formats, and desired knowledge gain as well as affordable computational costs. Different examples are reviewed involving case-by-case dependent application of different types of artificial neural networks and other data-driven approaches such as support vector machines, decision trees and random forests as well as Bayesian learning, and model order reduction procedures such as principal component analysis, among others. These techniques are applied to accelerate the identification of material parameters or salient features for materials characterization, to support rapid design and optimization of novel materials or manufacturing methods, to improve and correct complex measurement devices, or to better understand and predict fatigue behavior, among other examples. Besides experimentally obtained datasets, numerous studies draw required information from simulation-based data mining. Altogether, it is shown that experiment- and simulation-based data mining in combination with machine leaning tools provide exceptional opportunities to enable highly reliant identification of fundamental interrelations within materials for characterization and optimization in a scale-bridging manner. Potentials of further utilizing applied machine learning in materials science and empowering significant acceleration of knowledge output are pointed out.\n",
            "----\n",
            "Paper 491:\n",
            "Title: Special issue on artificial intelligence 2.0\n",
            "Abstract: With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn\n",
            "----\n",
            "Paper 492:\n",
            "Title: From Business Intelligence to Artificial Intelligence\n",
            "Abstract: With today’s growing information and overloading of its volume based on tremendous size of data growing to the level of big data, Business Intelligence (BI) is not enough to handle any day-to-day business operation of any enterprises. It is becoming tremendously difficult to analyze the huge amounts of data that contain the information and makes it very strenuous and inconvenient to introduce an appropriate methodology of decision-making fast enough to the point that it can be, considered as real time, a methodology that we used to call it BI. The demand for real time processing information and related data both structured and unstructured is on the rise and consequently makes it harder and harder to implement correct decision making at enterprise level that was driven by BI, in order to keep the organization robust and resilient against either man made threats or natural disasters. With smart malware in modern computation world and necessity for Internet-of-Things (IoT), we are in need of a better intelligence system that today we know it as Artificial Intelligence (AI). AI with its two other subset that are called Machine Learning (ML) and Deep Learning (DL), we have a better chance against any cyber-attack and makes our day-to-day operation within our organization a more robust one as well makes our decision making as stakeholder more trust worthy one as well\n",
            "----\n",
            "Paper 493:\n",
            "Title: Machine and deep learning methods for radiomics.\n",
            "Abstract: Radiomics is an emerging area in quantitative image analysis that aims to relate large-scale extracted imaging information to clinical and biological endpoints. The development of quantitative imaging methods along with machine learning has enabled the opportunity to move data science research towards translation for more personalized cancer treatments. Accumulating evidence has indeed demonstrated that noninvasive advanced imaging analytics, that is, radiomics, can reveal key components of tumor phenotype for multiple three-dimensional lesions at multiple time points over and beyond the course of treatment. These developments in the use of CT, PET, US, and MR imaging could augment patient stratification and prognostication buttressing emerging targeted therapeutic approaches. In recent years, deep learning architectures have demonstrated their tremendous potential for image segmentation, reconstruction, recognition, and classification. Many powerful open-source and commercial platforms are currently available to embark in new research areas of radiomics. Quantitative imaging research, however, is complex and key statistical principles should be followed to realize its full potential. The field of radiomics, in particular, requires a renewed focus on optimal study design/reporting practices and standardization of image acquisition, feature calculation, and rigorous statistical analysis for the field to move forward. In this article, the role of machine and deep learning as a major computational vehicle for advanced model building of radiomics-based signatures or classifiers, and diverse clinical applications, working principles, research opportunities, and available computational platforms for radiomics will be reviewed with examples drawn primarily from oncology. We also address issues related to common applications in medical physics, such as standardization, feature extraction, model building, and validation.\n",
            "----\n",
            "Paper 494:\n",
            "Title: Artificial Intelligence in Medicine and Cardiac Imaging: Harnessing Big Data and Advanced Computing to Provide Personalized Medical Diagnosis and Treatment\n",
            "Abstract: None\n",
            "----\n",
            "Paper 495:\n",
            "Title: Artificial Intelligence, Real Radiology.\n",
            "Abstract: Welcome to this inaugural issue of Radiology: Artificial Intelligence. Our journal’s mission is to publish highquality scientific work that advances our understanding of artificial intelligence (AI) in radiology. AI has become a topic of great interest—especially the application of machine learning techniques to medical images—but AI itself is not new. The term artificial intelligence was proposed in 1956 to describe efforts to understand, simulate, and improve upon human qualities such as reasoning, learning, solving problems, understanding verbal and written language, processing visual information, and playing games like chess and poker. What is new is a resurgence of interest in AI, particularly in the use of machine learning to recognize patterns in images. And, curiously, it is game playing that has opened this new frontier—but not the games of chess, checkers, or Go. Rather, think Xbox and PlayStation. Video games require a rapidly changing three-dimensional scene to be transformed into two-dimensional images shown in real time. The need to compute images efficiently spurred the development of highly parallelized graphics processing units. These specialized processors, in turn, have powered software for increasingly complex and sophisticated “deep” artificial neural network models. Whereas neural networks developed 10 years ago typically had three or four layers, today’s deep networks comprise hundreds of layers (1). Deep learning models have engendered both great excitement and a great deal of hyperbole. After all, if AI systems can pick out pictures of cats on the web, then surely such systems are ready to replace radiologists, right? Well, perhaps not, at least not right now. There is much work to be done to build and validate systems that can detect and characterize the thousands of imaging findings and their associated diseases that can be seen across a panoply of radiology studies. And that brings us to the quote from Shakespeare. Anyone can claim to build an AI system, but that doesn’t mean that the system will do their bidding as imagined. Our journal is here to assure that the science and applications of AI in radiology are built on thoughtful, innovative, and well-validated research. What sorts of topics will this journal publish? We will bring you the same high caliber of research that is found in RSNA’s flagship scientific journal, Radiology, but focused here on AI, machine learning, and data science in radiology. In particular, we seek to publish first-rate work that provides rigorous evaluation of AI’s applications to clinical problems in radiology. We invite manuscripts that show the impact of AI to extract information, diagnose and manage disease in patients, streamline radiology workflow, or improve health care outcomes. We’re interested in image segmentation, image reconstruction, automated detection of abnormalities, diagnostic reasoning, natural language processing, clinical workflow analysis, radiomics, and radiogenomics. We also invite manuscripts that demonstrate novel applications of AI in radiology or highlight innovative AI methodologies. Developers of publicly available sets of radiologic images, image annotations, radiology reports, or algorithms can present their work as a Data Resources report. AI and radiology do not exist in isolation: they are part of broad endeavors to advance knowledge and improve health. As such, this journal will feature articles on the ethical, legal, social, and economic implications of AI in radiology. AI is and must be a human—and humane—activity (2). We must engage in this work with an eye to how these technologies will help us care for our patients more effectively and humanely. Our goal is not to replace, but rather to extend our human abilities to provide medical care— and to improve the lives of those we are privileged to serve. All RSNA members receive access to this online bimonthly journal. We invite all readers (RSNA members or not) to sign up for our Editor’s Blog, The Vasty Deep (https://pubs.rsna.org/page/ai/blog) and to follow us on Twitter (@Radiology_AI). These social media platforms will augment the journal and offer innovative online features. Again, welcome!\n",
            "----\n",
            "Paper 496:\n",
            "Title: How real is the impact of artificial intelligence? The business information survey 2018\n",
            "Abstract: World economy experiencing fragile growth, but the uncertainty around Brexit means a more cautious and pessimistic UK outlook. Artificial intelligence (AI) and the data economy is one of the UK Government’s four grand challenges as set out in its industrial strategy whitepaper (HM Government, 2017). Information teams see their core deliverables as content management, training on information-related databases, and research and analysis. Information teams in banking and financial organisations indicate they feel they impact business decisions in their organizations more directly than information teams in other sectors. AI is rapidly impacting the information profession: AI projects described by information teams range from pilot to fully-implemented. Data quality, and trust and confidence in data, are concerns for information professionals; survey participants are clear that the ability to ensure data is of highest quality and integrity is a critical skill for information professionals. Information teams are frequently responsible for training and education on GDPR (General Data Protection Regulation) and other data governance legislation. They do assist with some aspects of implementation and application, but Legal, Risk and Compliance teams have the absolute responsibility in the organization. Information professionals foresee that new technical and AI solutions will mainly require the enhancement ofcurrent skills, rather than the acquisitionofnew ones. Information professionals see research analysis developing further to include providing implications and deductions; and this will require more confidence. As new technological solutions, such as AI and machine learning are introduced then there will be an increased importance of digital and information literacy. Introduction\n",
            "----\n",
            "Paper 497:\n",
            "Title: Eye on Developments in Artificial Intelligence and Children's Rights: Artificial Intelligence in Education (AIEd), EdTech, Surveillance, and Harmful Content\n",
            "Abstract: Like corporations, governments around the world have adopted strategies for becoming leaders in the development and use of Artificial Intelligence, fostering environments congenial to AI innovators. However, in most cases, neither corporations nor policymakers have sufficiently addressed how the rights of children fit into their AI strategies or products. \n",
            " \n",
            "The role of artificial intelligence in children’s lives—from how children play, to how they are educated, to how they consume information and learn about the world—is expected to increase exponentially over the coming years. Thus, it’s imperative that stakeholders evaluate the risks and assess opportunities to use artificial intelligence to maximize children’s wellbeing in a thoughtful and systematic manner. \n",
            " \n",
            "This paper discusses AI and children's rights law and ethics in the context of 1) social media platforms such as YouTube; 2) smart toys; and 3) AI education and EdTech applications, including during COVID-19. The Hello Barbie, Cloud Pets, and Cayla smart toys case studies are analyzed, as well as the ElsaGate social media hacks, and education's new Intelligent Tutoring Systems and surveillance apps. \n",
            " \n",
            "Though AI has valuable benefits for children, it presents some particular challenges around important issues including child safety, privacy, data privacy, device security, surveillance, and consent. \n",
            " \n",
            "Ethics by design will continue to gain strength as a consideration throughout the development and use of AI systems, including systems designed for children’s and youth’s use. With respect to children, the Children's Rights by Design of AI systems (“CRbD”) standard2 is useful to employ against data-driven business models from AIEd that could exploit or otherwise harm children. \n",
            " \n",
            "COVID-19 has greatly exacerbated pre-existing EdTech risks. Overnight, education was forced to depend on technology, rather than simply utilize it to enable new teaching methods. During the spring of 2020 alone, schools in 192 countries were closed.UNESCO estimates support this assertion, stating that 91% of the world’s student population were out of school in April of 2020.2This has vaulted AI and EdTech from an incoming phenomenon to a virtual necessity as one of the core mediums for the delivery of education. \n",
            " \n",
            "Surveillance of children is another use of AI that is booming due to advance machine learning and deep learning techniques.Although some degree of surveillance advances security, surveillance poses risks to children. Surveillance also creates privacy, safety, bias, and security risks and, especially in education contexts, limit children’s ability and willingness to take risks and otherwise express themselves. \n",
            " \n",
            "This article maps the potential positive and negative uses of AI on children’s lives, in hopes to contribute to the conversation on developing a child rights-based framework for artificial intelligence that delineates rights and corresponding duties for governments, educators, developers, corporations, parents, and children around the world. \n",
            " \n",
            "Of growing significance alongside AI technological issues are those of ethics. AI is ideological.The concern about AI is not that it won't deliver on the promise held forth by its advocates but, rather, that it will, but without due consideration of ethical implications. There are assumptions embedded in the algorithms that will shape how education is realized, and if students do not fit that conceptual model, they will find themselves outside of the area where a human could apply human wisdom to alter or intervene an unjust outcome. Perhaps one of the greatest contributions of AI will be to make us understand how important human wisdom truly is in education and everywhere else. \n",
            " \n",
            "The article concludes with some recommendations for corporations, parents, governments, and educators on Responsible AI development for children.\n",
            "----\n",
            "Paper 498:\n",
            "Title: A Survey on Artificial Intelligence in Chinese Sign Language Recognition\n",
            "Abstract: None\n",
            "----\n",
            "Paper 499:\n",
            "Title: A machine learning route between band mapping and band structure\n",
            "Abstract: None\n",
            "----\n",
            "Paper 500:\n",
            "Title: Classification of Mammography Images by Machine Learning Techniques\n",
            "Abstract: Early diagnosis and accurate treatment is crucial in increasing the survival rate of diseases that can result in death, such as breast cancer. Therefore, there is a greater need for artificial intelligence systems that will help doctors make decisions in health care, especially in fatal diseases. Because these systems are not affected by human nature factors such as distraction, stress etc. so that they can distinguish small and important issues that could be overlooked, especially in the scan results of the patient. The aim of this study is to predict whether a mass can be identified in breast and whether the mass found in the breast is benign or malignant with the help of machine learning which is a sub-study area of artificial intelligence. In this study, the images in the mini-MIAS database are used. Firstly, unwanted areas were eliminated. Then Gauss, Average, Median and Wiener filters were applied to reduce noise and smoothing the images and an algorithm based on Contrast-Limited Adaptive Histogram Equalization (CLAHE) was applied to make suspicious areas more visible. New data sets were created by using HOG (Histogram of Oriented Gradients), LBP (Local Binary Pattern), GLCM (Gray Level Co-occurrence Matrices) for feature extraction and correlation (COR) for feature selection. Selected features were classified in three different categories (normal, benign, malignant) and two different categories (normal, abnormal) using. Different machine learning algorithms (C5.0 (normal and boosted), Naive Bayes, CART and Random Forest) were applied to the data sets and the performances were compared. According to the research findings, to decide whether there was a breast mass, the highest accuracy value was calculated by applying Median and Wiener filters together, equating histogram with CLAHE and using the GLCM feature extraction method on the data set and the accuracy was found 0.657 with Naive Bayes algorithm. When trying to find out whether the mass found in the breast is benign or malignant, Median was applied together with Weiner Filter, equating histogram with CLAHE and HOG feature extraction method was used, and the accuracy was calculated as 0,660 with Random Forest algorithm.\n",
            "----\n",
            "Rate limit exceeded. Retrying in 40 seconds... (Attempt 1/5)\n",
            "Rate limit exceeded. Retrying in 80 seconds... (Attempt 2/5)\n",
            "Paper 501:\n",
            "Title: Towards Using Universal Big Data in Artificial Intelligence Research and Development to Gain Meaningful Insights and Automation Systems\n",
            "Abstract: The increasing number of human activities using information and communication technology (ICT) generates a tremendous amount of data. It brings the opportunity to use universal big data for further computation that may deliver new insights and automation system. The technology that enables this work is artificial intelligence (AI). As known, the utilization of AI technology is becoming more and more pervasive in our daily life. Furthermore, discussion related to AI has played an essential role in an organization's decision-making process. Thus, this paper discusses the utilization of AI technology that penetrates end-to-end various aspects of human activities, such as in education, health, business, social life and so forth. The end-to-end process begins with data collection covering various types of data (text, picture, audio, video, animation) and various methods (survey, observation, interview, experiment). Then, it continues with the pre-process data cleansing and process to determine data features, and seeks the relationship within them, using machine learning process with appropriate algorithms. The results may then be used for applications portfolio in order to improve organization strategies and programs. The organization's performance can be visualized from time to time for continuous quality improvement. The end-to-end cycles of processes continue, from data collection, data pre-processing and processing, performance computation monitoring (identification, classification, prediction, and prescription), improving strategy and program, and implementing in the real-life activities until generating more behavioral data, finding pattern and design the systems. As a whole, this end-to-end cycle leads us to an AI automation system, with the power to generate meaningful insights suited to solve current problems, predicting trending issues, and understanding phenomena.\n",
            "----\n",
            "Paper 502:\n",
            "Title: Learning Hidden Markov Model Structure for Information Extraction\n",
            "Abstract: Statistical machine learning techniques, while well proven in fields such as speech recognition, are just beginning to be applied to the information extraction domain. We explore the use of hidden Markov models for information extraction tasks, specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data. We show that a manually-constructed model that contains multiple states per extraction field outperforms a model with one state per field, and discuss strategies for learning the model structure automatically from data. We also demonstrate that the use of distantly-labeled data to set model parameters provides a significant improvement in extraction accuracy. Our models are applied to the task of extracting important fields from the headers of computer science research papers, and achieve an extraction accuracy of 92.9%.\n",
            "----\n",
            "Paper 503:\n",
            "Title: Use Cases of Pervasive Artificial Intelligence for Smart Cities Challenges\n",
            "Abstract: Software engineering has been historically topdown. From a fully specified problem, a software engineer needs to detail each step of the resolution to get a solution. The resulting program will be functionally adequate as long as its execution environment complies with the original specifications. With their large amount of data, their ever changing multi-level dynamics, smart cities are too complex for a topdown approach. They prompt the need for a paradigm shift in computer science. Programs should be able to self-adapt on the fly, to handle unspecified events,, to efficiently deal with tremendous amount of data. To this end, bottom-up approach should become the norm. Machine learning is a first step,, distributed computing helps. Multi-Agent Systems (MAS) can combine machine learning, distributed computing, may be easily designed with a bottom-up approach. This paper explores how MASs can answer challenges at various levels of smart cities, from sensors networks to ambient intelligence.\n",
            "----\n",
            "Paper 504:\n",
            "Title: Facing small and biased data dilemma in drug discovery with enhanced federated learning approaches\n",
            "Abstract: None\n",
            "----\n",
            "Paper 505:\n",
            "Title: AIDR: artificial intelligence for disaster response\n",
            "Abstract: We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., \"needs\", \"damage\", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.\n",
            "----\n",
            "Paper 506:\n",
            "Title: Machine Learning and Knowledge Discovery in Databases\n",
            "Abstract: None\n",
            "----\n",
            "Paper 507:\n",
            "Title: Symbiosis between Humans and Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 508:\n",
            "Title: Machine Learning Approach for Solar Wind Categorization\n",
            "Abstract: Solar wind classification is conducive to understanding the ongoing physical processes at the Sun and in solar wind evolution in interplanetary space, and, furthermore, it is helpful for early warning of space weather events. With rapid developments in the field of artificial intelligence, machine learning approaches are increasingly being used for pattern recognition. In this study, an approach from machine learning perspectives is developed to automatically classify the solar wind at 1 AU into four types: coronal‐hole‐origin plasma, streamer‐belt‐origin plasma, sector‐reversal‐region plasma, and ejecta. By exhaustive enumeration, an eight‐dimensional scheme (BT, NP, TP, VP, Nαp, Texp/TP, Sp, and Mf) is found to perform the best among 8,191 combinations of 13 solar wind parameters. Ten popular supervised machine learning models, namely, k‐nearest neighbors (KNN), Support Vector Machines with linear and radial basic function kernels, Decision Tree, Random Forest, Adaptive Boosting, Neural Network, Gaussian Naive Bayes, Quadratic Discriminant Analysis, and eXtreme Gradient Boosting, are applied to the labeled solar wind data sets. Among them, KNN classifier obtains the highest overall classification accuracy, 92.8%. Although the accuracy can be improved by 1.5% when O7+/O6+ information is additionally considered, our scheme without composition measurements is still good enough for solar wind classification. In addition, two application examples indicate that solar wind classification is helpful for the risk evaluation of predicted magnetic storms and surface charging of geosynchronous spacecraft.\n",
            "----\n",
            "Paper 509:\n",
            "Title: Applying Data Science for shop-Floor Performance Prediction\n",
            "Abstract: Against the backdrop of ubiquitous computing, companies from various industries are building up everincreasing amounts of business process data. Seeking to salvage these hidden “data treasures,” the need for analytical information systems is ever-growing to guide corporate decision-making. However, information systems research is still very much focused on static, explanatory modeling provided by business intelligence suites instead of embracing the opportunities offered by predictive analytics. Describing insights from a real-world manufacturing scenario, we seek to enhance the understanding of predictive modeling. In particular, we highlight that simply dumping data into “smart” algorithms is not a silver bullet. Rather, successful analytics projects require constant refinement and consolidation. To this end, we provide guidelines and best practices for modeling, feature engineering and interpretation leveraging tools from business information systems as well as machine learning.\n",
            "----\n",
            "Paper 510:\n",
            "Title: Artificial Intelligence in Medicine\n",
            "Abstract: None\n",
            "----\n",
            "Paper 511:\n",
            "Title: Machine Lifelong Learning: Challenges and Benefits for Artificial General Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 512:\n",
            "Title: Precision immunoprofiling by image analysis and artificial intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 513:\n",
            "Title: Bridging the gap between human knowledge and machine learning\n",
            "Abstract: Nowadays, great amount of data is being created by several sources from academic, scientific, business and industrial activities. Such data intrinsically contains meaningful information allowing for developing techniques, and have scientific validity to explore the information thereof. In this connection, the aim of artificial intelligence (AI) is getting new knowledge to make decisions properly. AI has taken an important place in scientific and technology development communities, and recently develops computer-based processing devices for modern machines. Under the premise, the premise that the feedback provided by human reasoning -which is holistic, flexible and parallel- may enhance the data analysis, the need for the integration of natural and artificial intelligence has emerged. Such an integration makes the process of knowledge discovery more effective, providing the ability to easily find hidden trends and patterns belonging to the database predictive model. As well, allowing for new observations and considerations from beforehand known data by using both data analysis methods and knowledge and skills from human reasoning. In this work, we review main basics and recent works on artificial and natural intelligence integration in order to introduce users and researchers on this emergent field. As well, key aspects to conceptually compare them are provided.\n",
            "----\n",
            "Paper 514:\n",
            "Title: A QFT Approach to Data Streaming in Natural and Artificial Neural Networks\n",
            "Abstract: : In the actual panorama of machine learning (ML) algorithms, the issue of the real-time information extraction/classiﬁcation/manipulation/analysis of data streams (DS) is acquiring an ever-growing relevance. They arrive generally at high speed and always require an unsupervised real-time analysis for individuating long-range and higher order correlations among data that are continuously changing over time (phase transitions). This emphasizes the inﬁnitary character of the issue, i.e., the continuous change of the signifying number of degrees of freedom characterizing the statistical representation function, challenging the classical ML algorithms, both in their classical and quantum versions, as far as all are based on the (stochastic) search for the global minimum of some cost/energy function. The physical analogue must be studied in the realm of quantum ﬁeld theory (QFT) for dissipative systems as biological and neural systems, which are able to map between different phases of quantum ﬁelds, using the formalism of the Bogoliubov transform (BT). By applying the BT in a reversed way, on the system-thermal bath energetically balanced states, it is possible to deﬁne the powerful computational tool of the “doubling of the degrees of freedom” (DDF), making the choice of the signifying ﬁnite number of the degrees of freedom dynamic and then automatic, so to suggest a different class of unsupervised ML algorithms for solving the DS issue.\n",
            "----\n",
            "Paper 515:\n",
            "Title: Applications of contemporary artificial intelligence technology in forensic odontology as primary forensic identifier: A scoping review\n",
            "Abstract: Background Forensic odontology may require a visual or clinical method during identification. Sometimes it may require forensic experts to refer to the existing technique to identify individuals, for example, by using the atlas to estimate the dental age. However, the existing technology can be a complicated procedure for a large-scale incident requiring a more significant number of forensic identifications, particularly during mass disasters. This has driven many experts to perform automation in their current practice to improve efficiency. Objective This article aims to evaluate current artificial intelligence applications and discuss their performance concerning the algorithm architecture used in forensic odontology. Methods This study summarizes the findings of 28 research papers published between 2010 and June 2022 using the Arksey and O'Malley framework, updated by the Joanna Briggs Institute Framework for Scoping Reviews methodology, highlighting the research trend of artificial intelligence technology in forensic odontology. In addition, a literature search was conducted on Web of Science (WoS), Scopus, Google Scholar, and PubMed, and the results were evaluated based on their content and significance. Results The potential application of artificial intelligence technology in forensic odontology can be categorized into four: (1) human bite marks, (2) sex determination, (3) age estimation, and (4) dental comparison. This powerful tool can solve humanity's problems by giving an adequate number of datasets, the appropriate implementation of algorithm architecture, and the proper assignment of hyperparameters that enable the model to perform the prediction at a very high level of performance. Conclusion The reviewed articles demonstrate that machine learning techniques are reliable for studies involving continuous features such as morphometric parameters. However, machine learning models do not strictly require large training datasets to produce promising results. In contrast, deep learning enables the processing of unstructured data, such as medical images, which require large volumes of data. Occasionally, transfer learning was used to overcome the limitation of data. In the meantime, this method's capacity to automatically learn task-specific feature representations has made it a significant success in forensic odontology.\n",
            "----\n",
            "Paper 516:\n",
            "Title: Implementation of Machine Learning and Data Mining to Improve Cybersecurity and Limit Vulnerabilities to Cyber Attacks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 517:\n",
            "Title: Pattern Recognition and Machine Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 518:\n",
            "Title: Emerging role of machine learning in light-matter interaction\n",
            "Abstract: None\n",
            "----\n",
            "Paper 519:\n",
            "Title: On the genealogy of machine learning datasets: A critical history of ImageNet\n",
            "Abstract: In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet’s creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research.\n",
            "----\n",
            "Paper 520:\n",
            "Title: ACS Central Science Virtual Issue on Machine Learning\n",
            "Abstract: Howwas your morning? Perhaps you woke up, did a little online shopping while brewing your coffee, posted some pictures on social media over breakfast, glanced over the world news, drove to work, checked your email, picked up your mail, and opened up your latest issue of ACS Central Science. Pretty unremarkable, right? Maybe, but in the few hours that you have been awake you have most likely interacted with numerous instances of machine learning algorithms ticking away just below the surface of our everyday lives. The term “machine learning” may be defined as algorithms that allow computers to learn to perform tasks, identify relationships, and discern patterns without the need for humans to provide the underlying instructions. Conventional algorithms operate by sequentially executing a preprogrammed set of rules to achieve a particular outcome. Machine learning algorithms, by contrast, are instead provided with a set of examples by the user and train themselves to learn the rules f rom the data. This powerful idea dates back to at least the 1950s, but has only been fully realized in recent years with the advent of sufficiently large digital data sets over which to perform trainingfor example, Google photo albums, Amazon shopping lists, Netflix viewing historiesand sufficiently powerful computer hardware and algorithms to perform the trainingtypically powerful graphics cards developed for the computer game industry that can be hijacked to conduct machine learning. This paradigm has revolutionized multiple domains of science and technology, with different variants of machine learning dominating, and in some cases enabling, multifarious applications such as retail recommendation engines, facial detection and recognition, language translation, autonomous and assisted driving, spam filtering, and character recognition. The success of these algorithms may be largely attributed to their enormous flexibility and power to extract patterns, correlations, and structure from data. These features can be nonintuitive and complicated functions that are difficult for humans to parse, or exist as weak signals that are only discernible from large, high-dimensional data sets that defy conventional analysis techniques. There remains a fundamental difference between artificial and human intelligenceno machine has yet exhibited generic human cognition, and for now, the Turing Test remains intact1but machine performance in certain specific tasks is unequivocally superhuman. A prominent example is provided by Google’s Go-playing computer program AlphaGo Zero. This program was provided only with the rules of the ancient board game and learned to play by playing games against itself in a form of reinforcement learning. After just 3 days of training, AlphaGo Zero roundly defeated the best previous best algorithm (AlphaGo Lee) that had itself beaten the 18-time (human) world champion Lee Sedol 100 games to 0. Remarkably, AlphaGo Zero employed previously unknown strategies of play that had never been discovered by human players over the 2500 year history of the game.\n",
            "----\n",
            "Paper 521:\n",
            "Title: Application of Artificial Intelligence in Community-Based Primary Health Care: Systematic Scoping Review and Critical Appraisal\n",
            "Abstract: Background Research on the integration of artificial intelligence (AI) into community-based primary health care (CBPHC) has highlighted several advantages and disadvantages in practice regarding, for example, facilitating diagnosis and disease management, as well as doubts concerning the unintended harmful effects of this integration. However, there is a lack of evidence about a comprehensive knowledge synthesis that could shed light on AI systems tested or implemented in CBPHC. Objective We intended to identify and evaluate published studies that have tested or implemented AI in CBPHC settings. Methods We conducted a systematic scoping review informed by an earlier study and the Joanna Briggs Institute (JBI) scoping review framework and reported the findings according to PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analysis-Scoping Reviews) reporting guidelines. An information specialist performed a comprehensive search from the date of inception until February 2020, in seven bibliographic databases: Cochrane Library, MEDLINE, EMBASE, Web of Science, Cumulative Index to Nursing and Allied Health Literature (CINAHL), ScienceDirect, and IEEE Xplore. The selected studies considered all populations who provide and receive care in CBPHC settings, AI interventions that had been implemented, tested, or both, and assessed outcomes related to patients, health care providers, or CBPHC systems. Risk of bias was assessed using the Prediction Model Risk of Bias Assessment Tool (PROBAST). Two authors independently screened the titles and abstracts of the identified records, read the selected full texts, and extracted data from the included studies using a validated extraction form. Disagreements were resolved by consensus, and if this was not possible, the opinion of a third reviewer was sought. A third reviewer also validated all the extracted data. Results We retrieved 22,113 documents. After the removal of duplicates, 16,870 documents were screened, and 90 peer-reviewed publications met our inclusion criteria. Machine learning (ML) (41/90, 45%), natural language processing (NLP) (24/90, 27%), and expert systems (17/90, 19%) were the most commonly studied AI interventions. These were primarily implemented for diagnosis, detection, or surveillance purposes. Neural networks (ie, convolutional neural networks and abductive networks) demonstrated the highest accuracy, considering the given database for the given clinical task. The risk of bias in diagnosis or prognosis studies was the lowest in the participant category (4/49, 4%) and the highest in the outcome category (22/49, 45%). Conclusions We observed variabilities in reporting the participants, types of AI methods, analyses, and outcomes, and highlighted the large gap in the effective development and implementation of AI in CBPHC. Further studies are needed to efficiently guide the development and implementation of AI interventions in CBPHC settings.\n",
            "----\n",
            "Paper 522:\n",
            "Title: Deep-learning seismology\n",
            "Abstract: Seismic waves from earthquakes and other sources are used to infer the structure and properties of Earth’s interior. The availability of large-scale seismic datasets and the suitability of deep-learning techniques for seismic data processing have pushed deep learning to the forefront of fundamental, long-standing research investigations in seismology. However, some aspects of applying deep learning to seismology are likely to prove instructive for the geosciences, and perhaps other research areas more broadly. Deep learning is a powerful approach, but there are subtleties and nuances in its application. We present a systematic overview of trends, challenges, and opportunities in applications of deep-learning methods in seismology. Description Large-scale learning The large amount and availability of datasets in seismology creates a great opportunity to apply machine learning and artificial intelligence to data processing. Mousavi and Beroza provide a comprehensive review of the deep-learning techniques being applied to seismic datasets, covering approaches, limitations, and opportunities. The trends in data processing and analysis can be instructive for geoscience and other research areas more broadly. —BG The ways in which deep learning can help process and analyze large seismological datasets are reviewed. BACKGROUND Seismology is the study of seismic waves to understand their origin—most obviously, sudden fault slip in earthquakes, but also explosions, volcanic eruptions, glaciers, landslides, ocean waves, vehicular traffic, aircraft, trains, wind, air guns, and thunderstorms, for example. Seismology uses those same waves to infer the structure and properties of planetary interiors. Because sources can generate waves at any time, seismic ground motion is recorded continuously, at typical sampling rates of 100 points per second, for three components of motion, and on arrays that can include thousands of sensors. Although seismology is clearly a data-rich science, it often is a data-driven science as well, with new phenomena and unexpected behavior discovered with regularity. And for at least some tasks, the careful and painstaking work of seismic analysts over decades and around the world has also made seismology a data label–rich science. This facet makes it fertile ground for deep learning, which has entered almost every subfield of seismology and outperforms classical approaches, often dramatically, for many seismological tasks. ADVANCES Seismic wave identification and onset-time, first-break determination for seismic P and S waves within continuous seismic data are foundational to seismology and are particularly well suited to deep learning because of the availability of massive, labeled datasets. It has received particularly close attention, and that has led, for example, to the development of deep learning–based earthquake catalogs that can feature more than an order of magnitude more events than are present in conventional catalogs. Deep learning has shown the ability to outperform classical approaches for other important seismological tasks as well, including the discrimination of earthquakes from explosions and other sources, separation of seismic signals from background noise, seismic image processing and interpretation, and Earth model inversion. OUTLOOK The development of increasingly cost-effective sensors and emerging ground-motion sensing technologies, such as fiber optic cable and accelerometers in smart devices, portend a continuing acceleration of seismological data volumes, so that deep learning is likely to become essential to seismology’s future. Deep learning’s nonlinear mapping ability, sequential data modeling, automatic feature extraction, dimensionality reduction, and reparameterization are all advantageous for processing high-dimensional seismic data, particularly because those data are noisy and, from the point of view of mathematical inference, incomplete. Deep learning for scientific discovery and direct extraction of insight into seismological processes is clearly just getting started. Aspects of seismology pose interesting additional challenges for deep learning. Many of the most important problems in earthquake seismology—such as earthquake forecasting, ground motion prediction, and rapid earthquake alerting—concern large and damaging earthquakes that are (fortunately) rare. That rarity poses a fundamental challenge for the data-hungry methods of deep learning: How can we train reliable models, and how do we validate them well enough to rely on them when data are scarce and opportunities to test models are infrequent? Further, how can we operationalize deep-learning techniques in such a situation, when the mechanisms by which they make predictions from data may not be easily explained, and the consequences of incorrect models are high? Incorporating domain knowledge through physics-based and explainable deep learning and setting up standard benchmarking and evaluation protocols will help ensure progress, as is the nascent emergence of a seismological data science ecosystem. More generally, a combination of data science literacy for geoscientists as well as recruiting data science expertise will help to ensure that deep-learning seismology reaches its full potential. Deep-learning processing of seismic data and incorporation of domain knowledge can lead to new capabilities and new insights across seismology. A. MASTIN/SCIENCE, TOP RIGHT: CARA HARWOOD/UNIVERSITY OF CALIFORNIA-DAVIS/CC BY-NC-SA 3.0 MIDDLE RIGHT: JOHAN SWANEPOEL/SCIENCE SOURCE\n",
            "----\n",
            "Paper 523:\n",
            "Title: Imagination Machines: A New Challenge for Artificial Intelligence\n",
            "Abstract: \n",
            " \n",
            " The aim of this paper is to propose a new overarching challenge for AI: the design of imagination machines. Imagination has been defined as the capacity to mentally transcend time, place, and/or circumstance. Much of the success of AI currently comes from a revolution in data science, specifically the use of deep learning neural networks to extract structure from data. This paper argues for the development of a new field called imagination science, which extends data science beyond its current realm of learning probability distributions from samples. Numerous examples are given in the paper to illustrate that human achievements in the arts, literature, poetry, and science may lie beyond the realm of data science, because they require abilities that go beyond finding correlations: for example, generating samples from a novel probability distribution different from the one given during training; causal reasoning to uncover interpretable explanations; or analogical reasoning to generalize to novel situations (e.g., imagination in art, representing alien life in a distant galaxy, understanding a story about talking animals, or inventing representations to model the large-scale structure of the universe). We describe the key challenges in automating imagination, discuss connections between ongoing research and imagination, and outline why automation of imagination provides a powerful launching pad for transforming AI.\n",
            " \n",
            "\n",
            "----\n",
            "Paper 524:\n",
            "Title: Data Science from the Lab to the Field to the Enterprise\n",
            "Abstract: DARPA has been investing in data science and building open source tools for applications ranging from counter threat finance, through radar operations and cancer research, to anti-human trafficking. This presentation will cover previous work at DARPA, experience building real-world applications for defense and law enforcement to analyze data, and the future of computer science as an enabler for content discovery, information extraction, relevance determination, and information visualization. The talk will be a mix of background, detailed examples, and software demonstration. It will cover the importance of anchoring in applications, minimization of design-to-testing time, development with users-in-the-loop, error tolerance of machine learning, design for diverse user populations, and the necessity of open source software integration. It will conclude by covering a few next directions for special projects at Microsoft.\n",
            "----\n",
            "Paper 525:\n",
            "Title: Artificial Intelligence in the Healthcare Sector\n",
            "Abstract: To an extent as never before in the history of medicine, computers are supporting human input, decision making and provision of data. In today’s healthcare sector and medical profession, AI, algorithms, robotics and big data are used to derive inferences for monitoring large-scale medical trends, detecting and measuring individual risks and chances based on data-driven estimations. A knowledge-intensive industry like the healthcare profession highly depends on data and analytics to improve therapies and practices. In recent years, there has been tremendous growth in the range of medical information collected, including clinical, genetic, behavioral and environmental data. Every day, healthcare professionals, biomedical researchers and patients produce vast amounts of data from an array of devices. These include electronic health records (EHRs), genome sequencing machines, high-resolution medical imaging, smartphone applications and ubiquitous sensing, as well as Internet of Things (IoT) devices that monitor patient health (OECD 2015). Through machine learning algorithms and unprecedented data storage and computational power, AI technologies have most advanced abilities to gain information, process it and give a well-defined output to the end-user. Daily monitoring thereby aids to create big data to recognize behavioral patterns’ relation to health status in order to create predictions with highest mathematical precision based on big data capturing large-scale samples. AI thereby enlightens to analyze the relation between prevention and treatment and patient outcomes in all stages of diagnosis, treatment, drug development and monitoring, personalized medicine, patient control and care. Advanced hospitals are looking into AI solutions to support and perform operational initiatives that increase precision and cost effectiveness. Robotics have been used for disabled and patient care assistance. Medical decision making has been supported through predictive analytics and general healthcare management technology. Network connectivity allows access to affordable healthcare around the globe in a cost-effective way.\n",
            "----\n",
            "Paper 526:\n",
            "Title: Latest Advancements in Artificial Intelligence-Enabled Technologies in Treating Type 1 Diabetes\n",
            "Abstract: Type 1 diabetes (T1D) is an autoimmune chronic disease characterized by absolute insulin deficiency, which causes abnormal blood glucose regulation. Currently, there are 1.25 million Americans diagnosed with T1D with approximately 64 000 new cases of T1D being diagnosed every year.1,2 There is no cure for T1D and patients with T1D rely on daily insulin administration. So, self-management of T1D is crucial to prevent or delay major complications. However, only a very small proportion of T1D patients have met the target range set by the American Diabetes Association.3 Recent years have witnessed an explosion of artificial intelligence (AI)-enabled and data-empowered innovations in treating T1D patients. For instance, closedloop systems (ie, “artificial pancreas”) and insulin pumps are reported to effectively improve T1D management as well as patient outcomes. To summarize the latest research in AI-enabled T1D treatment, we conducted a literature review with PubMed and Google Scholar to retrieve publications in peer-reviewed healthcare journals from January 2019 to May 2020 (see Table 1). Overall, continuous improvements in safety, effectiveness, efficiency, and patient satisfaction using closed-loop systems to control blood glucose level in a broader T1D population (especially pediatric patients) were reported. The newest evidence was derived from various types of studies ranging from well-designed multicenter randomized clinical trial with six-month follow-up on closedloop system to comparative study between real-time and intermittent-scanning continuous glucose monitoring (CGM) and to cost-effectiveness study on a hybrid closed-loop system compared with a continuous subcutaneous insulin infusion system.4-6 Notably, machine learning techniques applied in CGMs were reported to lead to significant reductions in postprandial hypoglycemia, decreased rates of closed-loop exits and alerts, improved glycemic control without detectable changes in psychosocial outcomes, and accurate prediction and prevention of nocturnal hypoglycemia. Also, the autonomous diagnostic AI system demonstrated superior sensitivity and specificity for diagnosing referable diabetic retinopathy and macular edema. Additionally, higher glycated hemoglobin level and workload required to use hybrid closed loop were suggested to be the significant factors associated with discontinuation. In a nutshell, there have been remarkable progress in AI-enabled technology in treating T1D reported in literature since 2019. Yet, practical challenges and opportunities exist. Whether the observed benefits will sustain for prolonged use in real-world settings and how AI technologies will protect patient data privacy and security are crucial questions to address. During the past few years, personalized data, machine learning (in particular, deep learning and convolutional neural network techniques), and realtime adaptive algorithms combined with wearable devices have improved prevention and early detection, clinical decision support, deterioration risk prediction, and patient self-management of T1D. Future research endeavors and technological innovations should meet patients’ needs such as mitigating user’s workload and further enhancing outcomes, optimizing data protection, and realizing patientcentered care. Unquestionably, AI, Big Data, interdisciplinary research, and global collaboration will play an increasingly important role in achieving precision medicine for T1D patients. 949940 DSTXXX10.1177/1932296820949940Journal of Diabetes Science and TechnologyQian et al letter2020\n",
            "----\n",
            "Paper 527:\n",
            "Title: Gene Expression Pattern Recognition Algorithm Based on Deep Learning\n",
            "Abstract: With the rapid development of bioinformatics, the analysis of gene expression data has become crucial for understanding life processes, disease mechanisms, and drug development. For binary classification tasks, the performance of most traditional machine learning models is highly dependent on the quality of features, which often requires researchers to have a deep professional background, and artificially constructed feature vectors may lose the original information in protein sequences. This study aims to use deep learning models to extract high-order features from gene expression data, in order to solve the problem of gene expression pattern recognition. Firstly, by analyzing the limitations of traditional biological methods in gene recognition, a new approach combining recurrent neural networks (RNNs) and Transformer models is proposed. RNN can effectively capture temporal dependencies in sequence data, while Transformer, through its multi head self-attention mechanism, can more effectively process long sequence data and capture complex dependencies in gene expression sequences. Through experiments on multiple publicly available gene expression datasets, it has been demonstrated that the Transformer model outperforms the RNN model, demonstrating its superiority in feature extraction. Then, in response to the problem of low efficiency in processing long sequences with the Transformer model, this study further proposes the CNN Transformer model. The model first uses one-dimensional convolutional layers to extract local features from gene expression data, and then uses Transformer layers to capture long-range dependencies between these features. The experimental results show that the CNN Transformer model exhibits higher balanced accuracy on multiple independent test sets, demonstrating the effectiveness of the model structure. Finally, this article discusses the potential application value of this method in fields such as personalized medicine and early disease diagnosis, providing direction for future research. In summary, this study not only provides a new solution for identifying gene expression patterns, but also opens up new avenues for a deeper understanding of fundamental issues in life sciences.\n",
            "----\n",
            "Paper 528:\n",
            "Title: Towards federated learning: An overview of methods and applications\n",
            "Abstract: Federated learning (FL) is a collaborative, decentralized privacy‐preserving method to attach the challenges of storing data and data privacy. Artificial intelligence, machine learning, smart devices, and deep learning have strongly marked the last years. Two challenges arose in data science as a result. First, the regulation protected the data by creating the General Data Protection Regulation, in which organizations are not allowed to keep or transfer data without the owner's authorization. Another challenge is the large volume of data generated in the era of big data, and keeping that data in one only server becomes increasingly tricky. Therefore, the data is allocated into different locations or generated by devices, creating the need to build models or perform calculations without transferring data to a single location. The new term FL emerged as a sub‐area of machine learning that aims to solve the challenge of making distributed models with privacy considerations. This survey starts by describing relevant concepts, definitions, and methods, followed by an in‐depth investigation of federated model evaluation. Finally, we discuss three promising applications for further research: anomaly detection, distributed data streams, and graph representation.\n",
            "----\n",
            "Paper 529:\n",
            "Title: Machine Learning Paradigms: Advances in Data Analytics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 530:\n",
            "Title: Elements for a History of Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 531:\n",
            "Title: Translating Artificial Intelligence Into Clinical Care.\n",
            "Abstract: Artificial intelligence has become a frequent topic in the news cycle, with reports of breakthroughs in speech recognition, computer vision, and textual understanding that have made their way into a bevy of products and services that are used every day. In contrast, clinical care has yet to reach the much lower bar of automating health care information transactions in the form of electronic health records. Medical leaders in the 1960s and 1970s were already speculating about the opportunities to bring automated inference methods to patient care,1 but the methods and data had not yet reached the critical mass needed to achieve those goals. The intellectual roots of “deep learning,” which power the commodity and consumer implementations of presentday artificial intelligence, were planted even earlier in the 1940s and 1950s with the development of “artificial neural network” algorithms.2,3 These algorithms, as their name suggests, are very loosely based on the way in which the brain’s web of neurons adaptively becomes rewired in response to external stimuli to perform learning and pattern recognition. Even though these methods have had many success stories over the past 70 years, their performance and adoption in medicine in the past 5 years has seen a quantum leap. The catalyzing event occurred in 2012 when a team of researchers from the University of Toronto reduced the error rate in half on a well-known computer vision challenge using a deep learning algorithm.4 This work rapidly accelerated research and development in deep learning and propelled the field forward at a staggering pace. With the increased availability of digital clinical data, it remains to be seen how these deep learning models might be applied to the medical domain. In this issue of JAMA, Gulshan and colleagues5 present findings from a study evaluating the use of deep learning for detection of diabetic retinopathy and macular edema. To build their model, the authors collected 128 175 annotated images from the EyePACs database. Each image was rated by 3 to 7 clinicians for referable diabetic retinopathy, diabetic macular edema, and overall image quality. Each rater was selected from a panel of 54 board-certified ophthalmologists and senior ophthalmology residents. Using this data set, the algorithm learned to predict the consensus grade of the raters along each clinical attribute: referable diabetic retinopathy, diabetic macular edema, and image quality. To validate their algorithm, the authors assessed its performance on 2 separate and nonoverlapping data sets consisting of 9963 and 1748 images. On the validation data, the algorithm had high sensitivity and specificity. Only one of these values (sensitivity on the second validation data set) failed to be superior at a statistically significant level. The other performance metrics (eg, area under the receiver operating characteristic curve, negative predictive value, positive predictive value) were likewise impressive, giving the authors confidence that this algorithm could be of clinical utility. This work closely mirrors a recent “Kaggle” contest in which 661 teams competed to build an algorithm to predict the grade of diabetic retinopathy, albeit on a smaller data set with fewer grades per image. Kaggle is a website that hosts machine learning and data science contests. Companies and researchers can post their data to Kaggle and have contestants from around the world build predictive models. In the diabetic retinopathy contest, nearly all of the top teams used some form of deep learning and had little to no knowledge of the eye or ophthalmology. The first-place team6 and secondplace team7 both used standard deep learning models and were data science practitioners, not medical professionals. Gulshan et al correctly pointed out that a prerequisite for a successful deep learning model is access to a large database of images with high-quality annotations. Accordingly, the investigators increased both the number of images available and the number of ratings per image, which allowed them to improve on the existing state of the art with respect to both Kaggle and the existing scientific literature. To build their algorithm, Gulshan et al leveraged a workhorse model in deep learning known as a convolutional neural network that has been critically important to recent advances in automatic image recognition. The convolutional neural network model used by the authors is known as the Inception-V3 network,8 which was developed by Google for entry in the Large Scale Visual Recognition Challenge, which it won in 2014. In this contest, known as ImageNet,9 researchers were given 1.2 million images that involve 1000 different categories that cover a wide variety of everyday objects, such as cats, dogs, automobiles, and different kinds of food. The goal of the contest was to build a classifier that could automatically recognize which object was present in an image and to identify which region of the image contained the object. This challenge was broad so that it covered many types of objects that a computer vision system could encounter in the real world. As a result of this contest, several techniques10-12 have been pioneered that improved the accuracy of these models immensely. As with the study by Gulshan et al, these improvements are beginning to trickle into other areas of computer vision, including medical image processing. For example, Gulshan et al not only used the same network that was originally built for ImageNet, they also used that network Editorial and Viewpoint\n",
            "----\n",
            "Paper 532:\n",
            "Title: Stochastic Information Granules Extraction for Graph Embedding and Classification\n",
            "Abstract: Graphs are data structures able to efficiently describe real-world systems and, as such, have been extensively used in recent years by many branches of science, including machine learning engineering. However, the design of efficient graph-based pattern recognition systems is bottlenecked by the intrinsic problem of how to properly match two graphs. In this paper, we investigate a granular computing approach for the design of a general purpose graph-based classification system. The overall framework relies on the extraction of meaningful pivotal substructures on the top of which an embedding space can be build and in which the classification can be performed without limitations. Due to its importance, we address whether information can be preserved by performing stochastic extraction on the training data instead of performing an exhaustive extraction procedure which is likely to be unfeasible for large datasets. Tests on benchmark datasets show that stochastic extraction can lead to a meaningful set of pivotal substructures with a much lower memory footprint and overall computational burden, making the proposed strategies suitable also for dealing with big datasets.\n",
            "----\n",
            "Paper 533:\n",
            "Title: Machine Learning and Deep Learning Approaches to Analyze and Detect COVID-19: A Review\n",
            "Abstract: None\n",
            "----\n",
            "Paper 534:\n",
            "Title: Machine Learning and Deep Learning applications-a vision using the SPSS Method\n",
            "Abstract: AI can be categorised as either machine learning or deep learning. Machine learning, in essence, is AI that can adjust automatically with little human involvement. Artificial neural networks are used in deep learning, a subclass of machine learning, to simulate the educational process of the human brain. Deep learning is more effective with vast amounts of data than other methods. Traditional machine learning methods, however, do better with smaller amounts of data. In order to train deep learning techniques in a timely manner, a highquality infrastructure is needed. The lengthy training process for a deep learning system is caused by the numerous parameters.It takes two weeks to properly train from scratch the well-known ResNet algorithm. Conventional machine learning algorithms can train in a matter of seconds or hours. The scenario is entirely\n",
            "turned around during the experimentation phase. The deep learning method runs quickly while being tested. When the amount of data increases, the testing time for k-nearest neighbours (a type of machine learning technique) increases. Certain machine learning algorithms also have brief test times, however this is not true of all of them. For many industries to apply other methods utilized in deep learning, interpretation is a major problem.Use this as a case study. Let's say we compute a document's relevance score using deep learning. It delivers very good performance that is comparable to human performance. Nevertheless, there is an issue. The rationale behind that score's award is unknown. Actually, it is mathematically possible to determine which\n",
            "nodes of a sophisticated neural network are active, but we are unsure of the expected appearance of the neurons and the function of these layers of neurons as a whole. As a result, we misinterpret the findings. For machine learning techniques like logistic regression and decision trees, this isn't the actual case. We may directly process photos using DL models, which are displayed as multi-layer chemically synthesized neural\n",
            "networks. The part on data curation covers picture labelling, annotation, data synchronisation, association learning, and segmentation, which is a crucial stage in radiomics and causes interference in non-AI imaging investigations due to variances in imaging procedures. Following that, we devote parts to sample size calculation and various AI techniques. Take into account tests, techniques for enhancing data to deal with limited and unbalanced datasets, and descriptions of Ai techniques (the so-called black box problem). advantages and disadvantages of using ML and DL to implement AI.In a synaptic fashion, applications towards medical imaging are eventually shown. Data science, which also encompasses statistics and predictive modelling, contains deep learning as a key component. Deep learning helps to make this process quicker and simpler for data scientists who are gathering, analysing, and interpreting enormous amounts of data. Simply defined, machine learning enables users to submit huge amounts of information to a computer algorithm, which then SPSS statistics is a multivariate analytics, business intelligence, and criminal investigation data\n",
            "management, advanced analytics, developed by IBM for a statistical software package. A long time, spa inc. Was created by, IBM purchased it in 2009. The brand name for the most recent versions is IBM SPSS statistics. Medical Images, Deep Feature Extraction, Predictive Modelling and Prediction. The Cronbach's Alpha Reliability result. The overall Cronbach's Alpha value for the model is .860which indicates 86% reliability. From the literature review, the above 50% Cronbach's Alpha value model can be considered for\n",
            "analysis. Emotional Intelligence the Cronbach's Alpha Reliability result. The overall Cronbach's Alpha value for the model is .860which indicates 86% reliability. From the literature review, the above 50% Cronbach's Alpha value model can be considered for analysis.\n",
            "----\n",
            "Paper 535:\n",
            "Title: Thematic Correlation of Human Cognition and Artificial Intelligence\n",
            "Abstract: Artificial intelligence formally started in 1956 and the world today has seen one change that is from manual worked frameworks to computerized frameworks and it is still during the time spent change. This is the consequence of Artificial Intelligence where gigantic research is going on in creating insight in generally minor guidance based machines. Man-made consciousness is tied in with creating keen machines that can apply information like the human do. The objective of the study is a) to understand the realms and dynamics of Human Cognition; b) To comprehend the dynamics and impacts of Artificial intelligence and c) To assess the correlation between Human Cognition and Artificial intelligence in contemporary world. The methodology followed is focus group interview with 15 experts from Dubai, followed by a thematic analysis of the data. The results highlight that AI and psychological science keep on preparing one another and Perception is one of the significant aspects of the cognition process and bases for Artificial intelligence in action.\n",
            "----\n",
            "Paper 536:\n",
            "Title: Consideration On Automation of 5G Network Slicing with Machine Learning\n",
            "Abstract: Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.\n",
            "----\n",
            "Paper 537:\n",
            "Title: Artificial intelligence-assisted tools for redefining the communication landscape of the scholarly world\n",
            "Abstract: The flood of research output and increasing demands for peer reviewers have necessitated the intervention of artificial intelligence (AI) in scholarly publishing. Although human input is seen as essential for writing publications, the contribution of AI slowly and steadily moves ahead. AI may redefine the role of science communication experts in the future and transform the scholarly publishing industry into a technology-driven one. It can prospectively improve the quality of publishable content and identify errors in published content. In this article, we review various AI and other associated tools currently in use or development for a range of publishing obligations and functions that have brought about or can soon leverage much-demanded advances in scholarly communications. Several AI-assisted tools, with diverse scope and scale, have emerged in the scholarly market. AI algorithms develop summaries of scientific publications and convert them into plain-language texts, press statements, and news stories. Retrieval of accurate and sufficient information is prominent in evidence-based science publications. Semantic tools may empower transparent and proficient data extraction tactics. From detecting simple plagiarism errors to predicting the projected citation impact of an unpublished article, AI’s role in scholarly publishing is expected to be multidimensional. AI, natural language processing, and machine learning in scholarly publishing have arrived for writers, editors, authors, and publishers. They should leverage these technologies to enable the fast and accurate dissemination of scientific information to contribute to the betterment of humankind.\n",
            "----\n",
            "Paper 538:\n",
            "Title: Artificial intelligence in sports on the example of weight training.\n",
            "Abstract: The overall goal of the present study was to illustrate the potential of artificial intelligence (AI) techniques in sports on the example of weight training. The research focused in particular on the implementation of pattern recognition methods for the evaluation of performed exercises on training machines. The data acquisition was carried out using way and cable force sensors attached to various weight machines, thereby enabling the measurement of essential displacement and force determinants during training. On the basis of the gathered data, it was consequently possible to deduce other significant characteristics like time periods or movement velocities. These parameters were applied for the development of intelligent methods adapted from conventional machine learning concepts, allowing an automatic assessment of the exercise technique and providing individuals with appropriate feedback. In practice, the implementation of such techniques could be crucial for the investigation of the quality of the execution, the assistance of athletes but also coaches, the training optimization and for prevention purposes. For the current study, the data was based on measurements from 15 rather inexperienced participants, performing 3-5 sets of 10-12 repetitions on a leg press machine. The initially preprocessed data was used for the extraction of significant features, on which supervised modeling methods were applied. Professional trainers were involved in the assessment and classification processes by analyzing the video recorded executions. The so far obtained modeling results showed good performance and prediction outcomes, indicating the feasibility and potency of AI techniques in assessing performances on weight training equipment automatically and providing sportsmen with prompt advice. Key pointsArtificial intelligence is a promising field for sport-related analysis.Implementations integrating pattern recognition techniques enable the automatic evaluation of data measurements.Artificial neural networks applied for the analysis of weight training data show good performance and high classification rates.\n",
            "----\n",
            "Paper 539:\n",
            "Title: The Role of AI in Cybersecurity: Addressing Threats in the Digital Age\n",
            "Abstract: In the contemporary digital landscape, cybersecurity stands as a paramount concern due to the increasing sophistication and frequency of cyber threats. Artificial Intelligence (AI) has emerged as a potent tool in fortifying defenses against these evolving threats. This paper examines the multifaceted role of AI in cybersecurity, elucidating its applications in threat detection, vulnerability assessment, incident response, and predictive analysis. By leveraging machine learning algorithms, AI systems can swiftly analyze vast troves of data to identify anomalous patterns indicative of potential security breaches. Moreover, AI-driven technologies enable proactive defense mechanisms, empowering organizations to preemptively mitigate risks and safeguard sensitive information. However, the deployment of AI in cybersecurity also raises pertinent ethical and privacy considerations, necessitating a balanced approach towards its implementation. Through a comprehensive analysis, this paper underscores the imperative of integrating AI into cybersecurity frameworks to effectively mitigate threats in the digital age.\n",
            "----\n",
            "Paper 540:\n",
            "Title: Improving Community Resiliency and Emergency Response With Artificial Intelligence\n",
            "Abstract: New crisis response and management approaches that incorporate the latest information technologies are essential in all phases of emergency preparedness and response, including the planning, response, recovery, and assessment phases. Accurate and timely information is as crucial as is rapid and coherent coordination among the responding organizations. We are working towards a multipronged emergency response tool that provide stakeholders timely access to comprehensive, relevant, and reliable information. The faster emergency personnel are able to analyze, disseminate and act on key information, the more effective and timelier their response will be and the greater the benefit to affected populations. Our tool consists of encoding multiple layers of open source geospatial data including flood risk location, road network strength, inundation maps that proxy inland flooding and computer vision semantic segmentation for estimating flooded areas and damaged infrastructure. These data layers are combined and used as input data for machine learning algorithms such as finding the best evacuation routes before, during and after an emergency or providing a list of available lodging for first responders in an impacted area for first. Even though our system could be used in a number of use cases where people are forced from one location to another, we demonstrate the feasibility of our system for the use case of Hurricane Florence in Lumberton, North Carolina.\n",
            "----\n",
            "Paper 541:\n",
            "Title: Assessment of Beer Quality Based on a Robotic Pourer, Computer Vision, and Machine Learning Algorithms Using Commercial Beers.\n",
            "Abstract: Sensory attributes of beer are directly linked to perceived foam-related parameters and beer color. The aim of this study was to develop an objective predictive model using machine learning modeling to assess the intensity levels of sensory descriptors in beer using the physical measurements of color and foam-related parameters. A robotic pourer (RoboBEER), was used to obtain 15 color and foam-related parameters from 22 different commercial beer samples. A sensory session using quantitative descriptive analysis (QDA® ) with trained panelists was conducted to assess the intensity of 10 beer descriptors. Results showed that the principal component analysis explained 64% of data variability with correlations found between foam-related descriptors from sensory and RoboBEER such as the positive and significant correlation between carbon dioxide and carbonation mouthfeel (R = 0.62), correlation of viscosity to sensory, and maximum volume of foam and total lifetime of foam (R = 0.75, R = 0.77, respectively). Using the RoboBEER parameters as inputs, an artificial neural network (ANN) regression model showed high correlation (R = 0.91) to predict the intensity levels of 10 related sensory descriptors such as yeast, grains and hops aromas, hops flavor, bitter, sour and sweet tastes, viscosity, carbonation, and astringency.\n",
            "\n",
            "\n",
            "PRACTICAL APPLICATIONS\n",
            "This paper is a novel approach for food science using machine modeling techniques that could contribute significantly to rapid screenings of food and brewage products for the food industry and the implementation of Artificial Intelligence (AI). The use of RoboBEER to assess beer quality showed to be a reliable, objective, accurate, and less time-consuming method to predict sensory descriptors compared to trained sensory panels. Hence, this method could be useful as a rapid screening procedure to evaluate beer quality at the end of the production line for industry applications.\n",
            "----\n",
            "Paper 542:\n",
            "Title: Application of Artificial Intelligence in Community-Based Primary Health Care: Systematic Scoping Review and Critical Appraisal (Preprint)\n",
            "Abstract: \n",
            " BACKGROUND\n",
            " Research on the integration of artificial intelligence (AI) into community-based primary health care (CBPHC) has highlighted several advantages and disadvantages in practice regarding, for example, facilitating diagnosis and disease management, as well as doubts concerning the unintended harmful effects of this integration. However, there is a lack of evidence about a comprehensive knowledge synthesis that could shed light on AI systems tested or implemented in CBPHC.\n",
            " \n",
            " \n",
            " OBJECTIVE\n",
            " We intended to identify and evaluate published studies that have tested or implemented AI in CBPHC settings.\n",
            " \n",
            " \n",
            " METHODS\n",
            " We conducted a systematic scoping review informed by an earlier study and the Joanna Briggs Institute (JBI) scoping review framework and reported the findings according to PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analysis-Scoping Reviews) reporting guidelines. An information specialist performed a comprehensive search from the date of inception until February 2020, in seven bibliographic databases: Cochrane Library, MEDLINE, EMBASE, Web of Science, Cumulative Index to Nursing and Allied Health Literature (CINAHL), ScienceDirect, and IEEE Xplore. The selected studies considered all populations who provide and receive care in CBPHC settings, AI interventions that had been implemented, tested, or both, and assessed outcomes related to patients, health care providers, or CBPHC systems. Risk of bias was assessed using the Prediction Model Risk of Bias Assessment Tool (PROBAST). Two authors independently screened the titles and abstracts of the identified records, read the selected full texts, and extracted data from the included studies using a validated extraction form. Disagreements were resolved by consensus, and if this was not possible, the opinion of a third reviewer was sought. A third reviewer also validated all the extracted data.\n",
            " \n",
            " \n",
            " RESULTS\n",
            " We retrieved 22,113 documents. After the removal of duplicates, 16,870 documents were screened, and 90 peer-reviewed publications met our inclusion criteria. Machine learning (ML) (41/90, 45%), natural language processing (NLP) (24/90, 27%), and expert systems (17/90, 19%) were the most commonly studied AI interventions. These were primarily implemented for diagnosis, detection, or surveillance purposes. Neural networks (ie, convolutional neural networks and abductive networks) demonstrated the highest accuracy, considering the given database for the given clinical task. The risk of bias in diagnosis or prognosis studies was the lowest in the participant category (4/49, 4%) and the highest in the outcome category (22/49, 45%).\n",
            " \n",
            " \n",
            " CONCLUSIONS\n",
            " We observed variabilities in reporting the participants, types of AI methods, analyses, and outcomes, and highlighted the large gap in the effective development and implementation of AI in CBPHC. Further studies are needed to efficiently guide the development and implementation of AI interventions in CBPHC settings.\n",
            "\n",
            "----\n",
            "Paper 543:\n",
            "Title: A Novel Integration of Data-Driven Rule Generation and Computational Argumentation for Enhanced Explainable AI\n",
            "Abstract: Explainable Artificial Intelligence (XAI) is a research area that clarifies AI decision-making processes to build user trust and promote responsible AI. Hence, a key scientific challenge in XAI is the development of methods that generate transparent and interpretable explanations while maintaining scalability and effectiveness in complex scenarios. Rule-based methods in XAI generate rules that can potentially explain AI inferences, yet they can also become convoluted in large scenarios, hindering their readability and scalability. Moreover, they often lack contrastive explanations, leaving users uncertain why specific predictions are preferred. To address this scientific problem, we explore the integration of computational argumentation—a sub-field of AI that models reasoning processes through defeasibility—into rule-based XAI systems. Computational argumentation enables arguments modelled from rules to be retracted based on new evidence. This makes it a promising approach to enhancing rule-based methods for creating more explainable AI systems. Nonetheless, research on their integration remains limited despite the appealing properties of rule-based systems and computational argumentation. Therefore, this study also addresses the applied challenge of implementing such an integration within practical AI tools. The study employs the Logic Learning Machine (LLM), a specific rule-extraction technique, and presents a modular design that integrates input rules into a structured argumentation framework using state-of-the-art computational argumentation methods. Experiments conducted on binary classification problems using various datasets from the UCI Machine Learning Repository demonstrate the effectiveness of this integration. The LLM technique excelled in producing a manageable number of if-then rules with a small number of premises while maintaining high inferential capacity for all datasets. In turn, argument-based models achieved comparable results to those derived directly from if-then rules, leveraging a concise set of rules and excelling in explainability. In summary, this paper introduces a novel approach for efficiently and automatically generating arguments and their interactions from data, addressing both scientific and applied challenges in advancing the application and deployment of argumentation systems in XAI.\n",
            "----\n",
            "Paper 544:\n",
            "Title: Machine Learning for Information Retrieval: Neural Networks, Symbolic Learning, and Genetic Algorithms\n",
            "Abstract: Information retrieval using probabilistic techniques has attracted significant attention on the part of researchers in information and computer science over the past few decades. In the 1980s, knowledge-based techniques also made an impressive contribution to “intelligent” information retrieval and indexing. More recently, information science researchers have turned to other newer artificial-intelligence-based inductive learning techniques including neural networks, symbolic learning, and genetic algorithms. These newer techniques, which are grounded on diverse paradigms, have provided great opportunities for researchers to enhance the information processing and retrieval capabilities of current information storage and retrieval systems. In this article, we first provide an overview of these newer techniques and their use in information science research. To familiarize readers with these techniques, we present three popular methods: the connectionist Hopfield network; the symbolic ID3/ID5R; and evolution-based genetic algorithms. We discuss their knowledge representations and algorithms in the context of information retrieval. Sample implementation and testing results from our own research are also provided for each technique. We believe these techniques are promising in their ability to analyze user queries, identify users' information needs, and suggest alternatives for search. With proper user-system interactions, these methods can greatly complement the prevailing full-text, keyword-based, probabilistic, and knowledge-based techniques. © 1995 John Wiley & Sons, Inc.\n",
            "----\n",
            "Paper 545:\n",
            "Title: A Brief Review of Machine Learning and Its Application\n",
            "Abstract: With the popularization of information and the establishment of the databases in great number, and how to extract data from the useful information is the urgent problem to be solved. Machine learning is the core issue of artificial intelligence research, this paper introduces the definition of machine learning and its basic structure, and describes a variety of machine learning methods, including rote learning, inductive learning, analogy learning , explained learning, learning based on neural network and knowledge discovery and so on. This paper also brings foreword the objectives of machine learning, and points out the development trend of machine learning. Keywords-machine learning; intelligence; methods; application\n",
            "----\n",
            "Paper 546:\n",
            "Title: Reducing Data Complexity in Feature Extraction and Feature Selection for Big Data Security Analytics\n",
            "Abstract: Feature extraction and feature selection are the first tasks in pre-processing of input logs in order to detect cybersecurity threats and attacks by utilizing data mining techniques in the field of Artificial Intelligence. When it comes to the analysis of heterogeneous data derived from different sources, these tasks are found to be time-consuming and difficult to be managed efficiently. In this paper, we present an approach for handling feature extraction and feature selection utilizing machine learning algorithms for security analytics of heterogeneous data derived from different network sensors. The approach is implemented in Apache Spark, using its python API, named pyspark.\n",
            "----\n",
            "Paper 547:\n",
            "Title: Feature extraction using Deep Learning for Intrusion Detection System\n",
            "Abstract: Deep Learning is an area of Machine Learning research, which can be used to manipulate large amount of information in an intelligent way by using the functionality of computational intelligence. A deep learning system is a fully trainable system beginning from raw input to the final output of recognized objects. Feature selection is an important aspect of deep learning which can be applied for dimensionality reduction or attribute reduction and making the information more explicit and usable. Deep learning can build various learning models which can abstract unknown information by selecting a subset of relevant features. This property of deep learning makes it useful in analysis of highly complex information one which is present in intrusive data or information flowing with in a web system or a network which needs to be analyzed to detect anomalies. Our approach combines the intelligent ability of Deep Learning to build a smart Intrusion detection system.\n",
            "----\n",
            "Paper 548:\n",
            "Title: Machine Learning for Urban Computing\n",
            "Abstract: Increasing urbanization rates and growth of population in urban areas have brought complex problems to be solved in various fields such as urban planning, energy, health, safety, and transportation. Urban computing (UC) is the name of the process that involves collection, integration and analysis of heterogeneous urban data coming from various types of sensors in the city, with the purpose of tackling urban problems (Zheng et al., 2014). Within this framework, the first step of UC is to collect data through sensors like mobile phones, surveillance cameras, vehicles with sensors, or social media applications. The second step is to store, clean, and index the spatiotemporal data through different data management techniques by preparing it for data analytics. The third step is to apply different analysis approaches to solve urban tasks related to different problems. Machine learning methods are some of the most powerful analysis approaches for this purpose, because they can help model arbitrarily complex relationships between measurements and target variables, and they can adapt to dynamically changing environments, which is very important in UC, as cities are constantly in motion. Machine learning (ML) is a subfield of artificial intelligence (AI) that combines computer science and statistics with the objective of optimizing a performance criterion by learning models from data or from past experience (Alpaydın, 2020). The capabilities of ML have sharply increased in the last decades due to increased abilities of computers to store and process large volumes of data, as well as due to theoretical advances. Especially in domains where human expertise is unable to crystallise the path between data sources and the performance measures, ML approaches offer new ways of model creation. In UC, data sources are rich and heterogenous. From a ML perspective, using heterogeneous sources of data as input to models is challenging, as most of the existing paradigms are designed to process one type of data. Additionally, urban data acquisition brings its own challenges. Urban data may come from sensors equipped by infrastructure elements, from public transportation systems, from satellites, from household surveys, governmental institutions, and even from people who act as sensors (called citizen science), either pro-actively supplying information by reporting issues, or by using mobile phones or applications whose data can be read to infer many things about the city and the way it is used by the people. These data sources have different bias and variance characteristics; each sensor comes with its own data gaps (e.g. mobile phone usage may exclude small kids, satellite coverage cannot include what goes on inside large buildings, public transport figures ignore other means of transport, etc.), and each data source is sampled at a different rate. These are all challenges in building unifying models. Apart from detecting patterns in urban usage, ML methods can be used for detecting outliers, such as anomalies in the daily life of the city. For example, mobile phone activity at a certain location can be monitored for automatically detecting unusual patterns (Gündoğdu et al., 2017). Combining ML with computer vision allows us to detect problems in urban settings visually, and to classify areas of the city in terms of safety, wealth, accessibility, etc. (Santani et al., 2015). Another important usage is in prediction of urban activity based on past data, such as predicting queues in gas stations, which can be used for improving waiting times across the city (Zheng et al., 2013). ML can also help with knowledge extraction, providing interpretable insights into data collected from urban sensing.\n",
            "----\n",
            "Paper 549:\n",
            "Title: Intrusions detection based on Support Vector Machine optimized with swarm intelligence\n",
            "Abstract: Intrusion Detection Systems(IDS) have become a necessary component of almost every security infrastructure. Recently, Support Vector Machines (SVM) has been employed to provide potential solutions for IDS. With its many variants for classification SVM is a state-of-the-art machine learning algorithm. However, the performance of SVM depends on selection of the appropriate parameters. In this paper we propose an IDS model based on Information Gain for feature selection combined with the SVM classifier. The parameters for SVM will be selected by a swarm intelligence algorithm (Particle Swarm Optimization or Artificial Bee Colony). We use the NSL-KDD data set and show that our model can achieve higher detection rate and lower false alarm rate than regular SVM.\n",
            "----\n",
            "Paper 550:\n",
            "Title: An Optimization Strategy for Weighted Extreme Learning Machine based on PSO\n",
            "Abstract: Machine learning is a subfield of artificial intelligence concerned with techniques that allow computers to improve their outputs based on previous experiences. Among numerous machine learning algorithms, Weighted Extreme Learning Machine (WELM) is one of the famous cases recently. It not only has Extreme Learning Machine (ELM)’s extremely fast training speed and better generalization performance than traditional Neuron Network (NN), but also has the merit in handling imbalance data by assigning more weight to minority class and less weight to majority class. But it still has the limitation of its weight generated according to class distribution of training data, thereby, creating dependency on input data [R. Sharma and A. S. Bist, Genetic algorithm based weighted extreme learning machine for binary imbalance learning, 2015 Int. Conf. Cognitive Computing and Information Processing (CCIP) (IEEE, 2015), pp. 1–6; N. Koutsouleris, Classification/machine learning approaches, Annu. Rev. Clin. Psychol. 13(1) (2016); G. Dudek, Extreme learning machine for function approximation–interval problem of input weights and biases, 2015 IEEE 2nd Int. Conf. Cybernetics (CYBCONF) (IEEE, 2015), pp. 62–67; N. Zhang, Y. Qu and A. Deng, Evolutionary extreme learning machine based weighted nearest-neighbor equality classification, 2015 7th Int. Conf. Intelligent Human-Machine Systems and Cybernetics (IHMSC), Vol. 2 (IEEE, 2015), pp. 274–279]. This leads to the lack of finding optimal weight at which good generalization performance could be achieved [R. Sharma and A. S. Bist, Genetic algorithm based weighted extreme learning machine for binary imbalance learning, 2015 Int. Conf. Cognitive Computing and Information Processing (CCIP) (IEEE, 2015), pp. 1–6; N. Koutsouleris, Classification/machine learning approaches, Annu. Rev. Clin. Psychol. 13(1) (2016); G. Dudek, Extreme learning machine for function approximation–interval problem of input weights and biases, 2015 IEEE 2nd Int. Conf. Cybernetics (CYBCONF) (IEEE, 2015), pp. 62–67; N. Zhang, Y. Qu and A. Deng, Evolutionary extreme learning machine based weighted nearest-neighbor equality classification, 2015 7th Int. Conf. Intelligent Human-Machine Systems and Cybernetics (IHMSC), Vol. 2 (IEEE, 2015), pp. 274–279]. To solve it, a hybrid algorithm which composed by WELM algorithm and Particle Swarm Optimization (PSO) is proposed. Firstly, it distributes the weight according to the number of different samples, determines weighted method; Then, it combines the ELM model and the weighted method to establish WELM model; finally it utilizes PSO to optimize WELM’s three parameters (input weight, bias, the weight of imbalanced training data). Experiment data from both prediction and recognition show that it has better performance than classical WELM algorithms.\n",
            "----\n",
            "Paper 551:\n",
            "Title: Integrative analysis of AI-driven optimization in HIV treatment regimens\n",
            "Abstract: The integration of artificial intelligence (AI) into HIV treatment regimens has revolutionized the approach to personalized care and optimization strategies. This study presents an in-depth analysis of the role of AI in transforming HIV treatment, focusing on its ability to tailor therapy to individual patient needs and enhance treatment outcomes. AI-driven optimization in HIV treatment involves the utilization of advanced algorithms and computational techniques to analyze vast amounts of patient data, including genetic information, viral load measurements, and treatment history. By harnessing the power of machine learning and predictive analytics, AI algorithms can identify patterns and trends in patient data that may not be readily apparent to human clinicians. One of the key benefits of AI-driven optimization is its ability to personalize treatment regimens based on individual patient characteristics and disease progression. By considering factors such as drug resistance profiles, comorbidities, and lifestyle factors, AI algorithms can recommend the most effective and well-tolerated treatment options for each patient, leading to improved adherence and clinical outcomes. Furthermore, AI enables continuous monitoring and adjustment of treatment regimens in real time, allowing healthcare providers to respond rapidly to changes in patient status and evolving viral dynamics. This proactive approach to HIV management can help prevent treatment failure and the development of drug resistance, ultimately leading to better long-term outcomes for patients. Despite its transformative potential, AI-driven optimization in HIV treatment is not without challenges. Ethical considerations, data privacy concerns, and the need for robust validation and regulatory oversight are all important factors that must be addressed to ensure the safe and effective implementation of AI algorithms in clinical practice. In conclusion, the integrative analysis presented in this study underscores the significant impact of AI-driven optimization on the personalization and optimization of HIV treatment regimens. By leveraging AI technologies, healthcare providers can tailor treatment approaches to individual patient needs, leading to improved outcomes and quality of life for people living with HIV. \n",
            "Keywords: Integrative Analysis, AI- Driven, Optimization, HIV Treatment, Regimens.\n",
            "----\n",
            "Paper 552:\n",
            "Title: Connectionist Models of Neurons, Learning Processes, and Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 553:\n",
            "Title: AI and the transformation of social science research\n",
            "Abstract: Careful bias management and data fidelity are key Advances in artificial intelligence (AI), particularly large language models (LLMs), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (1, 2), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational AI? And how can this be done while ensuring transparent and replicable research?\n",
            "----\n",
            "Paper 554:\n",
            "Title: Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 555:\n",
            "Title: Computer Aided Teaching System Based on Artificial Intelligence in Football Teaching and Training\n",
            "Abstract: As the world's largest sport, football has affected a wide area and a large number of participants and had a great impact on political economy and culture, which has become the best embodiment of the social function of football. Throughout the experience of football in developed countries in the world, the stable development of youth football is the best way to improve the level of football in a country, and the Chinese Football Association has invested more energy in professional leagues and national teams. The development of youth campus football is basically in a state of no management. Therefore, people gradually realize the concept that “football should serve education.” In order to solve the problem of football players' lack of exercise in multiple subjects, it is particularly important to design systems and make plans for their respective physical characteristics. After the failure of various important competitions, the Chinese national football team reflected on the specific factors of backwardness. Under today's system, no one manages the specific development of youth campus football. Especially for young people, training programs that adapt to their individual characteristics should be formulated according to their growth stage and physical characteristics. It can effectively improve the efficiency of football teaching and training (FTT) by managing football players' training information and coaches' teaching information in an intelligent and informatized way. The different sports in which athletes participate in training are identified through the motion recognition layer. The data generated during the entire exercise process, including exercise time, number of exercises, score settlement, and other data, are stored, and the data are finally uploaded to the server, to carry out scientific analysis and management and generate sports training prescriptions in line with their own characteristics. This paper proposes research methods based on the intelligent integrated system of FTT, including literature retrieval, questionnaire survey, training empirical method, comparative analysis method, interview method, and support vector machine model for action recognition, which are used in football teaching and the design experiment of the intelligent integrated system for training; the overall architecture design of the football teaching intelligent integration system and the specific design of the football teaching intelligent integration system are proposed. The experimental results of this article show that 90.70% of players like the teaching mode of intelligent FTT, and the intelligent FTT system can help improve the enthusiasm of players in training and learning.\n",
            "----\n",
            "Paper 556:\n",
            "Title: Tensor Deep Learning Model for Heterogeneous Data Fusion in Internet of Things\n",
            "Abstract: With the rapid evolvement of the Internet and data acquisition technology as well as the continuous advancement of science and technology, the amount of data in many fields has reached the level of terabyte or petabyte and most data collection comes from the Internet of Things (IoT). The rapid advancement of IoT big data has provided valuable opportunities for the development of people in all areas of society. At the same time, it has also brought severe challenges to various types of current information processing systems. Effectively using the big data technology, discovering the hidden laws in big data, tapping the potential value of big data, and predicting the development trend of things to allocate resources more reasonably will promote the overall development of society. However, most of the IoT big data are presented as heterogeneous data, with high dimensions, different forms of expression, and a lot of redundant information. The current machine learning model works in vector space, which makes it impossible to gain big data features because vectors cannot simulate the highly nonlinear distribution of IoT big data. This paper presents a deep learning calculation model called tensor deep learning (TDL), which further improves big data feature learning and high-level feature fusion. It uses tensors to model the complexity of multisource heterogeneous data and extends the vector space data to the tensor space, when feature extraction in the tensor space is included. To fully understand the underlying data distribution, the tensor distance is adopted as the average square sum error term of the output layer reconstruction error. Based on the conventional back-propagation algorithm, this study proposes a high-order back-propagation algorithm to extend the data from the linear space to multiple linear space and train the parameters of the proposed model. Then, to evaluate its performance, the proposed TDL model is compared with the stacked auto encoder and the multimodal deep learning model. Furthermore, experiments are performed on two representative datasets, namely CUAVE and STL-10. Experimental results show that the proposed model not only excels in heterogeneous data fusion but also provides a higher recognition accuracy than the conventional deep learning model or the multimodal learning model for big data.\n",
            "----\n",
            "Paper 557:\n",
            "Title: A Novel Approach to Enriching Well Master Data, Leveraging a Composable Modular Services Architecture Underpinned by AI/ML and the OSDU Data Platform\n",
            "Abstract: \n",
            " PETRONAS Upstream wellbore data, encompassing more than 150,000 documents from over 7,000 wellbores, represents a valuable asset for operational decision-making. The volume and complexity of this data, with its various formats and templates of well reports—including end-of-well reports, final well reports, well completion reports, and other documents from operators and partners—create significant challenges for efficient data management. Traditional manual processes for data extraction, classification, and loading, which are often prone to human error, no longer meet operational and regulatory requirements. In response, Upstream data management has launched an initiative to modernize its wellbore master data management by leveraging advanced technologies.\n",
            " This paper describes a novel approach to enriching Well Master Data, leveraging a composable modular services architecture underpinned by Artificial Intelligence (AI) / Machine Learning (ML) and the OSDU Data Platform. The approach was conceived with the goal of improving Well Master Data quality and its objective, problem statement, solution architecture, challenges and impact on sustaining Upstream data quality are outlined in this paper.\n",
            " Master data, or golden records such as Well Master Data, are managed and governed through well-defined, highly-controlled processes supported by the OSDU data platform. These records serve as the single source of truth across Upstream operations. This robust management ensures consistency, accuracy, and quality across all Upstream business systems. Well Master Data enables functions such as exploration, development, production, decarbonization, regulatory and other supporting Upstream activities to utilize, share, and contextualize well-related information based on the same curated dataset, avoiding discrepancies.\n",
            " Ongoing reconciliation and maintenance of Well Master Data continue to uncover gaps and opportunities to further improve its quality. Low data quality scores for Well Master Data highlight inconsistencies that can lead to incorrect decisions, missed opportunities, inefficiencies, and increased costs related to data sharing, integration, and interoperability. As part of its continuous effort to sustain and enhance data quality, Upstream Data has launched an initiative to significantly enrich Well Master Data through a step-change approach. The solution conceived from the initiative includes a powerful data enrichment process that utilizes cutting-edge technologies, such as advanced Fact Extraction services, Artificial Intelligence (AI), Machine Learning (ML), and modern Optical Character Recognition (OCR), supported by a modular microservices architecture and leveraging the comprehensive data management capabilities (WKS and WKE) of the OSDU data platform.\n",
            " The solution has enabled automated wellbore information extraction from multiple well reports available as an OSDU Kind record. Well information is reliably extracted through using the modern Optical Character Recognition technology. Appropriate machine Learning (ML) models were developed, trained, and underwent a series of validation and testing iterations to identify and extract 19 critical wellbore master data attributes, including: CountryBlockRegionFieldWellbore NameOperatorPhaseElevationElevation PointReference DatumReference PointSpud DateStart DateCompletion DateTVDDFTVDSSTotal DepthTrajectory ShapeWater Depth\n",
            " The above well summary attributes were sourced from a variety of document types that were generated from well campaigns. The report types include: Final Well ReportEnd of Well ReportWell Completion ReportPost Drilling Report\n",
            " The output of the Machine Learning (ML) model execution is ingested into OSDU raw kinds, which are then used to enrich the Well Golden Record via the standard OSDU Well Known Schema (WKS) and Well Known Entity (WKE) services. This automated workflow runs daily, processing any new documents ingested into the OSDU Data Platform, thereby contributing to sustainability efforts to ensure the integrity of upstream data operations. The Machine Learning model has been successfully tested and has generated results for more than 5,000 wellbores by processing over 150,000 documents, extracting approximately 50,000 master well data attributes, and improving data quality completeness with around 4,500 new data attributes.\n",
            " These innovative Master Well Data Enrichment processes have improved the process cycle time (PCE) of manual data entry through machine-driven automation, eliminating mundane and error-prone manual tasks while enhancing data quality. This allows the business to make better and faster decisions. A key characteristic of this solution is the ease and speed with which these processes were modeled, built, and deployed, made possible by a modular services architecture—a core design philosophy of the solution. The modular services architecture is implemented through the Dataiku Data Science system, within which the Master Well Data Enrichment processes are encapsulated and deployed. In summary, these processes are as follows: Data Preparation and Exploration: The process discovers well reports from OSDU Document Kind, OCR storage, and wellbore WKE using elastic-search and document classification metadata. The exploratory screening analyzes report contents, annotates and labels relevant data, and generates statistical distributions and rankings of the relevant content.Machine Learning Model: The model was developed through a train, validate, and test process. As a rule of thumb, 80% of well report samples with sufficient variation were added to the corpus for training, 10% for testing, and 10% for validation. A validation algorithm was established to compare the extracted data with the source data and Well-Known Entity (WKE) data.Golden Record: The next process involves ingesting the extracted data into OSDU Raw Kind, followed by its maturation into the OSDU Wellbore Well-Known Schema (WKS) before being merged into the Well-Known Entity (WKE) as a golden record.\n",
            " All these processes are scripted to automatically detect new well reports and extract Master Well Data as soon as a new report is ingested. Another key advantage of the modular services architecture is its ability to support process deployment at scale. This means that as new data enrichment approaches are developed, they can be seamlessly translated into executable processes.\n",
            " The details of the scope, methodology, technology used, challenges encountered, and results of this novel approach will be explained in the subsequent sections.\n",
            "----\n",
            "Paper 558:\n",
            "Title: Big Data and Deep Learning Analytics\n",
            "Abstract: There has been an enormous growth of the Internet, mobile phone, medical facilities, and many more in the 21st century, which can also be known as the beginning of the knowledge era. Knowledge is defined not for what it is, but for what it can do. In this fast-moving technological era, as a result, a huge amount of data is generated in different regions of the world and it is growing day by day, this growing data is known as “Big Data”. To extract useful information (analyze) from large unstructured data (like Web, sales, customer contact center, social media, mobile data, and so on) is a complex task, as data being generated is a combination of structured, semi-structured and unstructured data. Traditional systems are not capable to handle semi-structured or unstructured data generated whose volume could range in petabytes or exabytes, as the major challenges are limited memory usage, computational hurdles and slower response time, data redundancy, etc. This problem can be overcome with big data analytics having technologies like Apache Hadoop, Apache Spark, Hive, Pig, etc. which can extract useful information from these large data. Authors are going to explore more on them in these chapters. Alongside authors will explore “Deep Learning” also known as “Deep Neural Learning” or “Deep Neural Network”, which is a class of Machine Learning that progressively extract higher-level features from raw data automatically. It performs 'end-to- end learning' and uses layers of algorithms to process data, understand human speech, and visually recognize objects, which is an important part of it. Feature extraction, self-driving cars, fraud detection, healthcare, neural language processing, etc. are some of the areas where it is applied in daily life. Algorithms like RNN, CNN, FNN, Backpropagation, etc, are some of the algorithms used in deep learning. The authors will explore how Machine learning is different from deep learning. Deep learning (DL) is also associated with data science in many ways as the DL algorithms work better than older learning algorithms for prediction or feature extraction etc. Which has brought it, more closer towards one of its main objectives i.e., artificial intelligence (AI)? Hence it is immensely advantageous to the data scientists who aim for making predictions and draw useful information to analyze and interpret it for helping the organization in its growth. The processing of Big Data and the evolution of Artificial Intelligence are both dependent on Deep Learning. Deep learning technology came up along with big data analytics. The concept of deep learning is supportive in the big data analytics due to its efficient use for processing huge and enormous data. This chapter explains about deep learning and big data analytics use in healthcare and alongside authors will study about algorithms used in deep learning and technologies used in big data analytics with its architecture. After reading this chapter, authors must be able to connect deep learning with big data analytics for building new products and contribute to society in a much better way\n",
            "----\n",
            "Paper 559:\n",
            "Title: Multiscale Modeling Meets Machine Learning: What Can We Learn?\n",
            "Abstract: None\n",
            "----\n",
            "Paper 560:\n",
            "Title: Research on adaptive learning methods of Chinese medicine based on big data\n",
            "Abstract: Data-driven refers to a decision support method that takes data as the dominant factor. From a broad perspective, all behaviors are data-driven. From a narrow perspective, the opposite of data-driven is experience-driven. After data collection, data modeling, and processes of data analysis, the messy data can be converted into decision support results. The concept of individualized medical treatment embodied in “Adaptive and Precision Medical Learning of Chinese Medicine” is in the same line with the traditional Chinese medicine based on individual, place, and time conditions. Therefore, the “precision” diagnosis and treatment proposed in this study refers to the interpretation of the accuracy of traditional Chinese medicine on the correlation between data and data, and the improvement and introduction of artificial intelligence methods such as machine learning to understand the classic literature and traditional Chinese medicine with rich knowledge and resources. Feature extraction and visual combing of cases and related data and information, to achieve precise classification of the different states and stages of “disease_syndrome”, find the precise corresponding law of “symptoms, diseases, syndromes and treatments”, and further optimize the medical process of syndrome differentiation and treatment. Improving the clinical efficacy of TCM is the result of cross-research on the objectification, standardization and informationization of TCM diagnosis and treatment.\n",
            "----\n",
            "Paper 561:\n",
            "Title: Accelerating the design and development of polymeric materials via deep learning: Current status and future challenges\n",
            "Abstract: The design and development of polymeric materials have been a hot domain for decades. However, traditional experiments and molecular simulations are time-consuming and labor-intensive, which no longer meet the requirements of new materials development. With the rapid advances of artificial intelligence and materials informatics, machine learning algorithms are increasingly applied in materials science, aiming to shorten the development period of new materials. With the evolution of polymeric materials, the structure of polymers has become more and more complex. Traditional machine learning algorithms often do not perform satisfactorily when dealing with complex data. Presently, deep learning algorithms, including deep neural networks, convolutional neural networks, generative adversarial networks, recurrent neural networks, and graph neural networks, show their uniquely excellent learning capabilities for large and complex data, which will be a powerful tool for the design and development of polymeric materials. This Review introduces principles of several currently popular deep learning algorithms and discusses their multiple applications in the materials field. Applications range from property prediction and molecular generation at the molecular level to structure identification and material synthesis in polymers. Finally, future challenges and opportunities for the application of deep learning in polymeric materials are discussed.\n",
            "----\n",
            "Paper 562:\n",
            "Title: Graph Learning: A Survey\n",
            "Abstract: Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed, respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.\n",
            "----\n",
            "Paper 563:\n",
            "Title: Fostering international AML cooperation: The role of analytical tools in enhancing cross-border regulatory frameworks\n",
            "Abstract: In an increasingly interconnected world, the necessity for robust Anti-Money Laundering (AML) frameworks that transcend national borders has become paramount. This paper examines the critical role of analytical tools in fostering international AML cooperation and enhancing cross-border regulatory frameworks. Effective AML measures rely on comprehensive data analysis to identify and mitigate financial crime risks. Analytical tools, including big data analytics, machine learning, and artificial intelligence, facilitate the collection, processing, and interpretation of vast amounts of financial data from diverse jurisdictions, allowing for more accurate risk assessments. The research highlights how these tools can enhance collaboration among regulatory authorities by providing insights into complex money laundering schemes that often span multiple countries. By leveraging advanced analytics, financial institutions and regulatory bodies can share critical information on suspicious activities, thereby improving the effectiveness of their AML strategies. Furthermore, the paper discusses the significance of developing standardized analytical frameworks that can be adopted globally to ensure consistency in AML practices and reporting requirements. One of the key challenges identified is the need for harmonization of regulatory standards across different jurisdictions. The disparity in AML regulations can hinder effective cooperation and information sharing among nations. The adoption of analytical tools can bridge these gaps by offering standardized metrics and methodologies for evaluating risks associated with cross-border transactions. Additionally, the study examines case studies where analytical tools have successfully been implemented to enhance international AML cooperation. These examples illustrate the potential for increased transparency and accountability in global financial systems, leading to more effective detection and prevention of money laundering activities. In conclusion, fostering international AML cooperation through the strategic use of analytical tools is vital for strengthening cross-border regulatory frameworks. The findings emphasize the importance of innovation and collaboration in combating financial crimes on a global scale. \n",
            "Keywords: Anti-Money Laundering (AML), International Cooperation, Analytical Tools, Cross-Border Regulation, Big Data Analytics, Machine Learning, Financial Crime, Risk Assessment, Standardization, Transparency.\n",
            "----\n",
            "Paper 564:\n",
            "Title: Advances in rock physics for pore pressure prediction: A comprehensive review and future directions\n",
            "Abstract: Advances in rock physics have significantly enhanced pore pressure prediction, a critical aspect of subsurface exploration and drilling operations. This comprehensive review delves into the latest developments in rock physics methodologies, integrating empirical, theoretical, and computational approaches to predict pore pressure more accurately. Traditional pore pressure prediction methods often rely on well log data and seismic attributes, but recent advancements have introduced innovative techniques that leverage the physical properties of rocks to provide more reliable predictions. Key advances include the development of improved rock physics models that better account for the complexities of subsurface environments, such as heterogeneity and anisotropy. These models integrate data from various sources, including well logs, core samples, and seismic surveys, to create a more comprehensive understanding of the subsurface. Additionally, the application of machine learning and artificial intelligence to rock physics has opened new avenues for analyzing large datasets, identifying patterns, and refining predictive models. This review also examines the role of laboratory experiments and field studies in validating and calibrating rock physics models. High-pressure and high-temperature experiments have provided valuable insights into the behavior of rocks under different conditions, which are essential for accurate pore pressure prediction. Field studies, on the other hand, offer real-world data that help in fine-tuning models and methodologies. Future directions in rock physics for pore pressure prediction include the integration of advanced geophysical techniques, such as full-waveform inversion and distributed acoustic sensing, which offer higher resolution data and more detailed subsurface imaging. The use of cloud computing and high-performance computing platforms is also expected to enhance the processing and analysis of large datasets, making predictive models more efficient and scalable. The comprehensive review concludes by highlighting the importance of interdisciplinary collaboration in advancing rock physics methodologies. By combining expertise from geophysics, petrophysics, geomechanics, and data science, the field can continue to innovate and improve the accuracy and reliability of pore pressure predictions, ultimately enhancing exploration and production efficiency in the oil and gas industry. \n",
            "Keywords: Advances, Rock Physics, Pore Pressure, Prediction, Future Directions.\n",
            "----\n",
            "Paper 565:\n",
            "Title: PRICAI 2004: Trends in Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 566:\n",
            "Title: Accelerating Museum AI Research and Application at the UK Natural History Museum: The NHM AI Lab Programme\n",
            "Abstract: The United Kingdom's Natural History Museum (NHM) AI Lab Programme represents a pioneering initiative aimed at harnessing the power of artificial intelligence (AI) to bridge the gap between the museum's extensive collection and cutting-edge AI technologies. Despite its immense potential, the application of AI in museum research remains nascent (e.g., He et al. 2024), with some individual research groups pursuing independent projects without cohesive collaboration with AI experts who know or have experience in similar endeavours. Moreover, differing standards in utilising AI among researchers add complexity to the field. The NHM AI Lab Programme addresses these challenges by co-creating AI pilot projects that bring together the NHM's collection, academic researchers, and AI experts. \n",
            " The NHM AI Lab Programme serves as a nexus for interdisciplinary collaboration, offering expertise in AI, machine learning, data science, and software engineering to support NHM researchers. Through one-to-one consultations and collaborative research projects, the NHM AI Lab Programme facilitates the integration of innovative AI-driven technologies into streamlining digitisation workflows and enhancing Earth and Life Science research at the NHM. \n",
            " In less than a year since its inception, our Programme has achieved several milestones, hosting around 20 diverse projects. These include research projects such as the application of AI for the automatic detection and identification of nannofossils in chalk, the classification of ancient shark and dinosaur teeth, the prediction of mammal disease outbreaks, and the extraction of data from historical bird egg records. Additional projects focus on the automation of mineral analysis and the detection of secondary impact craters on planetary surfaces using AI. Some led to journal publications (e.g., He et al. 2024), while others streamlined NHM researchers' workflows, enhancing their processes of research and digitisation. Moreover, several initiatives have paved the way for new funding streams and collaborative ventures, as well as promising commercial prospects. Certain projects have pioneered the creation or transformation of datasets to meet AI-ready standards, such as data quality, consistency, accessibility, usability, and data governance protocols, helping to embed AI practices into NHM research. \n",
            " This AI Lab Programme can act as a model for other institutions addressing a similar challenge of bridging the gap between AI and their research and collections. This presentation provides insights into the establishment and operation of the NHM AI Lab Programme, shares experiences, highlights successful collaborations, discusses challenges encountered, and outlines future directions.\n",
            "----\n",
            "Paper 567:\n",
            "Title: Enhancing Symbol Recognition in Library Science via Advanced Technological Solutions\n",
            "Abstract: This research introduces an artificial intelligence-based strategy for improving symbol recognition within the field of library science, concentrating on the creation and application of sophisticated technological solutions. Consistent with the objectives of the CHANGES project—Cultural Heritage Active Innovation for Sustainable Society, which focuses on the enhancement and management of cultural heritage through a multidisciplinary and interinstitutional approach—this strategy employs convolutional neural networks (CNNs) for accurate symbol classification. A CNN model was developed using an extensive dataset comprising over 6000 symbols, implementing meticulous preprocessing, feature extraction, and supervised learning protocols. The methodological pipeline incorporates advanced image segmentation techniques to isolate symbols from complex manuscripts, followed by data augmentation to enhance model resilience. The system is supported by a high-performance computing framework to manage large datasets efficiently, thereby facilitating more precise identification and analysis. This integration of machine learning techniques, exhaustive data management, and computational capabilities significantly advances existing symbol recognition methodologies, providing scholars with a potent tool for assisting in the classification and interpretation of historical symbols. The findings corroborate the potential of AI-enhanced symbol recognition in contributing to the broader objectives of computational library science and historical research.\n",
            "----\n",
            "Paper 568:\n",
            "Title: Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th Edition)\n",
            "Abstract: From the Publisher: \n",
            "Combines the theoretical foundations of intelligent problem-solving with he data structures and algorithms needed for its implementation. The book presents logic, rule, object and agent-based architectures, along with example programs written in LISP and PROLOG. \n",
            "The practical applications of AI have been kept within the context of its broader goal: understanding the patterns of intelligence as it operates in this world of uncertainty, complexity and change. \n",
            "The introductory and concluding chapters take a new look at the potentials and challenges facing artificial intelligence and cognitive science. An extended treatment of knowledge-based problem-solving is given including model-based and case-based reasoning. \n",
            "Includes new material on: \n",
            "Fundamentals of search, inference and knowledge representation \n",
            "AI algorithms and data structures in LISP and PROLOG Production systems, blackboards, and meta-interpreters including planers, rule-based reasoners, and inheritance systems. \n",
            "Machine-learning including ID3 with bagging and boosting, explanation based learning, PAC learning, and other forms of induction \n",
            "Neural networks, including perceptrons, back propogation, Kohonen networks, Hopfield networks, Grossberg learning, and counterpropagation. Emergent and social methods of learning and adaptation, including genetic algorithms, genetic programming and artificial life. \n",
            "Object and agent-based problem solving and other forms of advanced knowledge representation\n",
            "----\n",
            "Paper 569:\n",
            "Title: Quantum machine intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 570:\n",
            "Title: Feature extraction using Restricted Boltzmann Machine for stock price prediction\n",
            "Abstract: Recently, many different types of artificial neural networks (ANNs) have been applied to forecast stock price and good performance is obtained. However, most of these models use only a small number of features as input and there may not be enough information to make prediction due to the complexity of stock market. If having a larger number of features, the run time of training would be increased and the generalization performance would be deteriorated due to the curse of dimension. Therefore, an effective tool to extract highly discriminative low-dimensional features from the high-dimensional raw input would be a great help in improving the generalization performance of the regression model. Restricted Boltzmann Machine (RBM) is a new type of machine learning tool with strong power of representation, which has been utilized as the feature extractor in a large variety of classification problems. In this paper, we use the RBM to extract discriminative low-dimensional features from raw data with dimension up to 324, and then use the extracted features as the input of Support Vector Machine (SVM) for regression. Experimental results indicate that our approach for stock price prediction has great improvement in terms of low forecasting errors compared with SVM using raw data.\n",
            "----\n",
            "Paper 571:\n",
            "Title: Combibining Artificial Intelligence and Databases for Data Integration\n",
            "Abstract: None\n",
            "----\n",
            "Paper 572:\n",
            "Title: An In-Depth Analysis of the Role That ML and Big Data Play in Driving Digital Marketing’s Paradigm Shift\n",
            "Abstract: Machine learning (ML) is an artificial neural network (ANN) that helps developers improve their software’s predictive abilities before they have all the data they need. Because information is so priceless, progress toward fully autonomous agents requires better methods for managing the omnipresent content infrastructures that exist today. All sorts of fields have benefited from advancements in computer vision and AI, from medical diagnosis to data presentation and operations to scientific study, and so on. Learning from polluted or erroneous data may be expensive, much as training for a sport can be dangerous to those who are vulnerable to injury. An organization will incur costs rather than see benefits if its algorithms are improperly taught, as explained in Approaching Data Science. Organizations need to be able to verify the quality and consistency of any large datasets, as well as their sources, to ensure the efficacy of any algorithm.\n",
            "----\n",
            "Paper 573:\n",
            "Title: Diabetes Technology Meeting 2023.\n",
            "Abstract: Diabetes Technology Society hosted its annual Diabetes Technology Meeting from November 1 to November 4, 2023. Meeting topics included digital health; metrics of glycemia; the integration of glucose and insulin data into the electronic health record; technologies for insulin pumps, blood glucose monitors, and continuous glucose monitors; diabetes drugs and analytes; skin physiology; regulation of diabetes devices and drugs; and data science, artificial intelligence, and machine learning. A live demonstration of a personalized carbohydrate dispenser for people with diabetes was presented.\n",
            "----\n",
            "Paper 574:\n",
            "Title: An Attribute Extraction for Automated Malware Attack Classification and Detection Using Soft Computing Techniques\n",
            "Abstract: Malware has grown in popularity as a method of conducting cyber assaults in former decades as a result of numerous new deception methods employed by malware. To preserve networks, information, and intelligence, malware must be detected as soon as feasible. This article compares various attribute extraction techniques with distinct machine learning algorithms for static malware classification and detection. The findings indicated that merging PCA attribute extraction and SVM classifier results in the highest correct rate with the fewest possible attributes, and this paper discusses sophisticated malware, their detection techniques, and how and where to defend systems and data from malware attacks. Overall, 96% the proposed method determines the malware more accurately than the existing methods.\n",
            "----\n",
            "Paper 575:\n",
            "Title: Smart Farming Becomes Even Smarter With Deep Learning—A Bibliographical Analysis\n",
            "Abstract: Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection.\n",
            "----\n",
            "Paper 576:\n",
            "Title: Deep Learning for Political Science\n",
            "Abstract: Political science, and social science in general, have traditionally been using computational methods to study areas such as voting behavior, policy making, international conflict, and international development. More recently, increasingly available quantities of data are being combined with improved algorithms and affordable computational resources to predict, learn, and discover new insights from data that is large in volume and variety. New developments in the areas of machine learning, deep learning, natural language processing (NLP), and, more generally, artificial intelligence (AI) are opening up new opportunities for testing theories and evaluating the impact of interventions and programs in a more dynamic and effective way. Applications using large volumes of structured and unstructured data are becoming common in government and industry, and increasingly also in social science research. This chapter offers an introduction to such methods drawing examples from political science. Focusing on the areas where the strengths of the methods coincide with challenges in these fields, the chapter first presents an introduction to AI and its core technology - machine learning, with its rapidly developing subfield of deep learning. The discussion of deep neural networks is illustrated with the NLP tasks that are relevant to political science. The latest advances in deep learning methods for NLP are also reviewed, together with their potential for improving information extraction and pattern recognition from political science texts.\n",
            "----\n",
            "Paper 577:\n",
            "Title: From predicting to decision making: Reinforcement learning in biomedicine\n",
            "Abstract: Reinforcement learning (RL) is one important branch of artificial intelligence (AI), which intuitively imitates the learning style of human beings. It is commonly derived from solving game playing problems and is extensively used for decision‐making, control and optimization problems. It has been extensively applied for solving complicated problems with the property of Markov decision‐making processes. With data accumulation and comprehensive analysis, researchers are not only satisfied with predicting the results for experimental systems but also hope to design or control them for the sake of obtaining the desired properties or functions. RL is potentially facilitated to solve a large number of complicated biological and chemical problems, because they could be decomposed into multi‐step decision‐making process. In practice, substantial progress has been made in the application of RL to the field of biomedicine. In this paper, we will first give a brief description about RL, including its definition, basic theory and different type of methods. Then we will review some detailed applications in various domains, for example, molecular design, reaction planning, molecular simulation and etc. In the end, we will summarize the essentialities of RL approaches to solve more diverse problems compared with other machine learning methods and also outlook the possible trends to overcome their limitations in the future.This article is categorized under:\n",
            "Data Science > Chemoinformatics\n",
            "Data Science > Computer Algorithms and Programming\n",
            "Data Science > Artificial Intelligence/Machine Learning\n",
            "\n",
            "----\n",
            "Paper 578:\n",
            "Title: Hypercomplex Signal Processing in Digital Twin of the Ocean: Theory and application [Hypercomplex Signal and Image Processing]\n",
            "Abstract: The digital twin of the ocean (DTO) is a groundbreaking concept that uses interactive simulations to improve decision-making and promote sustainability in earth science. The DTO effectively combines ocean observations, artificial intelligence (AI), advanced modeling, and high-performance computing to unite digital replicas, forecasting, and what-if scenario simulations of the ocean systems. However, there are several challenges to overcome in achieving the DTO’s objectives, including the integration of heterogeneous data with multiple coordinate systems, multidimensional data analysis, feature extraction, high-fidelity scene modeling, and interactive virtual–real feedback. Hypercomplex signal processing offers a promising solution to these challenges, and this study provides a comprehensive overview of its application in DTO development. We investigate a range of techniques, including geometric algebra, quaternion signal processing, Clifford signal processing, and hypercomplex machine learning, as the theoretical foundation for hypercomplex signal processing in the DTO. We also review the various application aspects of the DTO that can benefit from hypercomplex signal processing, such as data representation and information fusion, feature extraction and pattern recognition, and intelligent process simulation and forecasting, as well as visualization and interactive virtual–real feedback. Our research demonstrates that hypercomplex signal processing provides innovative solutions for DTO advancement and resolving scientific challenges in oceanography and broader earth science.\n",
            "----\n",
            "Paper 579:\n",
            "Title: Recent advances in deep learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 580:\n",
            "Title: The implementation of an AI-driven advertising push system based on a NLP algorithm\n",
            "Abstract: The advertising industry is developing very rapidly, especially outdoor advertising, which has attracted people's attention. All kinds of commercial advertisements can be seen everywhere in outdoor public places, but the advertising delivery system on the market is relatively simple in function, and the evaluation of advertising effect lacks effective automatic analysis means, mainly carried out by manual observation, which is low in efficiency and difficult to conduct quantitative evaluation, which directly leads to the lack of targeted advertising. Artificial intelligence advertising refers to the use of artificial intelligence technology (such as voice recognition, face recognition, deep learning, machine learning, etc.), investigation, production, publishing advertising and other fields, the combination of artificial intelligence and advertising brings great value and convenience to people's lives. Among them, the most common algorithm that can achieve accurate and intelligent advertising is NLP, and the artificial intelligence advertising push system based on natural language processing (NLP) algorithm can provide a variety of useful applications in the advertising field. These applications can help advertisers better understand user needs, improve the accuracy and effectiveness of ads, and provide a better user experience. In order to fully tap the advertising information value contained in unstructured data, this paper introduces the text mining technology based on natural language processing, explores from principle to practice, and analyzes the push process and application of intelligent advertisements in daily life by analyzing the implementation steps of NLP algorithm model.\n",
            "----\n",
            "Paper 581:\n",
            "Title: Why Should I Trust Your Explanation? An Evaluation Approach for XAI Methods Applied to Predictive Process Monitoring Results\n",
            "Abstract: As a use case of process mining, predictive process monitoring (PPM) aims to provide information on the future course of running business process instances. A large number of available PPM approaches adopt predictive models based on machine learning (ML). With the improved efficiency and accuracy of ML models usually being coupled with increasing complexity, their understandability becomes compromised. Having the user at the center of attention, various eXplainable artificial intelligence (XAI) methods emerged to provide users with explanations of the reasoning process of an ML model. Though there is a growing interest in applying XAI methods to PPM results, various proposals have been made to evaluate explanations according to different criteria. In this article, we propose an approach to quantitatively evaluate XAI methods concerning their ability to reflect the facts learned from the underlying stores of business-related data, i.e., event logs. Our approach includes procedures to extract features that are crucial for generating predictions. Moreover, it computes ratios that have proven to be useful in differentiating XAI methods. We conduct experiments that produce useful insights into the effects of the various choices made through a PPM workflow. We can show that underlying data and model issues can be highlighted using the applied XAI methods. Furthermore, we could penalize and reward XAI methods for achieving certain levels of consistency with the facts learned about the underlying data. Our approach has been applied to different real-life event logs using different configurations of the PPM workflow.\n",
            "----\n",
            "Paper 582:\n",
            "Title: Knowledge-based Recommender System of Conceptual Learning in Science\n",
            "Abstract: With the advancement of technology and the development of IT, such as big data, artificial intelligence (AI), and optimization theory have triggered revolutions in many fields. Along with the massive digital dataset, it also gives the reform motivation to promote traditional teaching and learning. How to find the appropriate information that students are interested, valuable, and easy to be understood in the vast amount of information, and bring their creative ability, is still a difficult thing. A Recommender System (RS) for e-learning based on learners’ Science Knowledge (SK) is a powerful tool to solve such problems, which can guarantee the quality and efficiency of the science teaching and learning in the international context. It is one of science research directions with great research value. This paper discusses the overall framework design of a wisdom RS for conceptual learning (CL) in science, which analysis models are based on SK of the students, using a big data software platform. The research sample is made of totally 621 junior students who take the subject of an introductory in science concepts (SC) in Computer Studies of Program, Macao Polytechnic Institution, from 2006 to 2021 academic year. According to the students’ SK, their CL methods have been classified into six types of thinking modes, which has three different corresponding understanding levels for each mode. The appropriate recommended materials to the students who are interested in the information provided, such as, text-based teaching materials, computer assisted materials, simulation tools and game activities, can increases the motivations of the students to learn, considered their different characteristics and understanding. The performance of this RS has been tracked over a period of 16 years, which can effectively improve the personalized teaching quality in the area of computer science, since it may be useful in heightening students’ motivation and interest in CL in science. The recommendation algorithm presented in this paper may be applied for a similar fashion across different domains, topics and contexts.\n",
            "----\n",
            "Paper 583:\n",
            "Title: Data Mining With Computational Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 584:\n",
            "Title: Feature Extraction in Dermoscopic Images for Analysing Dryness Using Second Order GLCM Techniques\n",
            "Abstract: Skin is the primary vital largest organ of Human Body. It is seen than the majority of the people in the world doesn’t experience a good skin life due to various factors and causes. Dermatology is expected to have a startling global market of 8.7 billion dollars in 2022 as it grows daily. It is anticipated to reach approximately $30 billion in value between 2023 and 2028. Skin-related problems are to be expected following COVID-19 and the periodic changes in our lifestyle. In general, genetic or hereditary factors can be used to explain this problem. However, lifestyle factors may also play a role. Skin conditions can affect newborns, young children, teenagers, adults, the elderly, and others. In-depth analysis of the underlying cause and solutions is provided in \"Artificial Intelligence in the field of Dermatology.\" Prediction, diagnosis, and treatment are made more effective when a wide range of solid information is readily available. Artificial Intelligence and technology find a great place in the field of dermatology to comprehend, forecast, and create a wonderful future. Lack of additional data or information for machine learning processing is one of the problems. Despite the field's depth, there is room for improvement in terms of using technology to benefit patients. These are the main elements of challenges; the topic of this essay is skin dryness, which is a prevalent issue. Eczema, dermatitis, skin tumors, and itchiness are all associated with a higher likelihood of dry skin. The suggested method uses Second Order GLCM techniques to identify and examine the skin's textures in dermoscopic images. The improved performance of the classification model will be aided by the feature metrics that were obtained.\n",
            "----\n",
            "Paper 585:\n",
            "Title: Bayesian Modelling for Machine Learning\n",
            "Abstract: Learning algorithms are central to pattern recognition, artificial intelligence, machine learning, data mining, and statistical learning. The term often implies analysis of large and complex data sets with minimal human intervention. Bayesian learning has been variously described as a method of updating opinion based on new experience, updating parameters of a process model based on data, modelling and analysis of complex phenomena using multiple sources of information, posterior probabilistic expectation, and so on. In all of these guises, it has exploded in popularity over recent years. General texts on Bayesian statistics include Bernardo and Smith (1994), Gelman, Carlin, Stern, and Rubin (1995), and Lee (1997). Texts that derive more from the information science discipline, such as Mitchell (1997) and Sarker, Abbass, and Newton (2002), also include sections on Bayesian learning. Given recent advances and the intuitive appeal of the methodology, Bayesian learning is poised to become one of the dominant platforms for modelling and analysis in the 21st century. This article provides an overview of Bayesian learning in this context.\n",
            "----\n",
            "Paper 586:\n",
            "Title: Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 587:\n",
            "Title: Machine Learning Approach to Realtime Intrusion Detection System\n",
            "Abstract: None\n",
            "----\n",
            "Paper 588:\n",
            "Title: Machine Learning algorithms applied to the classification of robotic soccer formations and opponent teams\n",
            "Abstract: Machine Learning (ML) and Knowledge Discovery (KD) are research areas with several different applications but that share a common objective of acquiring more and new information from data. This paper presents an application of several ML techniques in the identification of the opponent team and also on the classification of robotic soccer formations in the context of RoboCup international robotic soccer competition. RoboCup international project includes several distinct leagues were teams composed by different types of real or simulated robots play soccer games following a set of pre-established rules. The simulated 2D league uses simulated robots encouraging research on artificial intelligence methodologies like high-level coordination and machine learning techniques. The experimental tests performed, using four distinct datasets, enabled us to conclude that the Support Vector Machines (SVM) technique has higher accuracy than the k-Nearest Neighbor, Neural Networks and Kernel Naïve Bayes in terms of adaptation to a new kind of data. Also, the experimental results enable to conclude that using the Principal Component Analysis SVM achieves worse results than using simpler methods that have as primary assumption the distance between samples, like k-NN.\n",
            "----\n",
            "Paper 589:\n",
            "Title: AI as an Epistemic Technology\n",
            "Abstract: None\n",
            "----\n",
            "Paper 590:\n",
            "Title: Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data\n",
            "Abstract: Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.\n",
            "----\n",
            "Paper 591:\n",
            "Title: The need for uncertainty quantification in machine-assisted medical decision making\n",
            "Abstract: None\n",
            "----\n",
            "Paper 592:\n",
            "Title: Similarity Intelligence: Similarity Based Reasoning, Computing, and Analytics\n",
            "Abstract: Similarity has been playing an important role in computer science, artificial intelligence (AI) and data science. However, similarity intelligence has been ignored in these disciplines. Similarity intelligence is a process of discovering intelligence through similarity. This article will explore similarity intelligence, similarity-based reasoning, similarity computing and analytics. More specifically, this article looks at the similarity as an intelligence and its impact on a few areas in the real world. It explores similarity intelligence accompanying experience-based intelligence, knowledge-based intelligence, and data-based intelligence to play an important role in computer science, AI, and data science. This article explores similarity-based reasoning (SBR) and proposes three similarity-based inference rules. It then examines similarity computing and analytics, and a multiagent SBR system. The main contributions of this article are: 1) Similarity intelligence is discovered from experience-based intelligence consisting of data-based intelligence and knowledge-based intelligence. 2) Similarity-based reasoning, computing and analytics can be used to create similarity intelligence. The proposed approach will facilitate research and development of similarity intelligence, similarity computing and analytics, machine learning and case-based reasoning.\n",
            "----\n",
            "Paper 593:\n",
            "Title: Artificial neural networks enabled by nanophotonics\n",
            "Abstract: None\n",
            "----\n",
            "Paper 594:\n",
            "Title: Data Science and Visual Computing\n",
            "Abstract: None\n",
            "----\n",
            "Paper 595:\n",
            "Title: Automatic extraction of materials and properties from superconductors scientific literature\n",
            "Abstract: ABSTRACT The automatic extraction of materials and related properties from the scientific literature is gaining attention in data-driven materials science (Materials Informatics). In this paper, we discuss Grobid-superconductors, our solution for automatically extracting superconductor material names and respective properties from text. Built as a Grobid module, it combines machine learning and heuristic approaches in a multi-step architecture that supports input data as raw text or PDF documents. Using Grobid-superconductors, we built SuperCon2, a database of 40,324 materials and properties records from 37,700 papers. The material (or sample) information is represented by name, chemical formula, and material class, and is characterized by shape, doping, substitution variables for components, and substrate as adjoined information. The properties include the Tc superconducting critical temperature and, when available, applied pressure with the Tc measurement method. Graphical Abstract\n",
            "----\n",
            "Paper 596:\n",
            "Title: Open-source intelligence: a comprehensive review of the current state, applications and future perspectives in cyber security\n",
            "Abstract: None\n",
            "----\n",
            "Paper 597:\n",
            "Title: Intelligent Health Care: Applications of Deep Learning in Computational Medicine\n",
            "Abstract: With the progress of medical technology, biomedical field ushered in the era of big data, based on which and driven by artificial intelligence technology, computational medicine has emerged. People need to extract the effective information contained in these big biomedical data to promote the development of precision medicine. Traditionally, the machine learning methods are used to dig out biomedical data to find the features from data, which generally rely on feature engineering and domain knowledge of experts, requiring tremendous time and human resources. Different from traditional approaches, deep learning, as a cutting-edge machine learning branch, can automatically learn complex and robust feature from raw data without the need for feature engineering. The applications of deep learning in medical image, electronic health record, genomics, and drug development are studied, where the suggestion is that deep learning has obvious advantage in making full use of biomedical data and improving medical health level. Deep learning plays an increasingly important role in the field of medical health and has a broad prospect of application. However, the problems and challenges of deep learning in computational medical health still exist, including insufficient data, interpretability, data privacy, and heterogeneity. Analysis and discussion on these problems provide a reference to improve the application of deep learning in medical health.\n",
            "----\n",
            "Paper 598:\n",
            "Title: Artificial Intelligence Techniques for Rational Decision Making\n",
            "Abstract: None\n",
            "----\n",
            "Paper 599:\n",
            "Title: Determination of Vocational Fields with Machine Learning Algorithm\n",
            "Abstract: The importance of vocational and technical training is growing day by day in parallel to the developing technology. It is inevitable to utilise opportunities presented by information and communication technologies in order to determine vocational fields in vocational and technical training in the most efficient manner. In this respect, it is possible to create a more efficient tool compared to the current methods by utilising machine learning which is an artificial intelligence model in energy applications that predicts events in the future depending on the past experiences. In the current study, a software is developed that ensures that the system learns about the successful and unsuccessful choices made in the past by applying “Naive Bayes” algorithm, which is a machine learning algorithm, to the data collected concerning the individuals who turned out to be successful or unsuccessful in the vocational technical training process in energy applications. In the software developed, it is aimed that the system recommends the most suitable vocational field for the individual by according to the data collected from the individual who is in the occupation selection process in field energy applications.\n",
            "----\n",
            "Paper 600:\n",
            "Title: Intelligent Health Risk and Disease Prediction Using Optimized Naive Bayes Classifier\n",
            "Abstract: Machine learning is the subset of Artificial Intelligence and it is used for prediction various real time data analytics applications. Health care monitoring is the major area to analyse the result and make effective decisions. We need intelligent and automated process for predicting diseases using medical dataset. Machine learning methods are proposed to handle the dataset. Smart healthcare prediction is proposed to identify the user or patient information or symptoms as an input. Our system has forecasting accuracy index based on likelihood of the disease and health information. We use Naive bayes classifier algorithm for handling classification, prediction and accuracy index of dataset. Our algorithm measures the disease percentage and train the dataset. Once the prediction result will appears based on effective decision to be taken. In our work, we are taken 20000 train dataset and 7500 test data set for evaluation. TensorFlow simulator is used to simulate the system and measure accuracy. In this system achieves 95% accuracy and performance result compared with existing methods.\n",
            "----\n",
            "Paper 601:\n",
            "Title: Artificial Intelligence: A Systems Approach\n",
            "Abstract: This book offers students and AI programmers a new perspective on the study of artificial intelligence concepts. The essential topics and theory of AI are presented, but it also includes practical information on data input & reduction as well as data output (i.e., algorithm usage). Because traditional AI concepts such as pattern recognition, numerical optimization and data mining are now simply types of algorithms, a different approach is needed. This sensor / algorithm / effecter approach grounds the algorithms with an environment, helps students and AI practitioners to better understand them, and subsequently, how to apply them. The book has numerous up to date applications in game programming, intelligent agents, neural networks, artificial immune systems, and more. A CD-ROM with simulations, code, and figures accompanies the book. *Features *Covers not only AI theory, but modern applications e.g., game programming, machine learning, swarming, artificial immune systems, genetic algorithms, pattern recognition, numerical optimization, data mining, and more *Discusses the various computer languages of AI from LISP to JAVA and Python *Includes a CD-ROM with 100MB of simulations, code, and fi gures *Table of Contents 1. Introduction. 2. Search. 3. Games. 4. Logic. 5. Planning. 6. Knowledge Representation. 7. Machine Learning. 8. Probabilistic Reasoning. 9. Stochastic Search. 10. Neural Networks. 11. Intelligent Agents. 12. Hybrid Models. 13. Languages of AI.\n",
            "----\n",
            "Paper 602:\n",
            "Title: E-Discovery revisited: the need for artificial intelligence beyond information retrieval\n",
            "Abstract: None\n",
            "----\n",
            "Paper 603:\n",
            "Title: Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System\n",
            "Abstract: To establish an automatic conversation system between humans and computers is regarded as one of the most hardcore problems in computer science, which involves interdisciplinary techniques in information retrieval, natural language processing, artificial intelligence, etc. The challenges lie in how to respond so as to maintain a relevant and continuous conversation with humans. Along with the prosperity of Web 2.0, we are now able to collect extremely massive conversational data, which are publicly available. It casts a great opportunity to launch automatic conversation systems. Owing to the diversity of Web resources, a retrieval-based conversation system will be able to find at least some responses from the massive repository for any user inputs. Given a human issued message, i.e., query, our system would provide a reply after adequate training and learning of how to respond. In this paper, we propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. The proposed model is general and unified for different conversation scenarios in open domain. We incorporate the impact of multiple data inputs, and formulate various features and factors with optimization into the deep learning framework. In the experiments, we investigate the effectiveness of the proposed deep neural network structures with better combinations of all different evidence. We demonstrate significant performance improvement against a series of standard and state-of-art baselines in terms of p@1, MAP, nDCG, and MRR for conversational purposes.\n",
            "----\n",
            "Paper 604:\n",
            "Title: Capabilities of cellebrite universal forensics extraction device in mobile device forensics\n",
            "Abstract: The powerful digital forensics tool cellebrite universal forensics extraction device (UFED) extracts and analyzes mobile device data, helping investigators solve criminal and cybersecurity cases. Advanced methods and algorithms allow Cellebrite UFED to recover data from erased or obscured devices. Cellebrite UFED can pull data from call logs, texts, emails, and social media, providing valuable evidence for investigations. The use of smartphones and tablets in personal and professional settings has spurred the development of mobile device forensics. The intuitive user interface speeds up data extraction and analysis, revealing crucial information. It can decrypt encrypted data, recover deleted files, and extract data from multiple devices. The sector's best data extraction functionality, Cellebrite UFED, helps forensic analysts gather crucial evidence for investigations. Legal and ethical considerations are crucial in mobile device forensics. Legal considerations include allowing access to data, protecting privacy, and adhering to chain of custody protocols. Ethics include transparency, defamation, and information exploitation protection. Using Cellebrite UFED, researchers can navigate complex data on mobile devices more efficiently and precisely. Artificial intelligence (AI) and machine learning (ML) algorithms may automate data extraction in future tools. Examiners must train, maintain, and establish clear protocols for using Cellebrite UFED in forensic investigations.\n",
            "----\n",
            "Paper 605:\n",
            "Title: Developing a Comprehensive Framework for Multimodal Feature Extraction\n",
            "Abstract: Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions--ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (videos, images, audio, and text), and is expressly implemented with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct simple feature extraction workflows while increasing code clarity and maintainability.\n",
            "----\n",
            "Paper 606:\n",
            "Title: PRICAI 2002: Trends in Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 607:\n",
            "Title: Benchmarking Active Learning Strategies for Materials Optimization and Discovery\n",
            "Abstract: \n",
            " \n",
            " \n",
            " Autonomous physical science is revolutionizing materials science. In these systems, machine learning controls experiment design, execution, and analysis in a closed loop. Active learning, the machine learning field of optimal experiment design, selects each subsequent experiment to maximize knowledge toward the user goal. Autonomous system performance can be further improved with implementation of scientific machine learning, also known as inductive bias-engineered artificial intelligence, which folds prior knowledge of physical laws (e.g., Gibbs phase rule) into the algorithm. As the number, diversity, and uses for active learning strategies grow, there is an associated growing necessity for real-world reference datasets to benchmark strategies. We present a reference dataset and demonstrate its use to benchmark active learning strategies in the form of various acquisition functions.\n",
            " \n",
            " \n",
            " \n",
            " Active learning strategies are used to rapidly identify materials with optimal physical properties within a compositional phase diagram mapping a ternary materials system. The data is from an actual Fe-Co-Ni thin-film library and includes previously acquired experimental data for materials compositions, X-ray diffraction patterns, and two functional properties of magnetic coercivity and the Kerr rotation.\n",
            " \n",
            " \n",
            " \n",
            " Popular active learning methods along with a recent scientific active learning method are benchmarked for their materials optimization performance.\n",
            " \n",
            " \n",
            " \n",
            " Among the acquisition functions benchmarked, Expected Improvement demonstrated the best overall performance. We discuss the relationship between algorithm performance, materials search space complexity, and the incorporation of prior knowledge, and we encourage benchmarking more and novel active learning schemes.\n",
            "\n",
            "----\n",
            "Paper 608:\n",
            "Title: AI*IA 2009: Emergent Perspectives in Artificial Intelligence, XIth International Conference of the Italian Association for Artificial Intelligence, Reggio Emilia, Italy, December 9-12, 2009, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 609:\n",
            "Title: Efficiency investigation of artificial neural networks in human activity recognition\n",
            "Abstract: None\n",
            "----\n",
            "Paper 610:\n",
            "Title: A Survey on Artificial Intelligence Approaches for Medical Image Classification\n",
            "Abstract: In this paper, a survey has been made on the applications of intelligent computing techniques for diagnostic sciences in biomedical image classification. Several state-of-the-art Artificial Intelligence (AI) techniques for automation of biomedical image classification are investigated. This study gathers representative works that exhibit how AI is applied to the solution of very different problems related to different diagnostic science analysis. It also detects the methods of artificial intelligence that are used frequently together to solve the special problems of medicine. SVM neural network is used in almost all imaging modalities of medical image classification. Similarly fuzzy C means and improvements to it are important tool in segmentation of brain images. Various diagnostic studies like mammogram analysis, MRI brain analysis, bone and retinal analysis etc., using neural network approach result in use of back propagation network, probabilistic neural network, and extreme learning machine recurrently. Hybrid approach of GA and PSO are also commonly used for feature extraction and feature selection.\n",
            "----\n",
            "Paper 611:\n",
            "Title: DIResUNet: Architecture for multiclass semantic segmentation of high resolution remote sensing imagery data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 612:\n",
            "Title: Automatic Machine Translation of Poetry and a Low-Resource Language Pair\n",
            "Abstract: Automatic machine translation is gaining more and more attention in a particular segment of the research community that treats various topics from artificial intelligence, natural language processing, computational linguistics, machine learning and data science. Machine translation is a complex task in which a computer is utilised for the purpose of translating from source to one or more target languages without human involvement, or with a minimum of interventions. In general, machine translation could be implemented in higher education and academic curricula in a variety of possible fields and applications. Although there are several approaches to machine translation, two are dominant today – statistical and neural machine translation. Both are being used in this research in form of two online machine translation systems. The aim of this paper is to examine the usability of machine translation for poetry and a low-resource language pair, such as Croatian-German. The authors chose to use a data set that contained the works of a relevant contemporary poet of the Croatian language and the translations of his poems in German that were conducted by two professional literary translators. The paper demonstrates the effectiveness of machine translation of poetry with regard to special automatic quality metrics.\n",
            "----\n",
            "Paper 613:\n",
            "Title: Multi-Label Chest X-Ray Classification via Deep Learning\n",
            "Abstract: In this era of pandemic, the future of healthcare industry has never been more exciting. Artificial intelligence and machine learning (AI & ML) present opportunities to develop solutions that cater for very specific needs within the industry. Deep learning in healthcare had become incredibly powerful for supporting clinics and in transforming patient care in general. Deep learning is increasingly being applied for the detection of clinically important features in the images beyond what can be perceived by the naked human eye. Chest X-ray images are one of the most common clinical method for diagnosing a number of diseases such as pneumonia, lung cancer and many other abnormalities like lesions and fractures. Proper diagnosis of a disease from X-ray images is often challenging task for even expert radiologists and there is a growing need for computerized support systems due to the large amount of information encoded in X-Ray images. The goal of this paper is to develop a lightweight solution to detect 14 different chest conditions from an X ray image. Given an X-ray image as input, our classifier outputs a label vector indi-cating which of 14 disease classes does the image fall into. Along with the image features, we are also going to use non-image features available in the data such as X-ray view type, age, gender etc. The original study conducted Stanford ML Group is our base line. Original study focuses on predicting 5 diseases. Our aim is to improve upon previous work, expand prediction to 14 diseases and provide insight for future chest radiography research.\n",
            "----\n",
            "Paper 614:\n",
            "Title: A review for cervical histopathology image analysis using machine vision approaches\n",
            "Abstract: None\n",
            "----\n",
            "Paper 615:\n",
            "Title: Bibliometric mining of research directions and trends for big data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 616:\n",
            "Title: Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy\n",
            "Abstract: Machine unlearning (MU) is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and ethics. This article presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are presented. This article also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this article include discussions about the potential benefits of MU and its future directions. Additionally, this article emphasizes the need for researchers and practitioners to continue exploring and refining unlearning techniques to ensure that ML models can adapt to changing circumstances while maintaining user trust. The importance of unlearning is further highlighted in making artificial intelligence (AI) more trustworthy and transparent, especially with the growing importance of AI across various domains that involve large amounts of personal user data.\n",
            "----\n",
            "Paper 617:\n",
            "Title: A survey of swarm and evolutionary computing approaches for deep learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 618:\n",
            "Title: A homomorphic-encryption-based vertical federated learning scheme for rick management\n",
            "Abstract: With continuous improvements of computing power, great progresses in algorithms and massive growth of data, artificial intelligence technologies have entered the third rapid development era. However, With the great improvements in artificial intelligence and the arrival of the era of big data, contradictions between data sharing and user data privacy have become increasingly prominent. Federated learning is a technology that can ensure the user privacy and train a better model from different data providers. In this paper, we design a vertical federated learning system for the for Bayesian machine learning with the homomorphic encryption. During the training progress, raw data are leaving locally, and encrypted model information is exchanged. The model trained by this system is comparable (up to 90%) to those models trained by a single union server under the consideration of privacy. This system can be widely used in risk control, medical, financial, education and other fields. It is of great significance to solve data islands problem and protect users? privacy.\n",
            "----\n",
            "Paper 619:\n",
            "Title: Deep learning and intelligent system towards smart manufacturing\n",
            "Abstract: Machine learning has been applied to solve complex problems in human society for years. The success of machine learning is because of the support of computing capabilities as well as sensing technology. An evolution of artificial intelligence and data-driven approaches will soon cause considerable impacts on the field. Search engines, image recognition, biometrics, speech and handwriting recognition, natural language processing, and even medical diagnostics and financial credit ratings are all common examples. It is clear that many challenges will be brought to public as artificial intelligence infiltrates our world, and more specifically, our lives. With the integration and extensive applications of the new generation of information technologies (such as cloud computing, IoT, big data, deep learning, AVG) in manufacturing industry, a number of countries have put forward their national advanced manufacturing development strategies, such as Industry 4.0 in Germany, Industrial Internet and manufacturing system based on CPS (Cyber-Physical Systems) in the USA, as well as Made in China 2025 and Internet Plus Manufacturing in China. Smart Manufacturing and the Smart Factory enables all information about the manufacturing process to be available when and where it is needed across entire manufacturing supply chains and product lifecycles. Smart Manufacturing is being predicted as the next Industrial Revolution or Industry 4.0. And, as with many other advances throughout recent years, it all has to do with technology connectivity and the advances in the contextualisation of data. However, with neither the intelligent system support nor the support of data science technology, ‘smart’ cannot be achieved.\n",
            "----\n",
            "Paper 620:\n",
            "Title: Ontology Learning and Population: Bridging the Gap between Text and Knowledge - Volume 167 Frontiers in Artificial Intelligence and Applications\n",
            "Abstract: The promise of the Semantic Web is that future web pages will be annotated not only with bright colors and fancy fonts as they are now, but with annotation extracted from large domain ontologies that specify, to a computer in a way that it can exploit, what information is contained on the given web page. The presence of this information will allow software agents to examine pages and to make decisions about content as humans are able to do now. The classic method of building an ontology is to gather a committee of experts in the domain to be modeled by the ontology, and to have this committee agree on which concepts cover the domain, on which terms describe which concepts, on what relations exist between each concept and what the possible attributes of each concept are. All ontology learning systems begin with an ontology structure, which may just be an empty logical structure, and a collection of texts in the domain to be modeled. An ontology learning system can be seen as an interplay between three things: an existing ontology, a collection of texts, and lexical syntactic patterns. The Semantic Web will only be a reality if we can create structured, unambiguous ontologies that model domain knowledge that computers can handle. The creation of vast arrays of such ontologies, to be used to mark-up web pages for the Semantic Web, can only be accomplished by computer tools that can extract and build large parts of these ontologies automatically. This book provides the state-of-art of many automatic extraction and modeling techniques for ontology building. The maturation of these techniques will lead to the creation of the Semantic Web.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences\n",
            "----\n",
            "Paper 621:\n",
            "Title: Proceedings. 15th IEEE International Conference on Tools with Artificial Intelligence\n",
            "Abstract: The following topics are discussed: bioinformatics; software engineering with computational intelligence; data mining; evolutionary computing; planning and scheduling; knowledge management and sharing; machine learning; agents; vision and imaging; artificial intelligence in medicine; fuzzy logic; intelligent information retrieval; knowledge representation; satisfiability; computer vision and pattern recognition.\n",
            "----\n",
            "Paper 622:\n",
            "Title: A Review of Deep Learning for Screening, Diagnosis, and Detection of Glaucoma Progression\n",
            "Abstract: Because of recent advances in computing technology and the availability of large datasets, deep learning has risen to the forefront of artificial intelligence, with performances that often equal, or sometimes even exceed, those of human subjects on a variety of tasks, especially those related to image classification and pattern recognition. As one of the medical fields that is highly dependent on ancillary imaging tests, ophthalmology has been in a prime position to witness the application of deep learning algorithms that can help analyze the vast amount of data coming from those tests. In particular, glaucoma stands as one of the conditions where application of deep learning algorithms could potentially lead to better use of the vast amount of information coming from structural and functional tests evaluating the optic nerve and macula. The purpose of this article is to critically review recent applications of deep learning models in glaucoma, discussing their advantages but also focusing on the challenges inherent to the development of such models for screening, diagnosis and detection of progression. After a brief general overview of deep learning and how it compares to traditional machine learning classifiers, we discuss issues related to the training and validation of deep learning models and how they specifically apply to glaucoma. We then discuss specific scenarios where deep learning has been proposed for use in glaucoma, such as screening with fundus photography, and diagnosis and detection of glaucoma progression with optical coherence tomography and standard automated perimetry. Translational Relevance Deep learning algorithms have the potential to significantly improve diagnostic capabilities in glaucoma, but their application in clinical practice requires careful validation, with consideration of the target population, the reference standards used to build the models, and potential sources of bias.\n",
            "----\n",
            "Paper 623:\n",
            "Title: Intelligence Science in Digital Healthcare Systems\n",
            "Abstract: Intelligent science (IS) is an interdisciplinary subject, which dedicates to joint research on informatics and technology of intelligence by brain science, artificial intelligence, cognitive science, data science and others. IS is devoted to create intelligent computer software that models the human behavior. The main goal of IS is to make computers smarter by creating intelligent algorithms that will allow a computer to mimic some of the functions of the human brain in selected applications. All of these applications employ knowledge base and differencing techniques to solve problems or help make decisions in specific domains. On the other side, digital healthcare systems (DHS) are smart systems and based on the concepts, methodologies and theories of many sciences, e.g. artificial intelligence, data science, social science, cognitive sciences, life sciences and others. Advances in digital DHS domains highlight the need for IoT and ICT systems that aim not only in the improvement of human’s quality of life but at their safety too. The well-known smart healthcare models are; Real-time monitoring devices, Computer-aided surgery devices, Telemedicine devices, Population-based care devices, Personalized medicine from a machine learning perspective, Ubiquities intelligent computing, Expert decision support systems, and Health 2.0. This talk discusses the potential role of AI methodologies in intelligence science as well as the intelligent computing paradigms, which are used in developing the DHS. The following three paradigms are presented:\n",
            "----\n",
            "Paper 624:\n",
            "Title: Preparing High School Teachers to Integrate AI Methods into STEM Classrooms\n",
            "Abstract: In this experience report, we describe an Artificial Intelligence (AI) Methods in Data Science (DS) curriculum and professional development (PD) program designed to prepare high school teachers with AI content knowledge and an understanding of the ethical issues posed by bias in AI to support their integration of AI methods into existing STEM classrooms. The curriculum consists of 5-day units on Data Analytics, Decision trees, Machine Learning, Neural Networks, and Transfer learning that follow a scaffolded learning progression consisting of introductions to concepts grounded in everyday experiences, hands-on activities, interactive web-based tools, and inspecting and modifying the code used to build, train and test AI models within Google Colab notebooks. The participants in the PD program were secondary school teachers from the Southwest and North-east regions of the United States who represented a variety of STEM disciplines: Biology, Chemistry, Physics, Engi-neering, and Mathematics. We share findings on teacher outcomes from the implementation of two one-week PD workshops during the summer of 2021 and share suggestions for improvements provided by teachers. We conclude with a discussion of affordances and challenges encountered in preparing teachers to integrate AI education into disciplinary classrooms.\n",
            "----\n",
            "Paper 625:\n",
            "Title: Methods and tools for causal discovery and causal inference\n",
            "Abstract: Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation‐based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.\n",
            "----\n",
            "Paper 626:\n",
            "Title: A reference model for learning analytics\n",
            "Abstract: Recently, there is an increasing interest in learning analytics in Technology-Enhanced Learning TEL. Generally, learning analytics deals with the development of methods that harness educational datasets to support the learning process. Learning analytics LA is a multi-disciplinary field involving machine learning, artificial intelligence, information retrieval, statistics and visualisation. LA is also a field in which several related areas of research in TEL converge. These include academic analytics, action analytics and educational data mining. In this paper, we investigate the connections between LA and these related fields. We describe a reference model for LA based on four dimensions, namely data and environments what?, stakeholders who?, objectives why? and methods how?. We then review recent publications on LA and its related fields and map them to the four dimensions of the reference model. Furthermore, we identify various challenges and research opportunities in the area of LA in relation to each dimension.\n",
            "----\n",
            "Paper 627:\n",
            "Title: Introduction to Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 628:\n",
            "Title: GANNET: A Machine Learning Approach to Document Retrieval\n",
            "Abstract: Information retrieval using probabilistic techniques has attracted significant attention on the part of researchers in information and computer science over the past few decades. In the 1980s, knowledge-based techniques also have made an impressive contribution to \"intelligent\" information retrieval and indexing. More recently, information science researchers have turned to other, newer artificial intelligence-based inductive learning techniques including neural networks, symbolic learning, and genetic algorithms. The newer techniques have provided great opportunities for researchers to experiment with diverse paradigms for effective information processing and retrieval.In this article we first provide an overview of newer techniques and their usage in information science research. We then present in detail the algorithms we adopted for a hybrid Genetic Algorithms and Neural Nets based system, called GANNET. GANNET performed concept (keyword) optimization for user-selected documents during information retrieval using the genetic algorithms. It then used the optimized concepts to perform concept exploration in a large network of related concepts through the Hopfield net parallel relaxation procedure. Based on a test collection of about 3,000 articles from DIALOG and an automatically created thesaurus, and using Jaccard's score as a performance measure, our experiment showed that GANNET improved the Jaccard's scores by about 50 percent and it helped identify the underlying concepts (keywords) that best describe the user-selected documents.\n",
            "----\n",
            "Paper 629:\n",
            "Title: Enhancing WIKISTIM.org Using Machine Learning Approaches\n",
            "Abstract: To the Editor: The number of published scientific papers has climbed by 8–9% each year over the past several decades. In the biomedical field alone, more than 1 million papers deluge the PubMed data base each year (1). In addition to this, there are at present 49,194 recruiting clinical trials registered on ClinicalTrials.gov and there are 283 currently recruiting studies matching the terms spinal cord stimulation or deep brain stimulation alone in this registry. There exists a clear and pressing need for methods that allow neuromodulation specialists and researchers to access and use research evidence in their clinical routines and in dayto-day decision making. As such, we read with interest, the recent article by North and Shipley concerning their development of WIKISTIM.org, a collaborative website that abstracts data from clinical studies across the breadth of therapeutic neuromodulation into structured data tables, with the apparent aim of enhancing the dissemination, clinical application, and opportunities for critique of research findings from this rapidly developing field (2). Nevertheless, we have concerns regarding the sustainability and utility of this resource for practicing clinicians and researchers. First, concerning sustainability of the database, there seems to be minimal, if any, current or planned implementation of automation technologies for study identification, screening, data abstraction, and data entry validation. At present, relevant studies seem to be identified manually and nonsystematically by section editors (“curated”), with subsequent data abstraction and validation also undertaken manually. Automation tools, which are being developed at a rapid rate, may provide superior results. Progress in related endeavors within evidence-based medicine has demonstrated that technologies that automate those workflow elements that can be reliably automated are required to maximize the use of scarce and valuable human capital. Efforts to establish and optimize the use of artificial intelligence and machine learning techniques for the development of systematic reviews and other forms of evidence synthesis such as rapid reviews, meta-analyses, and evidence overviews such as scoping reviews and health technology assessments are already well underway (3). As an example, it is clear that screening references from biomedical data bases such as the NCBI’s PubMed or Clarivate Analytics’ Web of Science is a lengthy and effortful process. Moreover, in the context of systematic reviews, screening by two reviewers is recommended to avoid missing relevant references. Adoption by the WIKISTIM developers of a software tool such as RobotAnalyst, one of many options in this space, could be a tremendous catalyst for growth of their database. RobotAnalyst is a Web-based screening system that leverages algorithms for information retrieval, text mining, natural language processing (NLP), and machine learning (ML) to assist reviewers in prioritizing references and exploring biomedical data bases using automatic terminology extraction, topic modeling, and descriptive clustering. In a recent study of the software, RobotAnalyst was associated with an up to 71% decrease in screening effort (4). WIKISTIM could then exploit semantic technologies, including NLP and ML techniques, to extract the appropriate datapoints from relevant identified papers, with a certain degree of human oversight required for this stage. Given the authors’ acknowledgement that the vast majority of WIKISTIM’s datasheets are currently empty or incomplete, adoption of this approach has the potential has considerable potential to accelerate datasheet completion. We, therefore, invite the authors to consider how the development and implementation of automation methods could reduce the time and, therefore, costs of screening and data abstraction for WIKISTIM, while at the same time potentially enhancing the comprehensiveness and currency of the WIKISTIM data base through rapid, real-time, systematic screening of the neuromodulation literature with ML-assisted data extraction. Second, the authors do not outline how the effortfully produced WIKISTIM datasheets might feed into evidence summaries and recommendations that work for clinicians at the point of care, with the ultimate aim of facilitating shared decision making with patients. WIKISTIM’s developers have previously stated that their data base has transformative potential to “extend the useful life of clinical reports and improve patient care,” for example by leading to fruitful critical discussion on their website (5). However, review of the WIKISTIM discussion pages reveals a current total of two threads and seven individual posts. In our view, there exists great and hitherto unrealized potential in integrating WIKISTIM into a technology-supported research pipeline with the ability to synthesize the datapoints from their effortfully produced datasheets, with the aim of producing systematic reviews and other contemporary forms of evidence synthesis or feeding directly into clinician decision support systems, for example within structured electronic medical records. Clinicians could use such syntheses to guide clinical decision making and they could also be used more broadly by policy makers to inform funding and policy decisions.\n",
            "----\n",
            "Paper 630:\n",
            "Title: Physics informed token transformer for solving partial differential equations\n",
            "Abstract: Solving partial differential equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing Physics Informed Token Transformer (PITT). The purpose of PITT is to incorporate the knowledge of physics by embedding PDEs into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, PITT is tested on challenging 1D and 2D PDE operator learning tasks. The results show that PITT outperforms popular neural operator models and has the ability to extract physically relevant information from governing equations.\n",
            "----\n",
            "Paper 631:\n",
            "Title: ORDSAENet: Outlier Resilient Semantic Featured Deep Driven Sentiment Analysis Model for Education Domain\n",
            "Abstract: The high pace rising global competitions across education sector has forced institutions to enhance aforesaid aspects,\n",
            "which require assessing students or related stakeholders’ perception and opinion towards the learning materials, courses, learning\n",
            "methods or pedagogies, etc. To achieve it, the use of reviews by students can of paramount significance; yet, annotating student’s\n",
            "opinion over huge heterogenous and unstructured data remains a tedious task. Though, the artificial intelligence (AI) and natural\n",
            "language processing (NLP) techniques can play decisive role; yet the conventional unsupervised lexicon, corpus-based solutions,\n",
            "and machine learning and/or deep driven approaches are found limited due to the different issues like class-imbalance, lack of\n",
            "contextual details, lack of long-term dependency, convergence, local minima etc. The aforesaid challenges can be severe over\n",
            "large inputs in Big Data ecosystems. In this reference, this paper proposed an outlier resilient semantic featuring deep driven\n",
            "sentiment analysis model (ORDSAENet) for educational domain sentiment annotations. To address data heterogeneity and\n",
            "unstructured-ness over unpredictable digital media, the ORDSAENet applies varied pre-processing methods including missing\n",
            "value removal, Unicode normalization, Emoji and Website link removal, removal of the words with numeric values, punctuations\n",
            "removal, lower case conversion, stop-word removal, lemmatization, and tokenization. Moreover, it applies a text size-constrained\n",
            "criteria to remove outlier texts from the input and hence improve ROI-specific learning for accurate annotation. The tokenized\n",
            "data was processed for Word2Vec assisted continuous bag-of-words (CBOW) semantic embedding followed by synthetic\n",
            "minority over-sampling with edited nearest neighbor (SMOTE-ENN) resampling. The resampled embedding matrix was then\n",
            "processed for Bi-LSTM feature extraction and learning that retains both local as well as contextual features to achieve efficient\n",
            "learning and classification. Executing ORDSAENet model over educational review dataset encompassing both qualitative\n",
            "reviews as well as quantitative ratings for the online courses, revealed that the proposed approach achieves average sentiment\n",
            "annotation accuracy, precision, recall, and F-Measure of 95.87%, 95.26%, 95.06% and 95.15%, respectively, which is higher\n",
            "than the LSTM driven standalone feature learning solutions and other state-of-arts. The overall simulation results and allied\n",
            "inferences confirm robustness of the ORDSAENet model towards real-time educational sentiment annotation solution.\n",
            "----\n",
            "Paper 632:\n",
            "Title: A review of machining monitoring systems based on artificial intelligence process models\n",
            "Abstract: None\n",
            "----\n",
            "Paper 633:\n",
            "Title: Stellar Classification vis-à-vis Convolutional Neural Network\n",
            "Abstract: As a result of recent advancements in technology, a variety of new computational fields have emerged. Some examples of these fields are machine learning and intelligence, information science, the internet of things, and others. The advancement of humanity will be greatly aided by these fields. The development of Artificial Intelligence led to the creation of a great deal of Neural Networks. Convolutional Neural Networks are one variation of Neural Networks that we are utilizing in this work. These networks are known to perform quite admirably for Image Categorization, which is one of the purposes for which we are utilizing them. The work encompasses Stellar Classification. There are many stellar entities occupying the region known as universal space. Astrophysicists are familiar with a good number of them, but there are still a great many of these types of entities that have not been discovered yet. Because of the great distance that separates our planet from other stellar entities, any attempt to communicate with them through any channel is highly unlikely to be successful. The most information we could possibly acquire is just a guess as to what kind of entity they are. So, if any scientific observatory comes with a nascent search of any distant entity, we could potentially predict which stellar group they belong to. For the purposes of this work, we are only going to focus on two different types of Stella: Stars and Galaxies. For the purpose of training the Convolutional Neural Network, we have used a dataset on Stellar Types with Image Categorization created by the Aryabhata Research Institute of Observational Sciences (ARIES), which is located in Nainital, India. This dataset was made publicly available.\n",
            "----\n",
            "Paper 634:\n",
            "Title: Finite-State Methods and Natural Language Processing - Post-proceedings of the 7th International Workshop FSMNLP 2008 - Volume 191 Frontiers in Artificial Intelligence and Applications\n",
            "Abstract: These proceedings contain the final versions of the papers presented at the 7th International Workshop on Finite-State Methods and Natural Language Processing (FSMNLP), held in Ispra, Italy, on September 1112, 2008. The aim of the FSMNLP workshops is to bring together members of the research and industrial community working on finite-state based models in language technology, computational linguistics, web mining, linguistics and cognitive science on one hand, and on related theory and methods in fields such as computer science and mathematics on the other. Thus, the workshop series is a forum for researchers and practitioners working on applications as well as theoretical and implementation aspects. The special theme of FSMNLP 2008 was high performance finite-state devices in large-scale natural language text processing systems and applications. The papers in this publication cover a range of interesting NLP applications, including machine learning and translation, logic, computational phonology, morphology and semantics, data mining, information extraction and disambiguation, as well as programming, optimization and compression of finite-state networks. The applied methods include weighted algorithms, kernels and tree automata. In addition, relevant aspects of software engineering, standardization and European funding programs are discussed.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences\n",
            "----\n",
            "Paper 635:\n",
            "Title: Leveraging Multimodality for Biodiversity Data: Exploring joint representations of species descriptions and specimen images using CLIP\n",
            "Abstract: In recent years, the field of biodiversity data analysis has witnessed significant advancements, with a number of models emerging to process and extract valuable insights from various data sources. One notable area of progress lies in the analysis of species descriptions, where structured knowledge extraction techniques have gained prominence. These techniques aim to automatically extract relevant information from unstructured text, such as taxonomic classifications and morphological traits. (Sahraoui et al. 2022, Sahraoui et al. 2023) By applying natural language processing (NLP) and machine learning methods, structured knowledge extraction enables the conversion of textual species descriptions into a structured format, facilitating easier integration, searchability, and analysis of biodiversity data.\n",
            " Furthermore, object detection on specimen images has emerged as a powerful tool in biodiversity research. By leveraging computer vision algorithms (Triki et al. 2020, Triki et al. 2021,Ott et al. 2020), researchers can automatically identify and classify objects of interest within specimen images, such as organs, anatomical features, or specific taxa. Object detection techniques allow for the efficient and accurate extraction of valuable information, contributing to tasks like species identification, morphological trait analysis, and biodiversity monitoring. These advancements have been particularly significant in the context of herbarium collections and digitization efforts, where large volumes of specimen images need to be processed and analyzed.\n",
            " On the other hand, multimodal learning, an emerging field in artificial intelligence (AI), focuses on developing models that can effectively process and learn from multiple modalities, such as text and images (Li et al. 2020, Li et al. 2021, Li et al. 2019, Radford et al. 2021, Sun et al. 2021, Chen et al. 2022). By incorporating information from different modalities, multimodal learning aims to capture the rich and complementary characteristics present in diverse data sources. This approach enables the model to leverage the strengths of each modality, leading to enhanced understanding, improved performance, and more comprehensive representations. \n",
            " Structured knowledge extraction from species descriptions and object detection on specimen images synergistically enhances biodiversity data analysis. This integration leverages textual and visual data strengths, gaining deeper insights. Extracted structured information from descriptions improves search, classification, and correlation of biodiversity data. Object detection enriches textual descriptions, providing visual evidence for the verification and validation of species characteristics.\n",
            " To tackle the challenges posed by the massive volume of specimen images available at the Herbarium of the National Museum of Natural History in Paris, we have chosen to implement the CLIP (Contrastive Language-Image Pretraining) model (Radford et al. 2021) developed by OpenAI. CLIP utilizes a contrastive learning framework to recognize joint representations of text and images. The model is trained on a large-scale dataset consisting of text-image pairs from the internet, enabling it to understand the semantic relationships between textual descriptions and visual content.\n",
            " Fine-tuning the CLIP model on our dataset of species descriptions and specimen images is crucial for adapting it to our domain. By exposing the model to our data, we enhance its ability to understand and represent biodiversity characteristics. This involves training the model on our labeled dataset, allowing it to refine its knowledge and adapt to biodiversity patterns.\n",
            " Using the fine-tuned CLIP model, we aim to develop an efficient search engine for the Herbarium's vast biodiversity collection. Users can query the engine with morphological keywords, and it will match textual descriptions with specimen images to provide relevant results. This research aligns with the current AI trajectory for biodiversity data, paving the way for innovative approaches to address conservation and understanding of our planet's biodiversity.\n",
            "----\n",
            "Paper 636:\n",
            "Title: Music Score Recognition Method Based on Deep Learning\n",
            "Abstract: In recent years, the recommendation application of artificial intelligence and deep music has gradually become a research hotspot. As a complex machine learning algorithm, deep learning can extract features with value laws through training samples. The rise of deep learning network will promote the development of artificial intelligence and also provide a new idea for music score recognition. In this paper, the improved deep learning algorithm is applied to the research of music score recognition. Based on the traditional neural network, the attention weight value improved convolutional neural network (CNN) and high execution efficiency deep belief network (DBN) are introduced to realize the feature extraction and intelligent recognition of music score. Taking the feature vector set extracted by CNN-DBN as input set, a feature learning algorithm based on CNN&DBN was established to extract music score. Experiments show that the proposed model in a variety of different types of polyphony music recognition showed more accurate recognition and good performance; the recognition rate of the improved algorithm applied to the soundtrack identification is as high as 98.4%, which is significantly better than those of other classic algorithms, proving that CNN&DBN can achieve better effect in music information retrieval. It provides data support for constructing knowledge graph in music field and indicates that deep learning has great research value in music retrieval field.\n",
            "----\n",
            "Paper 637:\n",
            "Title: Predicting Heart Failure using SMOTE-ENN-XGBoost\n",
            "Abstract: cardiovascular diseases rank among the top causes of death around the world. Anticipating cardiovascular illness is a major challenge for the healthcare industry. It has been demonstrated that the implementation of Machine Learning (ML), Artificial Intelligence (AI), and data science may effectively aid in decision-making and prediction using the huge quantities of data created by the healthcare industry. The medical field has profited immensely from the use of algorithms and correlation approaches for identifying patterns in the vitals. An imbalanced heart failure data set was analyzed using Logistic Regression, Naive Bayes, Decision Tree, AdaBoost, Random Forest, and XGBoost (XGB). The univariate feature selection model f_classif was used to identify the most relevant characteristics after the dataset was normalized using the Z-score method. This dataset was then balanced by oversampling and undersampling with SMOTE-ENN. Compared to the other ML models applied to the balanced dataset, XGBoost achieved higher levels of accuracy (97%), precision (96%), recall (96%), and F1-score (96%) in classifying heart failure.\n",
            "----\n",
            "Paper 638:\n",
            "Title: GANterfactual—Counterfactual Explanations for Medical Non-experts Using Generative Adversarial Learning\n",
            "Abstract: With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present GANterfactual, an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the art systems that work with saliency maps, namely LIME and LRP.\n",
            "----\n",
            "Paper 639:\n",
            "Title: Development of a pumping system decision support tool based on artificial intelligence\n",
            "Abstract: A framework for the development of a pumping system decision support tool based on artificial intelligence techniques has been investigated. Pump fault detection and diagnosis are key requirements of the decision support tool. Artificial Neural Networks (ANNs) were proposed for condition monitoring data interpretation utilising quantitative performance data. In the analysis, the Cumulative Sum (Cusum) charting procedure was successful in incipient fault identification. Various preprocessing techniques were investigated to obtain maximum diagnostic information despite the inherent problems of real industrial data. The orthonormal technique highlighted good generalisation ability in fast machine learning time. ANNs were successful for accurate, incipient diagnosis of pumping machinery fault conditions based on real industrial data corresponding to historical pump faults.\n",
            "----\n",
            "Paper 640:\n",
            "Title: AI 2003 : advances in artificial intelligence : 16th Australian Conference on AI, Perth, Australia, December 3-5, 2003 : proceedings\n",
            "Abstract: Keynote Papers.- Discovery of Emerging Patterns and Their Use in Classification.- Robot Soccer: Science or Just Fun and Games?.- On How to Learn from a Stochastic Teacher or a Stochastic Compulsive Liar of Unknown Identity.- Multimedia Analysis and Synthesis.- Ontology.- Modelling Message Handling System.- A New Approach for Concept-Based Web Search.- Representing the Spatial Relations in the Semantic Web Ontologies.- Inductive Construction of Ontologies from Formal Concept Analysis.- Problem Solving.- Dynamic Variable Filtering for Hard Random 3-SAT Problems.- A Proposal of an Efficient Crossover Using Fitness Prediction and Its Application.- A New Hybrid Genetic Algorithm for the Robust Graph Coloring Problem.- Estimating Problem Metrics for SAT Clause Weighting Local Search.- Knowledge Discovery and Data Mining I.- Information Extraction via Path Merging.- Natural Language Agreement Description for Reversible Grammars.- Token Identification Using HMM and PPM Models.- Korean Compound Noun Term Analysis Based on a Chart Parsing Technique.- Knowledge Discovery and Data Milling II.- A Language Modeling Approach to Search Distributed Text Databases.- Combining Multiple Host-Based Detectors Using Decision Tree.- Association Rule Discovery with Unbalanced Class Distributions.- Efficiently Mining Frequent Patterns from Dense Datasets Using a Cluster of Computers.- Expert Systems.- Weighted MCRDR: Deriving Information about Relationships between Classifications in MCRDR.- Fuzzy Cognitive Map Learning Based on Nonlinear Hebbian Rule.- MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes.- Selection of Parameters in Building Fuzzy Decision Trees.- Neural Networks Applications.- Tool Condition Monitoring in Drilling Using Artificial Neural Networks.- Software Verification of Redundancy in Neuro-Evolutionary Robotics.- A Firearm Identification System Based on Neural Network.- Predicting the Australian Stock Market Index Using Neural Networks Exploiting Dynamical Swings and Intermarket Influences.- Belief Revisioii and Theorem Proving.- A Tableaux System for Deontic Interpreted Systems.- Decidability of Propositionally Quantified Logics of Knowledge.- Some Logics of Belief and Disbelief.- Axiomatic Analysis of Negotiation Protocols.- Reasoning and Logic.- A Probabilistic Line Breaking Algorithm.- Semiring-Valued Satisfiability.- A Defeasible Logic of Policy-Based Intention.- Dynamic Agent Ordering in Distributed Constraint Satisfaction Problems.- Machine Learning I.- On Why Discretization Works for Naive-Bayes Classifiers.- Adjusting Dependence Relations for Semi-Lazy TAN Classifiers.- Reduction of Non Deterministic Automata for Hidden Markov Model Based Pattern Recognition Applications.- Unsupervised Learning of Correlated Multivariate Gaussian Mixture Models Using MML.- AI Applications.- Cooperative Learning in Self-Organizing E-Learner Communities Based on a Multi-Agents Mechanism.- The Effects of Material, Tempo and Search Depth on Win-Loss Ratios in Chess.- Using Multiple Classification Ripple Down Rules for Intelligent Tutoring System's Knowledge Acquisition.- Model-Based Reinforcement Learning for Alternating Markov Games.- Neural Networks.- HLabelSOM: Automatic Labelling of Self Organising Maps toward Hierarchical Visualisation for Information Retrieval.- Using Images to Compare Two Constructive Network Techniques.- Pareto Neuro-Ensembles.- Predicting the Distribution of Discrete Spatial Events Using Artificial Neural Networks.- Intelligent Agents.- Learning Action Selection Network of Intelligent Agent.- A Dynamic Self-Organizing E-Learner Communities with Improved Multi-agent Matchmaking Algorithm.- Learning to Survive: Increased Learning Rates by Communication in a Multi-agent System.- An Infrastructure for Agent Collaboration in Open Environments.- Computer Vision.- Fingerprint Images Segmentation Using Two Stages Coarse to Fine Discrimination Technique.- Automatic Fingerprint Center Point Determination by Using Modified Directional Field and Morphology.- Convolutional Neural Networks for Image Processing: An Application in Robot Vision.- Towards Automated Creation of Image Interpretation Systems.- AI & Medical Applications.- Dealing with Decision Costs in CBR in Medical Applications.- A Case Study in Feature Invention for Breast Cancer Diagnosis Using X-Ray Scatter Images.- Effectiveness of A Direct Speech Transform Method Using Inductive Learning from Laryngectomee Speech to Normal Speech.- Machine Learning II.- Robustness for Evaluating Rule's Generalization Capability in Data Mining.- Choosing Learning Algorithms Using Sign Tests with High Replicability.- Evaluating a Nearest-Neighbor Method to Substitute Continuous Missing Values.- Single-Class Classification Augmented with Unlabeled Data: A Symbolic Approach.- Machilie Learning and Language.- C3: A New Learning Scheme to Improve Classification of Rare Category Emails.- A New Approach for Scientific Citation Classification Using Cue Phrases.- Automatic Dialogue Segmentation Using Discourse Chunking.- Artificial Intelligence I.- On Using Prototype Reduction Schemes and Classifier Fusion Strategies to Optimize Kernel-Based Nonlinear Subspace Methods.- Noise Tolerance of EP-Based Classifiers.- Guided Operators for a Hyper-Heuristic Genetic Algorithm.- AI \\& Business.- Translating Novelty of Business Model into Terms of Modal Logics.- An eNegotiation Framework.- Teaching Computational Intelligent Techniques with Real-Life Problems in Stock Trading.- Soft Computing.- Finding Optimal Architectures and Weights for ANN: A Combined Hierarchical Approach.- Race Car Chassis Tuning Using Artificial Neural Networks.- Applications of Soft Computing for Musical Instrument Classification.- Nonlinear Time Series Prediction Based on Lyapunov Theory-Based Fuzzy Neural Network and Multiobjective Genetic Algorithm.- A Unified Stochastic Architecture for Spoken Dialogue Systems.- Language Understanding.- Evaluating Corpora for Named Entity Recognition Using Character-Level Features.- Active Learning: Applying RinSCut Thresholding Strategy to Uncertainty Sampling.- The Effect of Evolved Attributes on Classification Algorithms.- Semi-Automatic Construction of Metadata from a Series of Web Documents.- Theory.- Constructive Plausible Logic Is Relatively Consistent.- Heuristic Search Algorithms Based on Symbolic Data Structures.- BN+BN: Behavior Network with Bayesian Network for Intelligent Agent.- Effectiveness of Syntactic Information for Document Classification.- Off-Line Signature Verification and Forgery Detection System Based on Fuzzy Modeling.- Artificial Intelligence II.- Case Study: A Course Advisor Expert System.- Applications of the Ecological Visualization System Using Artificial Neural Network and Mathematical Analysis.- Dynamic Games to Assess Network Value and Performance.- Design and Implementation of an Intelligent Information Infrastructure.- MML Classification of Music Genres.\n",
            "----\n",
            "Paper 641:\n",
            "Title: A Review Study on Applications of Natural Language Processing\n",
            "Abstract: Natural Language Processing (NLP) is a subfield of computer science and artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. The goal of NLP is to enable computers to understand, interpret, and generate human language, allowing them to perform tasks that would typically require human intelligence. In recent years, NLP has experienced significant growth due to the increasing availability of digital content and the need for intelligent information extraction from these sources. This paper reviews recent advances in NLP, including text classification, named entity recognition, sentiment analysis, machine translation, and speech recognition. Additionally, it highlights some of the challenges and future directions for NLP research.\n",
            "----\n",
            "Paper 642:\n",
            "Title: Energy and Policy Considerations for Modern Deep Learning Research\n",
            "Abstract: The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.\n",
            "----\n",
            "Paper 643:\n",
            "Title: A Cognitive Solver with Autonomously Knowledge Learning for Reasoning Mathematical Answers\n",
            "Abstract: Reasoning answers to mathematical problems requires machines to think and operate like a human to learn knowledge from mathematical data, which is one of the fundamental tasks for exploring general artificial intelligence. Most solutions focus on mimicking how humans understand problems, which generate the necessary expressions for answers. However, they are still far from enough since they ignore the core ability of humans to acquire knowledge from experience. In this paper, we propose a Cognitive Solver (CogSolver) that is capable of autonomously learning knowledge from scratch to solve mathematical problems, inspired by two cognitive science theories. Specifically, we draw one insight from the dual process theory to establish an intelligent BRAIN-ARM framework, and refer to another information processing theory to summarize the knowledge learning process into Store-Apply-Update steps. In CogSolver, the BRAIN system stores three types of mathematical knowledge, including semantics knowledge, relation knowledge, and mathematic rule knowledge. Then, the ARM system applies the knowledge in BRAIN to answer the problems. Specifically, we design a knowledge-aware module and a commutative module in ARM to improve its reasoning ability, where the knowledge is organically integrated into answer reasoning process. After solving the problems, BRAIN updates the stored knowledge according to the feedback of ARM, where we develop knowledge filters to eliminate the redundant ones and further form a more reasonable knowledge base. Our CogSolver carries out the above three steps iteratively, which behaves more like a human. We conduct extensive experiments on real-world math word problem datasets. The experimental results demonstrate the improvement in answer reasoning and clearly show how CogSolver gains knowledge from the problems, leading to superior interpretability. Our codes are available at https://github.com/bigdata-ustc/CogSolver.\n",
            "----\n",
            "Paper 644:\n",
            "Title: Multi-stage transfer learning with BERTology-based language models for question answering system in vietnamese\n",
            "Abstract: None\n",
            "----\n",
            "Paper 645:\n",
            "Title: Breast Cancer Classification and Proof of Key Artificial Neural Network Terminologies\n",
            "Abstract: Classification is one of the interesting areas in the academic field of Neural Networks. Artificial Neural Networks (ANNs) have been extensively used in pattern recognition and classification of data in the supervised and unsupervised environment. The ANNs use advanced concepts of computer science where a machine mimics human intelligence while learning from possible experience. To make a machine self-adaptive and autonomous, the machine is properly trained on a training data-set and then subsequently tested on new data. The excellent quality of training of ANNs typically depends on the underlying architecture of the network they employ, for a specific instance, a considerable number of deep layers, number of key nodes in each distinct layer, epoch size, and activation function. In this academic paper, the practical importance of these architectural components is carefully investigated. This paper is precisely about providing a solution that how ANNs can help us in Breast Cancer Classification. Furthermore, sufficient proofs of some extremely important terminologies used in ANNs are also discussed which will clarify the important concepts of ANNs.\n",
            "----\n",
            "Paper 646:\n",
            "Title: Prediction of Breast Cancer through Tolerance-based Intuitionistic Fuzzy-rough Set Feature Selection and Artificial Neural Network\n",
            "Abstract: The importance of diagnosing breast cancer is one of the most significant issues in medical science. Diagnosing whether the cancer is benign or malignant is extremely essential in ascertaining the type of cure, moreover, to bringing down bills. This study aims to use the tolerance-based intuitionistic fuzzy-rough set approach to pick attributes and data processing with help of machine learning for the classification of breast cancer. The main purpose of selecting a feature is to make a subset of input variables by removing irrelevant variables or variables that lack predictive information. This study shows how to eliminate redundant data in big data and achieve more efficient results. Rough set theory has already been used successfully to set down attributes, but this theory is insufficient to reduce the properties of a real- value dataset because it will possibly drop knowledge through the decomposition procedure. and this prevents us from getting the right results. In this study, we used the tolerance based intuitive fuzzy rough method for attribute selection. In this technique, lower and upper approaches are used to intuitive fuzzy sets from rough sets to remove uncertainty due to having simultaneous membership, non-membership, and hesitation degrees and obtain better results. The used method is demonstrated to be better performing in the shape of chosen attributes.\n",
            "----\n",
            "Paper 647:\n",
            "Title: An Entropy-Based Model for Hierarchical Learning\n",
            "Abstract: Machine learning is the dominant approach to artificial intelligence, through which computers learn from data and experience. In the framework of supervised learning, a necessity for a computer to learn from data accurately and efficiently is to be provided with auxiliary information about the data distribution and target function through the learning model. This notion of auxiliary information relates to the concept of regularization in statistical learning theory. A common feature among real-world datasets is that data domains are multiscale and target functions are well-behaved and smooth. This paper proposes an entropy-based learning model that exploits this data structure and discusses its statistical and computational benefits. The hierarchical learning model is inspired by human beings' logical and progressive easy-to-hard learning mechanism and has interpretable levels. The model apportions computational resources according to the complexity of data instances and target functions. This property can have multiple benefits, including higher inference speed and computational savings in training a model for many users or when training is interrupted. We provide a statistical analysis of the learning mechanism using multiscale entropies and show that it can yield significantly stronger guarantees than uniform convergence bounds.\n",
            "----\n",
            "Paper 648:\n",
            "Title: A Transdisciplinary Review of Deep Learning Research and Its Relevance for Water Resources Scientists\n",
            "Abstract: Deep learning (DL), a new generation of artificial neural network research, has transformed industries, daily lives, and various scientific disciplines in recent years. DL represents significant progress in the ability of neural networks to automatically engineer problem‐relevant features and capture highly complex data distributions. I argue that DL can help address several major new and old challenges facing research in water sciences such as interdisciplinarity, data discoverability, hydrologic scaling, equifinality, and needs for parameter regionalization. This review paper is intended to provide water resources scientists and hydrologists in particular with a simple technical overview, transdisciplinary progress update, and a source of inspiration about the relevance of DL to water. The review reveals that various physical and geoscientific disciplines have utilized DL to address data challenges, improve efficiency, and gain scientific insights. DL is especially suited for information extraction from image‐like data and sequential data. Techniques and experiences presented in other disciplines are of high relevance to water research. Meanwhile, less noticed is that DL may also serve as a scientific exploratory tool. A new area termed AI neuroscience, where scientists interpret the decision process of deep networks and derive insights, has been born. This budding subdiscipline has demonstrated methods including correlation‐based analysis, inversion of network‐extracted features, reduced‐order approximations by interpretable models, and attribution of network decisions to inputs. Moreover, DL can also use data to condition neurons that mimic problem‐specific fundamental organizing units, thus revealing emergent behaviors of these units. Vast opportunities exist for DL to propel advances in water sciences.\n",
            "----\n",
            "Paper 649:\n",
            "Title: How Can AI Help Improve Food Safety?\n",
            "Abstract: With advances in artificial intelligence (AI) technologies, the development and implementation of digital food systems are becoming increasingly possible. There is tremendous interest in using different AI applications, such as machine learning models, natural language processing, and computer vision to improve food safety. Possible AI applications are broad and include, but are not limited to, (a) food safety risk prediction and monitoring as well as food safety optimization throughout the supply chain, (b) improved public health systems (e.g., by providing early warning of outbreaks and source attribution), and (c) detection, identification, and characterization of foodborne pathogens. However, AI technologies in food safety lag behind in commercial development because of obstacles such as limited data sharing and limited collaborative research and development efforts. Future actions should be directed toward applying data privacy protection methods, improving data standardization, and developing a collaborative ecosystem to drive innovations in AI applications to food safety. Expected final online publication date for the Annual Review of Food Science and Technology, Volume 14 is March 2023. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\n",
            "----\n",
            "Paper 650:\n",
            "Title: Markov Logic: An Interface Layer for Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 651:\n",
            "Title: BiodivNERE: Gold standard corpora for named entity recognition and relation extraction in the biodiversity domain\n",
            "Abstract: Abstract Background Biodiversity is the assortment of life on earth covering evolutionary, ecological, biological, and social forms. To preserve life in all its variety and richness, it is imperative to monitor the current state of biodiversity and its change over time and to understand the forces driving it. This need has resulted in numerous works being published in this field. With this, a large amount of textual data (publications) and metadata (e.g. dataset description) has been generated. To support the management and analysis of these data, two techniques from computer science are of interest, namely Named Entity Recognition (NER) and Relation Extraction (RE). While the former enables better content discovery and understanding, the latter fosters the analysis by detecting connections between entities and, thus, allows us to draw conclusions and answer relevant domain-specific questions. To automatically predict entities and their relations, machine/deep learning techniques could be used. The training and evaluation of those techniques require labelled corpora. New information In this paper, we present two gold-standard corpora for Named Entity Recognition (NER) and Relation Extraction (RE) generated from biodiversity datasets metadata and abstracts that can be used as evaluation benchmarks for the development of new computer-supported tools that require machine learning or deep learning techniques. These corpora are manually labelled and verified by biodiversity experts. In addition, we explain the detailed steps of constructing these datasets. Moreover, we demonstrate the underlying ontology for the classes and relations used to annotate such corpora.\n",
            "----\n",
            "Paper 652:\n",
            "Title: Transforming Accounts Payable Management: Harnessing the Power of AI-Embedded OCR and Digitalization in the Accounting Sector\n",
            "Abstract: : In the ever-changing digital landscape, the fusion of artificial intelligence (AI) and digitalization is significantly impacting various industries, including the accounting sector. This paper provides a comprehensive analysis of AI's transformative impact on accounts payable processes, focusing on the integration of AI algorithms and the digitalization of the accounting industry to optimize operational efficiency. By investigating the capabilities of innovative machine learning algorithms, such as Optical Character Recognition (OCR), Natural Language Processing (NLP), and predictive analytics, this research examines their ability to streamline data extraction, validation, routing, matching, and prediction processes while reducing errors and accelerating the accounts payable process. Additionally, the paper explores the implementation of AI-embedded OCR systems in accounts payable processes, revealing their transformative potential in various sectors, including the finance industry, and discusses the challenges they face, such as handwriting recognition, multilingual OCR, adaptation to new fonts and styles, and robustness against noise and distortion. The paper concludes by emphasizing the importance of embracing AI-driven solutions to enhance financial performance in an increasingly competitive and digital landscape and highlights the future prospects for AI and OCR technologies in shaping information extraction and processing across various industries.\n",
            "----\n",
            "Paper 653:\n",
            "Title: A Survey of Data-Driven and Knowledge-Aware eXplainable AI\n",
            "Abstract: We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.\n",
            "----\n",
            "Paper 654:\n",
            "Title: Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning\n",
            "Abstract: Significance Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in the wild. A key obstacle to harnessing their potential is the great cost of having humans analyze each image. Here, we demonstrate that a cutting-edge type of artificial intelligence called deep neural networks can automatically extract such invaluable information. For example, we show deep learning can automate animal identification for 99.3% of the 3.2 million-image Snapshot Serengeti dataset while performing at the same 96.6% accuracy of crowdsourced teams of human volunteers. Automatically, accurately, and inexpensively collecting such data could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with >93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving >8.4 y (i.e., >17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.\n",
            "----\n",
            "Paper 655:\n",
            "Title: A Survey on Brain Effective Connectivity Network Learning\n",
            "Abstract: Human brain effective connectivity characterizes the causal effects of neural activities among different brain regions. Studies of brain effective connectivity networks (ECNs) for different populations contribute significantly to the understanding of the pathological mechanism associated with neuropsychiatric diseases and facilitate finding new brain network imaging markers for the early diagnosis and evaluation for the treatment of cerebral diseases. A deeper understanding of brain ECNs also greatly promotes brain-inspired artificial intelligence (AI) research in the context of brain-like neural networks and machine learning. Thus, how to picture and grasp deeper features of brain ECNs from functional magnetic resonance imaging (fMRI) data is currently an important and active research area of the human brain connectome. In this survey, we first show some typical applications and analyze existing challenging problems in learning brain ECNs from fMRI data. Second, we give a taxonomy of ECN learning methods from the perspective of computational science and describe some representative methods in each category. Third, we summarize commonly used evaluation metrics and conduct a performance comparison of several typical algorithms both on simulated and real datasets. Finally, we present the prospects and references for researchers engaged in learning ECNs.\n",
            "----\n",
            "Paper 656:\n",
            "Title: Deep Learning Algorithms based Fingerprint Authentication: Systematic Literature Review\n",
            "Abstract: Deep Learning algorithms (DL) have been applied in different domains such as computer vision, image detection, robotics and speech processing, in most cases, DL demonstrated better performance than the conventional machine learning algorithms (shallow algorithms). The artificial intelligence research community has leveraged the robustness of the DL because of their ability to process large data size and handle variations in biometric data such as aging or expression problem. Particularly, DL research in automatic fingerprint recognition system (AFRS) is gaining momentum starting from the last decade in the area of fingerprint pre-processing, fingerprints quality enhancement, fingerprint feature extraction, security of fingerprint and performance improvement of AFRS. However, there are limited studies that address the application of DL to model fingerprint biometric for different tasks in the fingerprint recognition process. To bridge this gap, this paper presents a systematic literature review and an insightful meta-data analysis of a decade applications of DL in AFRS. Discussion on proposed model’s tasks, state of the art study, dataset, and training architecture are presented. The Convolutional Neural Networks models were the most saturated models in developing fingerprint biometrics authentication. The study revealed different roles of the DL in training architecture of the models: feature extractor, classifier and end-to-end learning. The review highlights open research challenges and present new perspective for solving the challenges in the future. The author believed that this paper will guide researchers in propose novel fingerprint authentication scheme.\n",
            "----\n",
            "Paper 657:\n",
            "Title: Deep Challenges Associated with Deep Learning\n",
            "Abstract: Deep learning is a recent emerging field of research in data science. Deep learning is essentially a combination of artificial intelligence and machine learning. Inspired by brain neurons, this has proven greater flexibility and builds more accurate models compared to machine learning. But making theoretical designs and to perform desired experiments are quite challenging due to many aspects. In the present paper, these challenges have been discussed to provide researchers a clear vision for the futuristic research in the field of deep learning.\n",
            "----\n",
            "Paper 658:\n",
            "Title: A Comparative Analysis of Unsupervised Machine Techniques for Liver Disease Prediction\n",
            "Abstract: Machine learning is a branch of Artificial Intelligence(AI) which is heavily used in the field of data science. It has a strong potential in health-related data analysis for automated disease prediction. The work focuses on three different machine learning techniques, i.e., DBSCAN, K-Means, and Affinity Propagation to compare their prediction accuracy and computational complexity. The study concentrates on liver disease-related health care data set and uses the Silhouette coefficient for comparative performance measurement of the three techniques mentioned above. The Silhouette coefficient determines prediction accuracy giving K-Means as the optimal method. The overall results will then be analyzed on the basis of prediction accuracy and computational complexity to determine the best technique for prediction of liver diseases using unsupervised machine learning.\n",
            "----\n",
            "Paper 659:\n",
            "Title: Use of NLP Techniques in Translation by ChatGPT: Case Study\n",
            "Abstract: Use of NLP Techniques in Translation by ChatGPT: Case Study Natural Language Processing (NLP) refers to a field of study within the domain of artificial intelligence (AI) and computational linguistics that focuses on the interaction between computers and human language. NLP seeks to develop computational models and algorithms capable of understanding, analyzing, and generating natural language text and speech (Brown et al., 1990). At its core, NLP aims to bridge the gap between human language and machine understanding by employing various techniques from linguistics, computer science, and statistics. It involves the application of linguistic and computational theories to process, interpret, and extract meaningful information from unstructured textual data (Bahdanau, Cho and Bengio, 2015). Researchers and practitioners in NLP employ diverse methodologies, including rule-based approaches, statistical models, machine learning techniques (such as neural networks), and more recently, deep learning architectures. These methodologies enable the development of robust algorithms that can learn from large-scale language data to improve the accuracy and effectiveness of language processing systems (Nilsson, 2010). NLP has numerous real-world applications across various domains, including information retrieval, virtual assistants, chatbots, social media analysis, sentiment monitoring, automated translation services, and healthcare, among others (kaynak). As the field continues to advance, NLP strives to overcome challenges such as understanding the nuances of human language, handling ambiguity, context sensitivity, and incorporating knowledge from diverse sources to enable machines to effectively communicate and interact with humans in a more natural and intuitive manner. Natural Language Processing (NLP) and translation are interconnected fields that share a symbiotic relationship, as NLP techniques and methodologies greatly contribute to the advancement and effectiveness of machine translation systems. NLP, a subfield of artificial intelligence (AI), focuses on the interaction between computers and human language. It encompasses a wide range of tasks, including text analysis, syntactic and semantic parsing, sentiment analysis, information extraction, and machine translation (Bahdanau, Cho and Bengio, 2014). NMT models employ deep learning architectures, such as recurrent neural networks (RNNs) and more specifically, long short-term memory (LSTM) networks, to learn the mapping between source and target language sentences. These models are trained on large-scale parallel corpora, consisting of aligned sentence pairs in different languages. The training process involves optimizing model parameters to minimize the discrepancy between predicted translations and human-generated translations (Wu et al., 2016) NLP techniques are crucial at various stages of machine translation. Preprocessing techniques, such as tokenization, sentence segmentation, and morphological analysis, help break down input text into meaningful linguistic units, making it easier for translation models to process and understand the content. Syntactic and semantic parsing techniques aid in capturing the structural and semantic relationships within sentences, improving the overall coherence and accuracy of translations. Furthermore, NLP-based methods are employed for handling specific translation challenges, such as handling idiomatic expressions, resolving lexical ambiguities, and addressing syntactic divergences between languages. For instance, statistical alignment models, based on NLP algorithms, enable the identification of correspondences between words or phrases in source and target languages, facilitating the generation of more accurate translations (kaynak). Several studies have demonstrated the effectiveness of NLP techniques in enhancing machine translation quality. For example, Bahdanau et al. (2015) introduced the attention mechanism, an NLP technique that enables NMT models to focus on relevant parts of the source sentence during translation. This attention mechanism significantly improved the translation quality of neural machine translation models. ChatGPT is a language model developed by OpenAI that utilizes the principles of Natural Language Processing (NLP) for various tasks, including translations. NLP is a field of artificial intelligence that focuses on the interaction between computers and human language. It encompasses a range of techniques and algorithms for processing, analyzing, and understanding natural language. When it comes to translation, NLP techniques can be applied to facilitate the conversion of text from one language to another. ChatGPT employs a sequence-to-sequence model, a type of neural network architecture commonly used in machine translation tasks. This model takes an input sequence in one language and generates a corresponding output sequence in the target language (OpenAI, 2023). The training process for ChatGPT involves exposing the model to large amounts of multilingual data, allowing it to learn patterns, syntax, and semantic relationships across different languages. This exposure enables the model to develop a general understanding of language structures and meanings, making it capable of performing translation tasks. To enhance translation quality, ChatGPT leverages the Transformer architecture, which has been highly successful in NLP tasks. Transformers utilize attention mechanisms, enabling the model to focus on different parts of the input sequence during the translation process. This attention mechanism allows the model to capture long-range dependencies and improve the overall coherence and accuracy of translations. Additionally, techniques such as subword tokenization, which divides words into smaller units, are commonly employed in NLP translation systems like ChatGPT. Subword tokenization helps handle out-of-vocabulary words and improves the model’s ability to handle rare or unknown words (GPT-4 Technical Report, 2023). As can be seen, there have been significant developments in artificial intelligence translations thanks to NLP. However, it is not possible to say that it has fully reached the quality of translation made by people. The only goal in artificial intelligence translations is to reach translations made by humans. In general, there are some fundamental differences between human and ChatGPT translations. Human-made translations and translations generated by ChatGPT (or similar language models) have several key differences (Kelly and Zetzsche, 2014; Koehn, 2010; Sutskever, Vinyals and Le, 2014; Costa-jussà and Fonollosa, 2018) Translation Quality: Human translators are capable of producing high-quality translations with a deep understanding of both the source and target languages. They can accurately capture the nuances, cultural references, idioms, and context of the original text. On the other hand, ChatGPT translations can sometimes be less accurate or may not fully grasp the intended meaning due to the limitations of the training data and the model’s inability to comprehend context in the same way a human can. While ChatGPT can provide reasonable translations, they may lack the finesse and precision of a human translator. Natural Language Processing: Human translators are skilled at processing and understanding natural language, taking into account the broader context, cultural implications, and the intended audience. They can adapt their translations to suit the target audience, tone, and purpose of the text. ChatGPT, although trained on a vast amount of text data, lacks the same level of natural language understanding. It often relies on pattern matching and statistical analysis to generate translations, which can result in less nuanced or contextually appropriate outputs. Subject Matter Expertise: Human translators often specialize in specific domains or subject areas, allowing them to have deep knowledge and understanding of technical or specialized terminology. They can accurately translate complex or industry-specific texts, ensuring the meaning is preserved. ChatGPT, while having access to a wide range of general knowledge, may struggle with domain-specific vocabulary or terminology, leading to inaccuracies or incorrect translations in specialized texts. Cultural Sensitivity: Human translators are well-versed in the cultural nuances of both the source and target languages. They can navigate potential pitfalls, adapt the translation to the cultural context, and avoid unintended offensive or inappropriate language choices. ChatGPT lacks this level of cultural sensitivity and may produce translations that are culturally tone-deaf or insensitive, as it lacks the ability to understand the subtleties and implications of language choices. Revision and Editing: Human translators go through an iterative process of revision and editing to refine their translations, ensuring accuracy, clarity, and quality. They can self-correct errors and refine their translations based on feedback or additional research. ChatGPT, while capable of generating translations, does not have the same ability to self-correct or improve based on feedback. It generates translations in a single pass, without the iterative refinement process that humans can employ. In summary, while ChatGPT can be a useful tool for generating translations, human-made translations generally outperform machine-generated translations in terms of quality, accuracy, contextuality, cultural sensitivity, and domain-specific expertise. In conclusion, NLP and machine translation are closely intertwined, with NLP providing essential tools, methodologies, and techniques that contribute to the development and improvement of machine translation systems. The integration of NLP methods has led to significant advancements in translation accuracy, fluency, and the ability to handle various linguistic complexities. As NLP continues to evolve, its impact\n",
            "----\n",
            "Paper 660:\n",
            "Title: Multi-Disease Prediction Based on Deep Learning: A Survey\n",
            "Abstract: In recent years, the development of artificial intelligence (AI) and the gradual beginning of AI's research in the medical field have allowed people to see the excellent prospects of the integration of AI and healthcare. Among them, the hot deep learning field has shown greater potential in applications such as disease prediction and drug response prediction. From the initial logistic regression model to the machine learning model, and then to the deep learning model today, the accuracy of medical disease prediction has been continuously improved, and the performance in all aspects has also been significantly improved. This article introduces some basic deep learning frameworks and some common diseases, and summarizes the deep learning prediction methods corresponding to different diseases. Point out a series of problems in the current disease prediction, and make a prospect for the future development. It aims to clarify the effectiveness of deep learning in disease prediction, and demonstrates the high correlation between deep learning and the medical field in future development. The unique feature extraction methods of deep learning methods can still play an important role in future medical research. © 2021 Tech Science Press. All rights reserved.\n",
            "----\n",
            "Paper 661:\n",
            "Title: An editorial of “AI + informetrics”: multi-disciplinary interactions in the era of big data\n",
            "Abstract: None\n",
            "----\n",
            "Paper 662:\n",
            "Title: Advances of geo-spatial intelligence at LIESMARS\n",
            "Abstract: ABSTRACT The enhancement of computing power, the maturity of learning algorithms, and the richness of application scenarios make Artificial Intelligence (AI) solution increasingly attractive when solving Geo-spatial Information Science (GSIS) problems. These include image matching, image target detection, change detection, image retrieval, and for generating data models of various types. This paper discusses the connection and synthesis between AI and GSIS in block adjustment, image search and discovery in big databases, automatic change detection, and detection of abnormalities, demonstrating that AI can integrate GSIS. Moreover, the concept of Earth Observation Brain and Smart Geo-spatial Service (SGSS) is introduced in the end, and it is expected to promote the development of GSIS into broadening applications.\n",
            "----\n",
            "Paper 663:\n",
            "Title: Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR)\n",
            "Abstract: Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.\n",
            "----\n",
            "Paper 664:\n",
            "Title: LGHAP v2: a global gap-free aerosol optical depth and PM2.5 concentration dataset since 2000 derived via big Earth data analytics\n",
            "Abstract: Abstract. The Long-term Gap-free High-resolution Air Pollutants (LGHAP) concentration dataset generated in our previous study has provided spatially contiguous daily aerosol optical depth (AOD) and fine particulate matter (PM2.5) concentrations at a 1 km grid resolution in China since 2000. This advancement empowered unprecedented assessments of regional aerosol variations and their influence on the environment, health, and climate over the past 20 years. However, there is a need to enhance such a high-quality AOD and PM2.5 concentration dataset with new robust features and extended spatial coverage. In this study, we present version 2 of a global-scale LGHAP dataset (LGHAP v2), which was generated using improved big Earth data analytics via a seamless integration of versatile data science, pattern recognition, and machine learning methods. Specifically, multimodal AODs and air quality measurements acquired from relevant satellites, ground monitoring stations, and numerical models were harmonized by harnessing the capability of random-forest-based data-driven models. Subsequently, an improved tensor-flow-based AOD reconstruction algorithm was developed to weave the harmonized multisource AOD products together for filling data gaps in Multi-Angle Implementation of Atmospheric Correction (MAIAC) AOD retrievals from Terra. The results of the ablation experiments demonstrated better performance of the improved tensor-flow-based gap-filling method in terms of both convergence speed and data accuracy. Ground-based validation results indicated good data accuracy of this global gap-free AOD dataset, with a correlation coefficient (R) of 0.85 and a root mean square error (RMSE) of 0.14 compared to the worldwide AOD observations from the AErosol RObotic NETwork (AERONET), outperforming the purely reconstructed AODs (R = 0.83, RMSE = 0.15), but they were slightly worse than raw MAIAC AOD retrievals (R = 0.88, RMSE = 0.11). For PM2.5 concentration mapping, a novel deep-learning approach, termed the SCene-Aware ensemble learning Graph ATtention network (SCAGAT), was hereby applied. While accounting for the scene representativeness of data-driven models across regions, the SCAGAT algorithm performed better during spatial extrapolation, largely reducing modeling biases over regions with limited and/or even absent in situ PM2.5 concentration measurements. The validation results indicated that the gap-free PM2.5 concentration estimates exhibit higher prediction accuracies, with an R of 0.95 and an RMSE of 5.7 µg m−3, compared to PM2.5 concentration measurements obtained from former holdout sites worldwide. Overall, while leveraging state-of-the-art methods in data science and artificial intelligence, a quality-enhanced LGHAP v2 dataset was generated through big Earth data analytics by cohesively weaving together multimodal AODs and air quality measurements from diverse sources. The gap-free, high-resolution, and global coverage merits render the LGHAP v2 dataset an invaluable database for advancing aerosol- and haze-related studies as well as triggering multidisciplinary applications for environmental management, health-risk assessment, and climate change attribution. All gap-free AOD and PM2.5 concentration grids in the LGHAP v2 dataset, as well as the data user guide and relevant visualization codes, are publicly accessible at https://zenodo.org/communities/ecnu_lghap (last access: 3 April 2024, Bai and Li, 2023a).\n",
            "\n",
            "----\n",
            "Paper 665:\n",
            "Title: Real-world data mining meets clinical practice: Research challenges and perspective\n",
            "Abstract: As Big Data Analysis meets healthcare applications, domain-specific challenges and opportunities materialize in all aspects of data science. Advanced statistical methods and Artificial Intelligence (AI) on Electronic Health Records (EHRs) are used both for knowledge discovery purposes and clinical decision support. Such techniques enable the emerging Predictive, Preventative, Personalized, and Participatory Medicine (P4M) paradigm. Working with the Infectious Disease Clinic of the University Hospital of Modena, Italy, we have developed a range of Data–Driven (DD) approaches to solve critical clinical applications using statistics, Machine Learning (ML) and Big Data Analytics on real-world EHR. Here, we describe our perspective on the challenges we encountered. Some are connected to medical data and their sparse, scarce, and unbalanced nature. Others are bound to the application environment, as medical AI tools can affect people's health and life. For each of these problems, we report some available techniques to tackle them, present examples drawn from our experience, and propose which approaches, in our opinion, could lead to successful real-world, end-to-end implementations. DESY report number DESY-22-153.\n",
            "----\n",
            "Paper 666:\n",
            "Title: The application of AI techniques in requirements classification: a systematic mapping\n",
            "Abstract: None\n",
            "----\n",
            "Paper 667:\n",
            "Title: RDCGAN: Unsupervised Representation Learning With Regularized Deep Convolutional Generative Adversarial Networks\n",
            "Abstract: In Recent years, Representation learning as one of the information extraction and data mapping methods in machine learning systems has received huge attention. Artificial deep neural networks are considered as one of the basic structures capable of representation learning. However, a large number of standard representation learning methods are supervised and requires a lot of labeled data. In this paper, we introduce an unsupervised representation learning by designing and implementing deep neural networks (DNNs) in combination with Generative Adversarial Networks (GANs). The main idea behind the proposed method, which causes the superiority of this method over others is representation learning via the generative models and encoder networks altogether. In this research, encoders are utilized in addition to the generative models to help the more features to be extracted. It is shown that the proposed method not only help feature extraction but accelerate and improve the performance of the learning in GANs which lead to better feature extraction. The results confirm the superiority of the proposed approach regarding classification accuracy by 2% to 6% improvement over other unsupervised feature learning methods.\n",
            "----\n",
            "Paper 668:\n",
            "Title: Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering\n",
            "Abstract: Clustering is among the most fundamental tasks in machine learning and artificial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into an observable. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 5 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training.\n",
            "----\n",
            "Paper 669:\n",
            "Title: Project-Team CORTEX Neuromimetic Intelligence\n",
            "Abstract: the elaboration and analysis of neuromimetic connectionist two of of distributed models can internal representations, manipulate knowledge propose complementary approaches cross-fertilization. Machine connectionist numerical models for information processing in a statistical framework, extract knowledge from theoretical and elementary that at explaining how the human or animal nervous system at various levels, perception to Another important issue in connectionism is to take beneﬁt from the parallel distributed nature of its computation and to develop implementations that exploit those characteristics. As such implementations lead to cope with the real nature of neural computation, they may improve the performance of algorithms and be embedded in electronic devices ( cf. § 3.5). Artiﬁcial neural networks are information processing systems that can be widely applied to data mining. They have a lot of capabilities for analyzing and pre-processing data, as well as visualizing and extracting knowledge. These capabilities can be developed through unsupervised and supervised networks to obtain data analysis close to the one performed by statistical methods but with other interesting properties. To improve the performance of such information processing system, statistical or neuronal pre-processing can deal with missing data, redundancy information and outliers. their relation to some cognitive tasks. The main cognitive tasks we are interested in are related to the autonomous navigation of a robot in an unknown environment (perception, sensorimotor coordination, planning). The corresponding neuronal structures we are modeling are part of the cortex (perceptive, associative, frontal maps) and the limbic system (hippocampus, amygdala, basal ganglia). Corresponding models of these neuronal structures are deﬁned at the level of the population of neurons and functioning and learning rules are built from neuroscience data to emulate the corresponding information processing (ﬁltering in perceptive maps, multimodal association in associative maps, temporal organization of behavior in frontal maps, episodic memory in hippocampus, emotional conditioning in amygdala, selection of action in basal ganglia). Our goal is to iteratively reﬁne these models, implement them on autonomous robots and make them cooperate and exchange information, toward a completely adaptive, integrated and autonomous behavior. to analyze. For this reason our work focuses on the use of simpliﬁed models, i.e. simple phenomenological models of spiking neurons, that try to capture the dynamic behavior of the neuron in leaky integrators that explain how spikes can be emitted through time from input integration. These models are interesting for several reasons. From a neuroscience point of view, they allow a better understanding of neuronal functioning. Indeed, although it is well known that real neurons communicate with spikes, i.e. a short electrical pulse called action potential, the precise nature of the neural code is a topic of intense debate. The ﬁring-rate coding hypothesis stating that the ﬁring frequency of a neuron estimated by temporal averaging encodes information is now challenged by a number of recent studies showing that precise spike timing is a signiﬁcant element in neural encoding. In particular, stimulus-induced synchronization and oscillatory patterning of spike trains have been experimentally observed in perceptive systems like in vision or olfaction. Moreover, synchronization of neural activities seems to play a role in olfactory perception; for example, when the synchronization is pharmacologically abolished, honeybees do not discriminate anymore between similar odors. From a computer science point of view, we investigate the spatio-temporal dynamics of simpliﬁed models of spiking networks using both mathematical analysis and numerical simulations. Therefore, we have to deﬁne (i) a tractable mathematical analysis with methods coming from the theory of nonlinear dynamical systems and (ii) an efﬁcient computing scheme with either event-driven or time-driven simulation engines. These models can also be applied to difﬁcult coding tasks for machine perception like vision and olfaction, and can help to understand how sensory information is encoded and processed by biological neural networks. very ﬁne-grain massive parallelism with densely interconnected computation units. The connectionist paradigm is the foundation of the robust, adaptive, embeddable and autonomous pro-cessings that we develop in our team. Therefore their speciﬁc massive parallelism has to be fully exploited. Furthermore, we use this intrinsic parallelism as a guideline to develop new models and algorithms for which parallel implementations are naturally made easier. Our approach claims that the parallelism of connectionist models makes them able to deal with strong implementation and application constraints. This claim is based on both theoretical and practical properties of neural networks. It is related to a very ﬁne parallelism grain that ﬁts parallel hardware devices, as well as to the emergence of very large reconﬁgurable systems that become able to handle both adaptability and massive parallelism of neural networks. More particularly, digital reconﬁgurable circuits (e.g. FPGA, Field Programmable Gate Arrays) stand as the most suitable and ﬂexible device for fully parallel implementations of neural models, according to numerous recent studies in the connectionist community. We carry out various arithmetical and topological studies that are required by the implementation of several neural models onto FPGAs, as well as the deﬁnition of hardware-targetted neural models of parallel computation. e.g. deﬁned as complex ﬁnite state machines. The time-driven simulator engine called SIRENE was written in C and developed for the simulation of a model of the antennal lobe, the ﬁrst structure of the insect olfactory system. This simulator engine can simulate any type of spiking neural network and is indeed more dedicated to the simulation of biologically detailed models of neurons —such as conductance-based neurons— and synapses. Its high ﬂexibility allows the user to implement easily any type of neuronal or synaptic model and use the appropriate numerical integration routine (e.g. Runge-Kutta at given order). In the context of an application to large networks ( cf. § 7.2), we developed this year a graphical interface to visualize the spikes and the evolution of our networks. same data with regard to different media. The MicroNOMAD-MultiSOM software has been used on different operational applications of data mining [30] ( cf. § 7.3 and § 7.4). The versions 1 and 2 of this software have been patented by INRIA. The MicroNOMAD-MultiSOM software has been chosen as one of the two softwares of reference for analyzing Web data in the framework of the European EICSTES project ( cf. § 7.3). The version 3 of this software that offers numerous extensions for analyzing unstructured data as well as numerous functions for the automation of analysis is currently ﬁnalized in the project. Many different analyses have been conducted with the MicroNOMAD-MultiSOM software in the framework of this project. Comparison of the results with the ones that have been obtained with the other software selected in the project have also been performed [9]. A new extension of the MicroNOMAD-MultiSOM is currently under construction. It consists in adding an automatic rule extraction module.\n",
            "----\n",
            "Paper 670:\n",
            "Title: Research on the dynamic application of trademark similarity determination based on dynamic time regularization and XGBoost\n",
            "Abstract: In order to improve the possibility of successful trademark registration in the tobacco industry and monitor the problem of whether the 34 categories of trademarks are preempted, this paper conducts an experimental design based on dynamic time regularization and XGBoost algorithm. The main innovation points are firstly, the similarity is calculated by the Dynamic Time Wrangling (DTW) algorithm by combining the information of text structure and location, and also as a comparison with the accuracy of machine learning prediction results; secondly, both Chinese similarity and English similarity are incorporated into the training set as input features; thirdly, the weekly updated data training results are checked and used as the test set to enhance the accuracy and adaptability of the model. This experiment is applied to automatically output the determination of trademark similarity results, and then take measures to adjust the trademark design or raise objections to the infringing trademark after obtaining the similarity results, which greatly reduces the workload of manual comparison and improves the accuracy rate of comparison at the same time. The experimental design of this paper evaluates the accuracy rate, and the experimental results are in the acceptable range, which can be applied to the whole tobacco industry for screening similar brand names, and can be extended to other text similarity determinations, and this study can continue to optimize the extraction of more features as input, which will be helpful in providing accuracy rate.\n",
            "----\n",
            "Paper 671:\n",
            "Title: Expl(AI)n It to Me – Explainable AI and Information Systems Research\n",
            "Abstract: None\n",
            "----\n",
            "Paper 672:\n",
            "Title: Smart Seed Classification System based on MobileNetV2 Architecture\n",
            "Abstract: The agricultural transformation in the last decade using artificial intelligence has led to significant gains in productivity and profitability. The traditional machine learning approaches present inherent limitations in extracting features and information from image data. Deep learning techniques, particularly CNN's, help to overcome these limitations due to their multi-level architecture. Various deep learning applications in agriculture include crop disease identification, fruit classification, and germination rate monitoring. Seed image analysis is considered a significant task for the preservation of biodiversity and sustainability. This research uses MobileNetV2, a deep learning convolutional neural network (DCNNs) for seed classification. This model has been preferred due to its simple architecture and memory-efficient characteristics. A total of 14 different classes of seeds were used for the experimentation. The results indicate accuracies of 98% and 95% on training and test sets, respectively.\n",
            "----\n",
            "Paper 673:\n",
            "Title: The Role of AI Model Documentation in Translational Science: A Scoping Review\n",
            "Abstract: Background: Translation of artificial intelligence/machine learning (AI/ML)-based medical modeling software (MMS) into clinical settings requires rigorous evaluation by interdisciplinary teams and across the AI lifecycle. The fragmented nature of available resources to support MMS documentation limits the transparent reporting of scientific evidence to support MMS, creating barriers and impeding the translation of software from code to bedside. Objective: The aim of this paper is to scope AI/ML-based MMS documentation practices and define the role of documentation in facilitating safe and ethical MMS translation into clinical workflows. Methods: A scoping review was conducted in accordance with PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. MEDLINE (PubMed) was searched using MeSH key concepts of AI/ML, ethical considerations, and explainability to identify publications detailing AI/ML-based MMS documentation, in addition to snowball sampling of selected reference lists. To include the possibility of implicit documentation practices not explicitly labeled as such, we did not use \"documentation\" as a key concept but rather as an inclusion criterion. A two-stage screening process (title and abstract screening and full-text review) was conducted by an independent reviewer. A data extraction template was utilized to record publication-related information, barriers to developing ethical and explainable MMS, available standards, regulations, frameworks, or governance strategies related to documentation, and recommendations for documentation for papers that met inclusion criteria. Results: Of the total 115 papers, 21 (18%) articles met the requirements for inclusion. Data regarding the current state and challenges of AI/ML-based documentation was synthesized and themes including bias, accountability, governance, and interpretability were identified. Conclusions: Our findings suggest that AI/ML-based MMS documentation practice is siloed across the AI life cycle and there exists a gray area for tracking and reporting of non-regulated MMS. Recommendations from the literature call for proactive evaluation, standards, frameworks, and transparency and traceability requirements to address ethical and explainability barriers, enhance documentation efforts, provide support throughout the AI lifecycle, and promote translation of MMS. If prioritized across multidisciplinary teams and across the AI lifecycle, AI/ML-based MMS documentation may serve as a method of coordinated communication and reporting toward resolution of AI translation barriers related to bias, accountability, governance, and interpretability.\n",
            "----\n",
            "Paper 674:\n",
            "Title: Knowledge Engineering Paradigms for Smart Education and Learning Systems\n",
            "Abstract: Recently, artificial intelligence (AI) receive increasing attention within the field of developing smart digital education. Researchers have been used the computational intelligence (CI) and machine learning techniques methodologies to develop a smart tutoring systems (STSs). On the other side, the convergence of AI, data science and Internet of Things (IoT) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the CI and knowledge engineering paradigms for developing the smart educational and learning systems. In this study the two popular CI paradigms; case-based reasoning and ontological engineering are discussed and analyzed namely. The main objective of this study is to determine and exploration the benefits and advantages of such intelligent paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems.\n",
            "----\n",
            "Paper 675:\n",
            "Title: Guest editorial: Machine learning in and for music\n",
            "Abstract: None\n",
            "----\n",
            "Paper 676:\n",
            "Title: Using satellite imagery to understand and promote sustainable development\n",
            "Abstract: Satellite monitoring of development Recent years have witnessed rapid growth in satellite-based approaches to quantifying aspects of land use, especially those monitoring the outcomes of sustainable development programs. Burke et al. reviewed this recent progress with a particular focus on machine-learning approaches and artificial intelligence methods. Drawing on examples mostly from Africa, they conclude that satellite-based methods enhance rather than replace ground-based data collection, and progress depends on a combined approach. Science, this issue p. eabe8628 BACKGROUND Accurate and comprehensive measurements of a range of sustainable development outcomes are fundamental inputs into both research and policy. For instance, good measures are needed to monitor progress toward sustainability goals and evaluate interventions designed to improve development outcomes. Traditional approaches to measurement of many key outcomes rely on household surveys that are conducted infrequently in many parts of the world and are often of low accuracy. The paucity of ground data stands in contrast to the rapidly growing abundance and quality of satellite imagery. Multiple public and private sensors launched in recent years provide temporal, spatial, and spectral information on changes happening on Earth’s surface. Here we review a rapidly growing scientific literature that seeks to use this satellite imagery to measure and understand various outcomes related to sustainable development. We pay particular attention to recent approaches that use methods from artificial intelligence to extract information from images, as these methods typically outperform earlier approaches and enable new insights. Our focus is on settings and applications where humans themselves, or what they produce, are the outcome of interest and on where these outcomes are being measured using satellite imagery. ADVANCES We describe and synthesize the variety of approaches that have been used to extract information from satellite imagery, with particular attention given to recent machine learning–based approaches and settings in which training data are limited or noisy. We then quantitatively assess predictive performance of these approaches in the domains of smallholder agriculture, economic livelihoods, population, and informal settlements. We show that satellite-based performance in predicting these outcomes is reasonably strong and improving. Performance improvements have come through a combination of more numerous and accurate training data, more abundant and higher-quality imagery, and creative application of advances in computer vision to satellite inputs and sustainability outcomes. Further, our analyses suggest that reported model performance likely understates true performance in many settings, given the noisy data on which predictions are evaluated and the types of noise typically observed in sustainability applications. For multiple outcomes of interest, satellite-based estimates can now equal or exceed the accuracy of traditional approaches to outcome measurement. We describe multiple methods through which the true performance of satellite-based approaches can be better understood. Integration of satellite-based sustainability measurements into research has been broad, and we describe applications in agriculture, fisheries, health, and economics. Documented uses of these measurements in public-sector decision-making are rarer, which we attribute in part to the novelty of the approaches, their lack of interpretability, and the potential benefits to some policy-makers of not having certain outcomes be measured. OUTLOOK The largest constraint to satellite-based model performance is now training data rather than imagery. While imagery has become abundant, the scarcity and frequent unreliability of ground data make both training and validation of satellite-based models difficult. Expanding the quantity and quality of such data will quickly accelerate progress in this field. Other opportunities for advancement include improvements in model interpretability, fusion of satellites with other nontraditional data that provide complementary information, and more-rigorous evaluation of satellite-based approaches (relative to available alternatives) in the context of specific use cases. Nevertheless, despite the current and future promise of satellite-based approaches, we argue that these approaches will amplify rather than replace existing ground-based data collection efforts in most settings. Many outcomes of interest will likely never be accurately estimated with satellites; for outcomes where satellites do have predictive power, high-quality local training data can nearly always improve model performance. Increasing collection of satellite imagery can help measure livelihood outcomes in areas where ground data are sparse. (Left) Interval between nationally representative economic surveys over the past three decades shows long lags in many developing countries. (Middle) Recently added public and private satellites have broken the traditional trade-off between temporal and spatial resolution. (Right) Performance in measuring the presence of informal settlements, crop yields on smallholder agricultural plots, and village-level asset wealth. R2, coefficient of determination. Accurate and comprehensive measurements of a range of sustainable development outcomes are fundamental inputs into both research and policy. We synthesize the growing literature that uses satellite imagery to understand these outcomes, with a focus on approaches that combine imagery with machine learning. We quantify the paucity of ground data on key human-related outcomes and the growing abundance and improving resolution (spatial, temporal, and spectral) of satellite imagery. We then review recent machine learning approaches to model-building in the context of scarce and noisy training data, highlighting how this noise often leads to incorrect assessment of model performance. We quantify recent model performance across multiple sustainable development domains, discuss research and policy applications, explore constraints to future progress, and highlight research directions for the field.\n",
            "----\n",
            "Paper 677:\n",
            "Title: AI Assistants: A Framework for Semi-Automated Data Wrangling\n",
            "Abstract: Data wrangling tasks such as obtaining and linking data from various sources, transforming data formats, and correcting erroneous records, can constitute up to 80% of typical data engineering work. Despite the rise of machine learning and artificial intelligence, data wrangling remains a tedious and manual task. We introduce AI assistants, a class of semi-automatic interactive tools to streamline data wrangling. An AI assistant guides the analyst through a specific data wrangling task by recommending a suitable data transformation that respects the constraints obtained through interaction with the analyst. We formally define the structure of AI assistants and describe how existing tools that treat data cleaning as an optimization problem fit the definition. We implement AI assistants for four common data wrangling tasks and make AI assistants easily accessible to data analysts in an open-source notebook environment for data science, by leveraging the common structure they follow. We evaluate our AI assistants both quantitatively and qualitatively through three example scenarios. We show that the unified and interactive design makes it easy to perform tasks that would be difficult to do manually or with a fully automatic tool.\n",
            "----\n",
            "Paper 678:\n",
            "Title: Simulation of Self-driving Car using Deep Learning\n",
            "Abstract: The rapid development of Artificial Intelligence has revolutionized the area of autonomous vehicles by incorporating complex models and algorithms. Self-driving cars are always one of the biggest inventions in computer science and robotic intelligence. Highly robust algorithms that facilitate the functioning of these vehicles will reduce many problems associated with driving such as the drunken driver problem. In this paper our aim is to build a Deep Learning model that can drive the car autonomously which can adapt well to the real-time tracks and does not require any manual feature extraction. This research work proposes a computer vision model that learns from video data. It involves image processing, image augmentation, behavioural cloning and convolutional neural network model. The neural network architecture is used to detect path in a video segment, linings of roads, locations of obstacles, and behavioural cloning is used for the model to learn from human actions in the video.\n",
            "----\n",
            "Paper 679:\n",
            "Title: Heart Disease Prediction using CNN, Deep Learning Model\n",
            "Abstract: — Heart disease is one of the most serious health threat growing among worldwide, for which mortality rate around the world is very high. Early detection of heart disease could save many lives, accurate detection of heart disease is crucial among the health care persons through regular clinical data and its analysis. Artificial intelligence is the effective solution for decision making and accurate heart disease predictions. Medical industry showing enormous development in using information technology, in which artificial intelligence play major role. In the proposed work, deep learning based approach on heart disease is done on Cleveland dataset. However existing studies are handled in Machine learning technique. The proposed work detects heart disease based in Convolutional Neural Networks. Experimental results shows our proposed work achieves high level of accuracy in prediction of heart disease.\n",
            "----\n",
            "Paper 680:\n",
            "Title: Knowledge discovery in manufacturing datasets using data mining techniques to improve business performance\n",
            "Abstract: Recently due to the explosion in the data field, there is a great interest in the data science areas such as big data, artificial intelligence, data mining, and machine learning. Knowledge gives control and power in numerous manufacturing areas. Companies, factories, and all organizations owners aim to benefit from their huge; recorded data that increases and expands very quickly to improve their business and improve the quality of their products. In this research paper, the knowledge discovery in databases (KDD) technique has been followed, “association rules” algorithms “Apriori algorithm”, and “chi-square automatic interaction detection (CHAID) analysis tree” have been applied on real datasets belonging to (Emisal factory). This factory annually loses tons of production due to the breakdowns that occur daily inside the factory, which leads to a loss of profit. After analyzing and understanding the factory product processes, we found some breakdowns occur a lot of days during the product lifecycle, these breakdowns affect badly on the production lifecycle which led to a decrease in sales. So, we have mined the data and used the mentioned methods above to build a predictive model that will predict the breakdown types and help the factory owner to manage the breakdowns risks by taking accurate actions before the breakdowns happen.\n",
            "----\n",
            "Paper 681:\n",
            "Title: Knowledge graph construction and application in geosciences: A review\n",
            "Abstract: Knowledge graph (KG) is a topic of great interests to geoscientists as it can be deployed throughout the data life cycle in data-intensive geoscience studies. Nevertheless, comparing with the large amounts of publications on machine learning applications in geosciences, summaries and reviews of geoscience KGs are still limited. The aim of this paper is to present a comprehensive review of KG construction and implementation in geosciences. It consists of four major parts: 1) concepts relevant to KG and approaches for KG construction, 2) KG application in data collection, curation, and service, 3) KG application in data analysis, and 4) challenges and trends of geo-science KG creation and application in the near future. For each of the first three parts, a list of concepts, exemplar studies, and best practices are summarized. Those summaries are synthesized together in the challenge and trend analyses. As artificial intelligence and data science are thriving in geosciences, we hope this review of geoscience KGs can be of value to practitioners in data-intensive geoscience studies.\n",
            "----\n",
            "Paper 682:\n",
            "Title: A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences\n",
            "Abstract: None\n",
            "----\n",
            "Paper 683:\n",
            "Title: D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks\n",
            "Abstract: None\n",
            "----\n",
            "Paper 684:\n",
            "Title: Technology Laboratories: Facilitating Instruction for Cyberinfrastructure Infused Data Sciences\n",
            "Abstract: While artificial intelligence and machine learning (AI/ML) frameworks gain prominence in science and engineering, most researchers face significant challenges in adopting complex AI/ML workflows to campus and national cyberinfrastructure (CI) environments. Data from the Texas A&M High Performance Computing (HPRC) researcher training program indicate that researchers increasingly want to learn how to migrate and work with their pre-existing AI/ML frameworks on large scale computing environments. Building on the continuing success of our work in developing innovative pedagogical approaches for CI-training approaches, we expand CI-infused pedagogical approaches to teach technology-based AI and data sciences. We revisit the pedagogical approaches used in the decades-old tradition of laboratories in the Physical Sciences that taught concepts via experiential learning. Here, we structure a series of exercises on interactive computing environments that give researchers immediate hands-on experience in AI/ML and data science technologies that they will use as they work on larger CI resources. These exercises, called “tech-labs,” assume that participating researchers are familiar with AI/ML approaches and focus on hands-on exercises that teach researchers how to use these approaches on large-scale CI. The tech-labs offer four consecutive sessions, each introducing a learner to specific technologies offered in CI environments for AI/ML and data workflows. We report on our tech-lab offered for Python-based AI/ML approaches during which learners are introduced to Jupyter Notebooks followed by exercises using Pandas, Matplotlib, Scikit-learn, and Keras. The program includes a series of enhancements such as container support and easy launch of virtual environments in our Web-based computing interface. The approach is scalable to programs using a command line interface (CLI) as well. In all, the program offers a shift in focus from teaching AI/ML toward increasing adoption of AI/ML in large-scale CI.\n",
            "----\n",
            "Paper 685:\n",
            "Title: Machine Learning Techniques in Bioinformatics\n",
            "Abstract: In the post-genome era, research in bioinformatics has been overwhelmed by the experimental data. Due to continued research, there is a continuous growth in the amount of biological data available and current biological databases are populated by vast experimental data. The exponential growth of these data available, raises two problems: on one hand, efficient information storage and management and, on the other hand, the extraction of useful information from these data. The second problem is one of the main challenges in computational biology, which requires the development of tools and methods capable of transforming all these heterogeneous data into biological knowledge about the underlying mechanism. The complexity of biological data ranges from simple strings (nucleotides and amino acids sequences) to complex graphs (biochemical networks); from 1-Dimentional (sequence data) to 3-Dimentional (protein and RNA structures). Considering the amount and complexity of the data, it is becoming impossible for an expert to compute and compare the entries within the current databases. Machine learning techniques are increasingly being used to address problems in computational biology and bioinformatics. Novel computational techniques to analyze high throughput data in the form of sequences, gene and protein expressions, pathways, and images are becoming vital for understanding diseases and future drug discovery. Machine learning techniques such as Markov models, support vector machines, neural networks, and graphical models have been successful in analyzing life science data because of their capabilities in handling randomness and uncertainty of data noise and in generalization. Thus, machine learning and artificial intelligence techniques have been widely applied in this domain to discover and mine the knowledge in the databases and has indeed gained a lot of success in this research area. At present, with various learning algorithms available in the literature, researchers are facing difficulties in choosing the best method that can apply to their data. Machine Learning in Bioinformatics is an indispensable resource for computer scientists, engineers, biologists, mathematicians, researchers, clinicians, physicians, and medical informaticists.\n",
            "----\n",
            "Paper 686:\n",
            "Title: A novel hybrid paper recommendation system using deep learning\n",
            "Abstract: None\n",
            "----\n",
            "Paper 687:\n",
            "Title: A Review on Basic Deep Learning Technologies and Applications\n",
            "Abstract: None\n",
            "----\n",
            "Paper 688:\n",
            "Title: A detailed study of interpretability of deep neural network based top taggers\n",
            "Abstract: Recent developments in the methods of explainable artificial intelligence (XAI) allow researchers to explore the inner workings of deep neural networks (DNNs), revealing crucial information about input–output relationships and realizing how data connects with machine learning models. In this paper we explore interpretability of DNN models designed to identify jets coming from top quark decay in high energy proton–proton collisions at the Large Hadron Collider. We review a subset of existing top tagger models and explore different quantitative methods to identify which features play the most important roles in identifying the top jets. We also investigate how and why feature importance varies across different XAI metrics, how correlations among features impact their explainability, and how latent space representations encode information as well as correlate with physically meaningful quantities. Our studies uncover some major pitfalls of existing XAI methods and illustrate how they can be overcome to obtain consistent and meaningful interpretation of these models. We additionally illustrate the activity of hidden layers as neural activation pattern diagrams and demonstrate how they can be used to understand how DNNs relay information across the layers and how this understanding can help to make such models significantly simpler by allowing effective model reoptimization and hyperparameter tuning. These studies not only facilitate a methodological approach to interpreting models but also unveil new insights about what these models learn. Incorporating these observations into augmented model design, we propose the particle flow interaction network model and demonstrate how interpretability-inspired model augmentation can improve top tagging performance.\n",
            "----\n",
            "Paper 689:\n",
            "Title: Rapid Prototyping of a Text Mining Application for Cryptocurrency Market Intelligence\n",
            "Abstract: Blockchain represents a technology for establishing a shared, immutable version of the truth between a network of participants that do not trust one another, and therefore has the potential to disrupt any financial or other industries that rely on third-parties to establish trust. In order to better understand the current ecosystem of Blockchain applications, a scalable proof-of-concept pipeline for analysis of multiple streams of semi-structured data posted on social media is demonstrated, based on open source components. Deep Web as well as conventional social media are considered. Preliminary analysis suggests that data found in the Deep Web is complimentary to that available on the conventional web. Future work is described that will scale the system to cloud-based, real-time, analysis of multiple data streams, with Information Extraction (IE) (ex. sentiment analysis) and Machine Learning capability.\n",
            "----\n",
            "Paper 690:\n",
            "Title: Intelligent query optimization and course recommendation during online lectures in E-learning system\n",
            "Abstract: None\n",
            "----\n",
            "Paper 691:\n",
            "Title: PRICAI 2008: Trends in Artificial Intelligence, 10th Pacific Rim International Conference on Artificial Intelligence, Hanoi, Vietnam, December 15-19, 2008. Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 692:\n",
            "Title: Challenges and Opportunities in Deep Reinforcement Learning With Graph Neural Networks: A Comprehensive Review of Algorithms and Applications\n",
            "Abstract: Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation systems, and gaming. Similarly, graph neural networks (GNNs) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This article provides a comprehensive review of these hybrid works. These works can be classified into two categories: 1) algorithmic contributions, where DRL and GNN complement each other with an objective of addressing each other’s shortcomings and 2) application-specific contributions that leverage a combined GNN-DRL formulation to address problems specific to different applications. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community.\n",
            "----\n",
            "Paper 693:\n",
            "Title: Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency\n",
            "Abstract: A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artificial intelligence research is to ascertain whether two or more decision makers (e.g. brains or algorithms) use the same strategy. Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are at or near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recognition. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition if we want to ascertain similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying error consistency to visual object recognition we obtain three main findings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone--indicating that humans and CNNs are likely implementing very different strategies (3.) CORnet-S, a recurrent model termed the \"current best model of the primate ventral visual stream\", fails to capture essential characteristics of human behavioural data and behaves essentially like a ResNet-50 in our analysis--that is, just like a standard feedforward network. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different--but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress.\n",
            "----\n",
            "Paper 694:\n",
            "Title: Accelerating COVID-19 research with graph mining and transformer-based learning\n",
            "Abstract: In 2020, the White House released the, “Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset,” wherein artificial intelligence experts are asked to collect data and develop text mining techniques that can help the science community answer high-priority scientific questions related to COVID-19. The Allen Institute for AI and collaborators announced the availability of a rapidly growing open dataset of publications, the COVID-19 Open Research Dataset (CORD-19). As the pace of research accelerates, biomedical scientists struggle to stay current. To expedite their investigations, scientists leverage hypothesis generation systems, which can automatically inspect published papers to discover novel implicit connections. We present an automated general purpose hypothesis generation systems AGATHA-C and AGATHA-GP for COVID-19 research. The systems are based on graph-mining and the transformer model. The systems are massively validated using retrospective information rediscovery and proactive analysis involving human-in-the-loop expert analysis. Both systems achieve high-quality predictions across domains (in some domains up to 0.97% ROC AUC) in fast computational time and are released to the broad scientific community to accelerate biomedical research. In addition, by performing the domain expert curated study, we show that the systems are able to discover on-going research findings such as the relationship between COVID-19 and oxytocin hormone. Reproducibility All code, details, and pre-trained models are available at https://github.com/IlyaTyagin/AGATHA-C-GP CCS CONCEPTS • Applied computing → Bioinformatics; Document management and text processing; • Computing methodologies → Learning latent representations; Neural networks; Information extraction; Semantic networks.\n",
            "----\n",
            "Paper 695:\n",
            "Title: Digital humanities și revoluția inteligenței artificiale\n",
            "Abstract: Digital Humanities (DH) is a scientific domain, albeit an elusive one. Broadly, DH studies digital cultural objects in a computational way. There is no commonly accepted definition. The challenge in defining DH stems from the wide variety of disciplines involved, like cultural heritage, linguistics, literature, digital archaeology, history, arts, philosophy, etc. that all have their own methodology and tools. A common misunderstanding is that the DH is concerned only with the conversion of physical objects like manuscripts, maps, cultural artefacts, sounds, into a machine-readable format. But this is only the first step: ob¬taining the object of study. The most challenging part is the computational processing of the digital objects by operations such as interrogation, editing, annotating, visualization, Data Mining, automatic classification, clustering, pattern recognition, information extraction, etc. Only a decade ago, all of these could have been performed exclusively by scholars with computer science background, but now the tendency is to develop user-friendly tools, or use prompt engineering for large language models, to make it easy for humanists to benefit from these computational analysis methods. In the last half of the decade, we witnessed a fourth industrial revolution, with the fulminating rise of Artificial Intelligence technologies. In this dynamic context, the role of DH scholars increases. Smart, creative, adaptive, and visionary digital humanists are needed to use AI systems to benefit people. In short, we need digital literacy on a large scale and adapting to continuous learning. An example of such good practice is the array of dissertation topics proposed by students graduating the Digital Humanities master Program at FLLS, like the computational analysis of the discourses of the last four Romanian presidents, the automatic detection and classification of mental illnesses from social media posts, the diachronic analysis of semantic shift of gender representation in corpora, the comparative semi-automatic analysis of literary cur¬rents, the automatic classification and sentiment analysis on a textual corpus of dreams, the comparison of human and AI-generated fanfiction texts, or testing the verbal creativity of large language models and compare it with human performance, etc. Pannapacker predicted as soon as 2012 that “It won’t be long until the Digital Humanities are, quite simply, The Humanities”. That moment has already passed.\n",
            "----\n",
            "Paper 696:\n",
            "Title: CDCEO'21 - First Workshop on Complex Data Challenges in Earth Observation\n",
            "Abstract: High-resolution remote sensing technology for Earth Observation (EO) has radically changed how we monitor the state of our planet around the clock. An effective interpretation of the resulting complex large-scale time series adopts the best machine learning techniques from signal processing, computer vision, pattern recognition, and artificial intelligence. The First Workshop on Complex Data Challenges in Earth Observation was open to both method development and advanced applications in a wide range of related topics, including image and signal processing, gap-filling, data fusion, feature extraction, prediction of spatio-temporal features, and the detection of rules underlying the observed state transitions and causal relationships. The full agenda, featuring keynotes and a selection of high quality contributed talks is available online at www.iarai.ac.at/cdceo21\n",
            "----\n",
            "Paper 697:\n",
            "Title: Research and Design of ERP System for Small and Medium-sized Enterprises under Great Intelligence Mobile Cloud\n",
            "Abstract: Based on the information management needs of SMEs, the article adopts the rapid prototyping method, designs the overall framework of the cloud ERP system, maps out the functional modules, adopts the B/S architecture, uses Java as the development tool, and SQL Server as the background server. The cloud computing and APP technologies are used to realize instant access between the PC and the mobile terminal, and the cloud ERP system with relatively complete system functions, simple interface and mobile operation is realized. The system incorporates new technologies such as big data analytics, artificial intelligence, machine learning and data mining to enable it to realize intelligent financial analysis, intelligent financial forecasting and intelligent financial decisions.\n",
            "----\n",
            "Paper 698:\n",
            "Title: Capstone Projects and their Transition into the Software Development Industry: A 10 Year Systematic Review of Literature\n",
            "Abstract: This study sought to examine the existing software engineering and Information Technology practices covered in final-year capstone projects in higher institutions of learning around the world, and the possibility of commercializing products developed from these projects. The researchers undertook a Systematic Literature Review methodology. The researchers used Artificial Intelligence (A.I.) tools, including Litmaps and Connected Papers for literature identification, while Quillbot and Scholarly A.I. were used for summary writing. Manual full paper review, data extraction and synthesis, and quantitative analyses were done on the final dataset using Microsoft Word, Excel, and Rayyan.ai. Eighty-eight papers from ACM Library, IEEE, Science Direct, EBSCOHost, Emerald Insight, Taylor & Francis, and Google Scholar were reviewed. Most papers were from Western countries, with only a few from Africa. Findings indicate a remarkable movement from traditional software engineering practices such as Waterfall to modern agile methods such as Scrum and Kaban. There was an increased use of Generative A.I. and Machine learning platforms such as GitHub and Arduino in projects profiled. Common scripting languages used by students were Java and Python. Most projects were undertaken in groups, with no prospective client. Most capstone courses were purely academic endeavours, and only a few were ever commercialized and transformed into industry products. One limitation of the review is that it includes grey literature from conferences and student theses. The study provides insights into State-of-the-Art approaches that can be adopted by higher institutions of learning to improve capstone courses.\n",
            "----\n",
            "Paper 699:\n",
            "Title: Adversarial XAI Methods in Cybersecurity\n",
            "Abstract: Machine Learning methods are playing a vital role in combating ever-evolving threats in the cybersecurity domain. Explanation methods that shed light on the decision process of black-box classifiers are one of the biggest drivers in the successful adoption of these models. Explaining predictions that address ‘Why?/Why Not?’ questions help users/stakeholders/analysts understand and accept the predicted outputs with confidence and build trust. Counterfactual explanations are gaining popularity as an alternative method to help users to not only understand the decisions of black-box models (why?) but also to provide a mechanism to highlight mutually exclusive data instances that would change the outcomes (why not?). Recent Explainable Artificial Intelligence literature has focused on three main areas: (a) creating and improving explainability methods that help users better understand how the internal of ML models work as well as their outputs; (b) attacks on interpreters with a white-box setting; (c) defining the relevant properties, metrics of explanations generated by models. Nevertheless, there is no thorough study of how the model explanations can introduce new attack surfaces to the underlying systems. A motivated adversary can leverage the information provided by explanations to launch membership inference, and model extraction attacks to compromise the overall privacy of the system. Similarly, explanations can also facilitate powerful evasion attacks such as poisoning and back door attacks. In this paper, we cover this gap by tackling various cybersecurity properties and threat models related to counterfactual explanations. We propose a new black-box attack that leverages Explainable Artificial Intelligence (XAI) methods to compromise the confidentiality and privacy properties of underlying classifiers. We validate our approach with datasets and models used in the cyber security domain to demonstrate that our method achieves the attacker’s goal under threat models which reflect the real-world settings.\n",
            "----\n",
            "Paper 700:\n",
            "Title: S5ELBP: Supervised SAR Image Classification using SVM from SSELBP Texture Features\n",
            "Abstract: Synthetic Aperture Radar (SAR) and Optical sensors are very popular depending on their strengths and application needs. The ocean surface will be covered with clouds most of the time and optical sensors cannot capture the information as they cannot operate in dust and cloudy conditions. So, SAR sensors come into existence to capture the atmospheric phenomena in those regions. There is a wide range of texture extraction techniques such as traditional-based wavelet analysis, statistical analysis, local pixel-based approaches, and dimensional reduction approaches. However, these approaches have limitations in boundary extraction, high-dimensional feature extraction, and uniformity representation. In recent times, deep learning (DL) approaches to extract features which varies based on the applications, image interpretation, and so on. At the same time, DL approaches are limited in transparency, computation requirements, and hyperparameter tuning. In contrast, the proposed S5ELBP i.e., Supervised SAR image classification using the integration of Support Vector Machine (SVM) and Scale Selective Extended Local Binary Pattern (SSELBP) extraction techniques shall overcome various challenges of existing traditional and DL approaches such as adaptive thresholding, less computational resources, and noise robustness. Hence, the objective of the proposed methodology is to extract the image features from the Scale Selective Extended Local Binary Pattern (SSELBP). Also, the SSELBP features are integrated with SVM to show the strength of the proposed approach in SAR image classification. Additionally, the performance of the present work is enhanced by comparing the extracted results with various classifiers. Finally, the work is concluded by recommending the scope of future work.\n",
            "----\n",
            "Paper 701:\n",
            "Title: S5ELBP: Supervised SAR Image Classification using SVM from SSELBP Texture Features\n",
            "Abstract: Synthetic Aperture Radar (SAR) and Optical sensors are very popular depending on their strengths and application needs. The ocean surface will be covered with clouds most of the time and optical sensors cannot capture the information as they cannot operate in dust and cloudy conditions. So, SAR sensors come into existence to capture the atmospheric phenomena in those regions. There is a wide range of texture extraction techniques such as traditional-based wavelet analysis, statistical analysis, local pixel-based approaches, and dimensional reduction approaches. However, these approaches have limitations in boundary extraction, high-dimensional feature extraction, and uniformity representation. In recent times, deep learning (DL) approaches to extract features which varies based on the applications, image interpretation, and so on. At the same time, DL approaches are limited in transparency, computation requirements, and hyperparameter tuning. In contrast, the proposed S5ELBP i.e., Supervised SAR image classification using the integration of Support Vector Machine (SVM) and Scale Selective Extended Local Binary Pattern (SSELBP) extraction techniques shall overcome various challenges of existing traditional and DL approaches such as adaptive thresholding, less computational resources, and noise robustness. Hence, the objective of the proposed methodology is to extract the image features from the Scale Selective Extended Local Binary Pattern (SSELBP). Also, the SSELBP features are integrated with SVM to show the strength of the proposed approach in SAR image classification. Additionally, the performance of the present work is enhanced by comparing the extracted results with various classifiers. Finally, the work is concluded by recommending the scope of future work.\n",
            "----\n",
            "Paper 702:\n",
            "Title: The Spectrum of Big Data Analytics\n",
            "Abstract: ABSTRACT Big data analytics is playing a pivotal role in big data, artificial intelligence, management, governance, and society with the dramatic development of big data, analytics, artificial intelligence. However, what is the spectrum of big data analytics and how to develop the spectrum are still a fundamental issue in the academic community. This article addresses these issues by presenting a big data derived small data approach. It then uses the proposed approach to analyze the top 150 profiles of Google Scholar, including big data analytics as one research field and proposes a spectrum of big data analytics. The spectrum of big data analytics mainly includes data mining, machine learning, data science and systems, artificial intelligence, distributed computing and systems, and cloud computing, taking into account degree of importance. The proposed approach and findings will generalize to other researchers and practitioners of big data analytics, machine learning, artificial intelligence, and data science.\n",
            "----\n",
            "Paper 703:\n",
            "Title: Python for Data Analytics, Scientific and Technical Applications\n",
            "Abstract: Since the invention of computers or machines, their capability to perform various tasks has experienced an exponential growth. In the current times, data science and analytics, a branch of computer science, has revived due to the major increase in computer power, presence of huge amounts of data, and better understanding in techniques in the area of Data Analytics, Artificial Intelligence, Machine Learning, Deep Learning etc. Hence, they have become an essential part of the technology industry, and are being used to solve many challenging problems. In the search for a good programming language on which many data science applications can be developed, python has emerged as a complete programming solution. Due to the low learning curve, and flexibility of Python, it has become one of the fastest growing languages. Python’s ever-evolving libraries make it a good choice for Data analytics. The paper talks about the features and characteristics of Python programming language and later discusses reasons behind python being credited as one of the fastest growing programming language and why it is at the forefront of data science applications, research and development.\n",
            "----\n",
            "Paper 704:\n",
            "Title: Distributed Denial-of-Service Prediction on IoT Framework by Learning Techniques\n",
            "Abstract: Abstract Distributed denial-of-service (DDoS) attacks on the Internet of Things (IoT) pose a serious threat to several web-based networks. The intruder’s ability to deal with the power of various cooperating devices to instigate an attack makes its administration even more multifaceted. This complexity can be further increased while lots of intruders attempt to overload an attack against a device. To counter and defend against modern DDoS attacks, several effective and powerful techniques have been used in the literature, such as data mining and artificial intelligence for the intrusion detection system (IDS), but they have some limitations. To overcome the existing limitations, in this study, we propose an intrusion detection mechanism that is an integration of a filter-based selection technique and a machine learning algorithm, called information gain-based intrusion detection system (IGIDS). In addition, IGIDS selects the most relevant features from the original IDS datasets that can help to distinguish typical low-speed DDoS attacks and, then, the selected features are passed on to the classifiers, i.e. support vector machine (SVM), decision tree (C4.5), naïve Bayes (NB) and multilayer perceptron (MLP) to detect attacks. The publicly available datasets as KDD Cup 99, CAIDA DDOS Attack 2007, CONFICKER worm, and UNINA traffic traces, are used for our experimental study. From the results of the simulation, it is clear that IGIDS with C4.5 acquires high detection and accuracy with a low false-positive rate.\n",
            "----\n",
            "Paper 705:\n",
            "Title: Hybrid Deep Learning (hDL)-Based Brain-Computer Interface (BCI) Systems: A Systematic Review\n",
            "Abstract: Background: Brain-Computer Interface (BCI) is becoming more reliable, thanks to the advantages of Artificial Intelligence (AI). Recently, hybrid Deep Learning (hDL), which combines different DL algorithms, has gained momentum over the past five years. In this work, we proposed a review on hDL-based BCI starting from the seminal studies in 2015. Objectives: We have reviewed 47 papers that apply hDL to the BCI system published between 2015 and 2020 extracting trends and highlighting relevant aspects to the topic. Methods: We have queried four scientific search engines (Google Scholar, PubMed, IEEE Xplore and Elsevier Science Direct) and different data items were extracted from each paper such as the database used, kind of application, online/offline training, tasks used for the BCI, pre-processing methodology adopted, type of normalization used, which kind of features were extracted, type of DL architecture used, number of layers implemented and which optimization approach were used as well. All these items were then investigated one by one to uncover trends. Results: Our investigation reveals that Electroencephalography (EEG) has been the most used technique. Interestingly, despite the lower Signal-to-Noise Ratio (SNR) of the EEG data that makes pre-processing of that data mandatory, we have found that the pre-processing has only been used in 21.28% of the cases by showing that hDL seems to be able to overcome this intrinsic drawback of the EEG data. Temporal-features seem to be the most effective with 93.94% accuracy, while spatial-temporal features are the most used with 33.33% of the cases investigated. The most used architecture has been Convolutional Neural Network-Recurrent Neural Network CNN-RNN with 47% of the cases. Moreover, half of the studies have used a low number of layers to achieve a good compromise between the complexity of the network and computational efficiency. Significance: To give useful information to the scientific community, we make our summary table of hDL-based BCI papers available and invite the community to published work to contribute to it directly. We have indicated a list of open challenges, emphasizing the need to use neuroimaging techniques other than EEG, such as functional Near-Infrared Spectroscopy (fNIRS), deeper investigate the advantages and disadvantages of using pre-processing and the relationship with the accuracy obtained. To implement new combinations of architectures, such as RNN-based and Deep Belief Network DBN-based, it is necessary to better explore the frequency and temporal-frequency features of the data at hand.\n",
            "----\n",
            "Paper 706:\n",
            "Title: Transformer technology in molecular science\n",
            "Abstract: A transformer is the foundational architecture behind large language models designed to handle sequential data by using mechanisms of self‐attention to weigh the importance of different elements, enabling efficient processing and understanding of complex patterns. Recently, transformer‐based models have become some of the most popular and powerful deep learning (DL) algorithms in molecular science, owing to their distinctive architectural characteristics and proficiency in handling intricate data. These models leverage the capacity of transformer architectures to capture complex hierarchical dependencies within sequential data. As the applications of transformers in molecular science are very widespread, in this review, we only focus on the technical aspects of transformer technology in molecule domain. Specifically, we will provide an in‐depth investigation into the algorithms of transformer‐based machine learning techniques in molecular science. The models under consideration include generative pre‐trained transformer (GPT), bidirectional and auto‐regressive transformers (BART), bidirectional encoder representations from transformers (BERT), graph transformer, transformer‐XL, text‐to‐text transfer transformer, vision transformers (ViT), detection transformer (DETR), conformer, contrastive language‐image pre‐training (CLIP), sparse transformers, and mobile and efficient transformers. By examining the inner workings of these models, we aim to elucidate how their architectural innovations contribute to their effectiveness in processing complex molecular data. We will also discuss promising trends in transformer models within the context of molecular science, emphasizing their technical capabilities and potential for interdisciplinary research. This review seeks to provide a comprehensive understanding of the transformer‐based machine learning techniques that are driving advancements in molecular science.This article is categorized under:\n",
            "Data Science > Chemoinformatics\n",
            "Data Science > Artificial Intelligence/Machine Learning\n",
            "\n",
            "----\n",
            "Paper 707:\n",
            "Title: CODATA and global challenges in data-driven science\n",
            "Abstract: This synthesis report presents the scientific results of the international conference \"Global Challenges and Data-Driven Science\" which took place in St. Petersburg, Russian Federation from 8 October to 13 October 2017. This event facilitated multidisciplinary scientific dialogue between leading scientists, data managers and experts, as well as Big Data researchers of various fields of knowledge. The St. Petersburg conference covered a wide range of topics related to data science. It featured discussions covering the collection and processing of large amounts of data, the implementation of system analysis methods into data science, machine learning, data mining, pattern recognition, decision-making robotics and algorithms of artificial intelligence. The conference was an outstanding event in the field of scientific diplomacy and brought together more than 150 participants from 35 countries. It's success ensured the effective data science dialog between nations and continents and established a new platform for future collaboration.\n",
            "----\n",
            "Paper 708:\n",
            "Title: An intent recognition pipeline for conversational AI\n",
            "Abstract: None\n",
            "----\n",
            "Paper 709:\n",
            "Title: XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source\n",
            "Abstract: . Question answering (QA) is a natural language understanding task within the fields of information retrieval and information extraction that has attracted much attention from the computational linguistics and artificial intelligence research community in recent years because of the strong development of machine reading comprehension-based models. A reader-based QA system is a high-level search engine that can find correct answers to queries or questions in open-domain or domain-specific texts using machine reading comprehension (MRC) techniques. The majority of advancements in data resources and machine-learning approaches in the MRC and QA systems, on the other hand, especially in two resource-rich languages such as English and Chinese. A low-resource language like Vietnamese has witnessed a scarcity of research on QA systems. This paper presents XLMRQA, the first Vietnamese QA system using a supervised transformer-based reader on the Wikipedia-based textual knowledge source (using the UIT-ViQuAD corpus), outperforming the two robust QA systems using deep neural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively. From the results obtained on the three systems, we analyze the influence of question types on the performance of the QA systems.\n",
            "----\n",
            "Paper 710:\n",
            "Title: PyPEF - An Integrated Framework for Data-Driven Protein Engineering\n",
            "Abstract: Data-driven strategies are gaining increased attention in protein engineering due to recent advances in access to large experimental databanks of proteins, next-generation sequencing (NGS), high-throughput screening (HTS) methods, and the development of artificial intelligence algorithms. However, the reliable prediction of beneficial amino acid substitutions, their combination, and the effect on functional properties remain the most significant challenges in protein engineering, which is applied to develop proteins and enzymes for biocatalysis, biomedicine, and life sciences. Here, we present a general-purpose framework (PyPEF: pythonic protein engineering framework) for performing data-driven protein engineering using machine learning methods combined with techniques from signal processing and statistical physics. PyPEF guides the identification and selection of beneficial proteins of a defined sequence space by systematically or randomly exploring the fitness of variants and by sampling random evolution pathways. The performance of PyPEF was evaluated concerning its predictive accuracy and throughput on four public protein and enzyme data sets using common regression models. It was proved that the program could efficiently predict the fitness of protein sequences for different target properties (predictive models with coefficient of determination values ranging from 0.58 to 0.92). By combining machine learning and protein evolution, PyPEF enabled the screening of proteins with various functions, reaching a screening capacity of more than 500,000 protein sequence variants in the timeframe of only a few minutes on a personal computer. PyPEF displayed significant accuracies on four public data sets (different proteins and properties) and underlined the potential of integrating data-driven technologies for covering different philosophies by either predicting the fitness of the variants to the highest accuracy accounting for epistatic effects or capturing the general trend of introduced mutations on the fitness in directed protein evolution campaigns. In essence, PyPEF can provide a powerful solution to current sequence exploration and combinatorial problems faced in protein engineering through exhaustive in silico screening of the sequence space.\n",
            "----\n",
            "Paper 711:\n",
            "Title: Transfer Learning for Financial Time Series Forecasting\n",
            "Abstract: None\n",
            "----\n",
            "Paper 712:\n",
            "Title: Accuracy of Computer-Aided Diagnosis of Melanoma: A Meta-analysis.\n",
            "Abstract: Importance\n",
            "The recent advances in the field of machine learning have raised expectations that computer-aided diagnosis will become the standard for the diagnosis of melanoma.\n",
            "\n",
            "\n",
            "Objective\n",
            "To critically review the current literature and compare the diagnostic accuracy of computer-aided diagnosis with that of human experts.\n",
            "\n",
            "\n",
            "Data Sources\n",
            "The MEDLINE, arXiv, and PubMed Central databases were searched to identify eligible studies published between January 1, 2002, and December 31, 2018.\n",
            "\n",
            "\n",
            "Study Selection\n",
            "Studies that reported on the accuracy of automated systems for melanoma were selected. Search terms included melanoma, diagnosis, detection, computer aided, and artificial intelligence.\n",
            "\n",
            "\n",
            "Data Extraction and Synthesis\n",
            "Evaluation of the risk of bias was performed using the QUADAS-2 tool, and quality assessment was based on predefined criteria. Data were analyzed from February 1 to March 10, 2019.\n",
            "\n",
            "\n",
            "Main Outcomes and Measures\n",
            "Summary estimates of sensitivity and specificity and summary receiver operating characteristic curves were the primary outcomes.\n",
            "\n",
            "\n",
            "Results\n",
            "The literature search yielded 1694 potentially eligible studies, of which 132 were included and 70 offered sufficient information for a quantitative analysis. Most studies came from the field of computer science. Prospective clinical studies were rare. Combining the results for automated systems gave a melanoma sensitivity of 0.74 (95% CI, 0.66-0.80) and a specificity of 0.84 (95% CI, 0.79-0.88). Sensitivity was lower in studies that used independent test sets than in those that did not (0.51; 95% CI, 0.34-0.69 vs 0.82; 95% CI, 0.77-0.86; P < .001); however, the specificity was similar (0.83; 95% CI, 0.71-0.91 vs 0.85; 95% CI, 0.80-0.88; P = .67). In comparison with dermatologists' diagnosis, computer-aided diagnosis showed similar sensitivities and a 10 percentage points lower specificity, but the difference was not statistically significant. Studies were heterogeneous and substantial risk of bias was found in all but 4 of the 70 studies included in the quantitative analysis.\n",
            "\n",
            "\n",
            "Conclusions and Relevance\n",
            "Although the accuracy of computer-aided diagnosis for melanoma detection is comparable to that of experts, the real-world applicability of these systems is unknown and potentially limited owing to overfitting and the risk of bias of the studies at hand.\n",
            "----\n",
            "Paper 713:\n",
            "Title: The Application of Deep Learning in Micro-Expression Recognition\n",
            "Abstract: With the advent of the era of big data, the application of artificial intelligence and big data technology has led to widespread interest in facial expression recognition. In addition to the common macro-expressions in daily life, facial expressions also have an imperceptible subtle expression called micro-expressions. Micro-expression is a very fast expression, lasting only 1/25 seconds to 1/5 seconds, expressing the real emotions that people try to suppress and hide. Micro-expressions have important applications in public safety, judicial criminal investigation, clinical medicine, etc. Therefore, the research on micro-expression recognition has been increasing in recent years. In the year of 2006, Hinton's proposal of deep learning in an article in «Science» made it appear in front of the world as a new field of machine learning, which set off a wave of deep learning. Deep learning is currently one of the hottest research directions in artificial intelligence and machine learning, and it is widely used in speech recognition, natural language processing, image recognition and other fields. Because of the characteristics of micro-expressions such as short time and subtle changes, traditional machine learning algorithms have poor robustness. Therefore, this paper summarizes the research on micro-expression recognition based on deep learning methods, mainly including some mainstream algorithms such as DBN, CNN, and discusses the development problems and trends of deep learning on micro-expression recognition, hoping to provide reference for the subsequent research in this field.\n",
            "----\n",
            "Paper 714:\n",
            "Title: Exploring contributors, collaborations, and research topics in educational technology: A joint analysis of mainstream conferences\n",
            "Abstract: None\n",
            "----\n",
            "Paper 715:\n",
            "Title: An Improved Incremental Training Approach for Large Scaled Dataset Based on Support Vector Machine\n",
            "Abstract: The Support Vector Machine(SVM) is well known in machine learning and artificial intelligence for its high performance in data classification, regression and forecasting. Usually for large scaled dataset, an incremental training algorithm is applied for tuning or balancing the training cost and the accuracy in SVM applications. This paper presents an improved incremental training approach for large scaled dataset on SVM. We focus on data's own distribution information to unfold our research, we proposed a self adaptive clustering method to extract the area and density information of data, a border detection technologies and uncertainty strategy is applied to maintain the border and some potential samples. Our proposed method can greatly reduce the training error for incremental training on SVM, especially for some uneven distribution dataset. We can greatly tuning or balancing the training cost and the accuracy of algorithms to achieve a better performance.\n",
            "----\n",
            "Paper 716:\n",
            "Title: Recent Advances in Learning Automata\n",
            "Abstract: None\n",
            "----\n",
            "Paper 717:\n",
            "Title: Smart city big data analytics: An advanced review\n",
            "Abstract: With the increasing role of ICT in enabling and supporting smart cities, the demand for big data analytics solutions is increasing. Various artificial intelligence, data mining, machine learning and statistical analysis‐based solutions have been successfully applied in thematic domains like climate science, energy management, transport, air quality management and weather pattern analysis. In this paper, we present a systematic review of the literature on smart city big data analytics. We have searched a number of different repositories using specific keywords and followed a structured data mining methodology for selecting material for the review. We have also performed a technological and thematic analysis of the shortlisted literature, identified various data mining/machine learning techniques and presented the results. Based on this analysis we also present a classification model that studies four aspects of research in this domain. These include data models, computing models, security and privacy aspects and major market drivers in the smart cities domain. Moreover, we present a gap analysis and identify future directions for research. For the thematic analysis we identified the themes smart city governance, economy, environment, transport and energy. We present the major challenges in these themes, the major research work done in the field of data analytics to address these challenges and future research directions.\n",
            "----\n",
            "Paper 718:\n",
            "Title: Shaping the Next Generation Pharmaceutical Supply Chain Control Tower with Autonomous Intelligence\n",
            "Abstract: This paper summarizes the findings of an industry panel study evaluating how new Autonomous Intelligence technologies, such as artificial intelligence and machine learning, impact the system and operational architecture of supply chain control tower (CT) implementations that serve the pharmaceutical industry. Such technologies can shift CTs to a model in which real-time information gathering, analysis, and decision making are possible. This can be achieved by leveraging these technologies to better manage decision complexity and execute decisions at levels that cannot otherwise be managed easily by humans. Some of the key points identified are in the areas of the fundamental capabilities that need to be supported and the improved level of decision visibility that they provide. We also consider some the challenges in achieving this, which include data quality and integrity, collaboration and data sharing across supply chain tiers, cross-system interoperability, decision-validation and organizational impacts, among others.\n",
            "----\n",
            "Paper 719:\n",
            "Title: Assessing Building Damage by Learning the Deep Feature Correspondence of Before and After Aerial Images\n",
            "Abstract: The damage caused by a natural disaster such as a hurricane, not only impacts human lives but can also be detrimental to the city's infrastructure and potentially cause the loss of historical buildings and essential records. Delivering an effective response requires quick and precise analyses concerning the impact of a disastrous event. With the current technological developments to acquire massive volumes of data and the recent advances in artificial intelligence and machine learning, now more than ever, disaster information integration and fusion have the potential to deliver enhanced situational awareness tools for humanitarian assistance and disaster relief efforts. Given the aerial images of a residential building taken before and after a natural disaster, recent applications of Convolutional Neural Networks (CNNs) work well when differentiating two types of damage (i.e., whether the structure is intact or destroyed) but underperform when trying to differentiate more damage levels. According to our findings: (1) including enough surrounding context provides essential visual clues that help the model better predict the building's level of damage and (2) learning the correspondence between the features extracted from pre-and post-imagery boosts the performance compared to a simple concatenation. We propose a two-stream CNN architecture that overcomes the difficulties of classifying the buildings at four damage levels and evaluate its performance on a curated, fully-labeled dataset assembled from open sources.\n",
            "----\n",
            "Paper 720:\n",
            "Title: Development of a Web-Based Prediction System for Students' Academic Performance\n",
            "Abstract: Educational Data Mining (EDM) is used to extract and discover interesting patterns from educational institution datasets using Machine Learning (ML) algorithms. There is much academic information related to students available. Therefore, it is helpful to apply data mining to extract factors affecting students’ academic performance. In this paper, a web-based system for predicting academic performance and identifying students at risk of failure through academic and demographic factors is developed. The ML model is developed to predict the total score of a course at the early stages. Several ML algorithms are applied, namely: Support Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbors (KNN), Artificial Neural Network (ANN), and Linear Regression (LR). This model applies to the data of female students of the Computer Science Department at Imam Abdulrahman bin Faisal University (IAU). The dataset contains 842 instances for 168 students. Moreover, the results showed that the prediction’s Mean Absolute Percentage Error (MAPE) reached 6.34%, and the academic factors had a higher impact on students’ academic performance than the demographic factors, the midterm exam score in the top. The developed web-based prediction system is available on an online server and can be used by tutors.\n",
            "----\n",
            "Paper 721:\n",
            "Title: Credit Card Fraud Detection using Random Forest Algorithm\n",
            "Abstract: This Project is focused on credit card fraud detection in real-world scenarios. Nowadays credit card frauds are drastically increasing in number as compared to earlier times. Criminals are using fake identity and various technologies to trap the users and get the money out of them. Therefore, it is very essential to find a solution to these types of frauds. In this proposed project we designed a model to detect the fraud activity in credit card transactions. This system can provide most of the important features required to detect illegal and illicit transactions. As technology changes constantly, it is becoming difficult to track the behavior and pattern of criminal transactions. To come up with the solution one can make use of technologies with the increase of machine learning, artificial intelligence and other relevant fields of information technology, it becomes feasible to automate this process and to save some of the intensive amounts of labor that is put into detecting credit card fraud. Initially, we will collect the credit card usage data-set by users and classify it as trained and testing dataset using a random forest algorithm and decision trees. Using this feasible algorithm, we can analyze the larger data-set and user provided current data-set. Then augment the accuracy of the result data. Proceeded with the application of processing of some of the attributes provided which can find affected fraud detection in viewing the graphical model of data visualization. The performance of the techniques is gauged based on accuracy, sensitivity, and specificity, precision. The results is indicated concerning the best accuracy for Random Forest are unit 98.6% respectively.\n",
            "----\n",
            "Paper 722:\n",
            "Title: The Role of Deep Learning in Improving Healthcare\n",
            "Abstract: None\n",
            "----\n",
            "Paper 723:\n",
            "Title: Deep language models for interpretative and predictive materials science\n",
            "Abstract: Machine learning (ML) has emerged as an indispensable methodology to describe, discover, and predict complex physical phenomena that efficiently help us learn underlying functional rules, especially in cases when conventional modeling approaches cannot be applied. While conventional feedforward neural networks are typically limited to performing tasks related to static patterns in data, recursive models can both work iteratively based on a changing input and discover complex dynamical relationships in the data. Deep language models can model flexible modalities of data and are capable of learning rich dynamical behaviors as they operate on discrete or continuous symbols that define the states of a physical system, yielding great potential toward end-to-end predictions. Similar to how words form a sentence, materials can be considered as a self-assembly of physically interacted building blocks, where the emerging functions of materials are analogous to the meaning of sentences. While discovering the fundamental relationships between building blocks and function emergence can be challenging, language models, such as recurrent neural networks and long-short term memory networks, and, in particular, attention models, such as the transformer architecture, can solve many such complex problems. Application areas of such models include protein folding, molecular property prediction, prediction of material failure of complex nonlinear architected materials, and also generative strategies for materials discovery. We outline challenges and opportunities, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and outline how tools such as ChatGPT and DALL·E can drive materials discovery.\n",
            "----\n",
            "Paper 724:\n",
            "Title: A Comprehensive Survey of COVID-19 Detection Using Medical Images\n",
            "Abstract: None\n",
            "----\n",
            "Paper 725:\n",
            "Title: Deep Learning Techniques for Electronic Health Record Analysis\n",
            "Abstract: Following the general trend, the amount of digital information in the stored electronic health records (EHRs) had an explosion in the last decade. EHRs are not anymore used, as in the past, to store basic information of the patient and administrative tasks, but they may include a range of data, including the medical history of the patient, laboratory test results, demographics, medication and allergies, immunization status, radiology images, vital signs. At the present, the problem has shifted from collecting massive amounts of data to understanding it, i.e. use EHRs for turning data into knowledge, conclusions and actions. EHRs were not designed to forecast disease risk or disease progression or to determine the right treatment, but if they are combined with artificial intelligence (AI) algorithm this issue became possible. The need for tools allowing to construct predictive models capturing disease progression is a priority. In the recent past EHRs were analyzed using traditional machine learning techniques, whereas recently the progress in the field of deep learning let to the application of deep learning techniques to EHRs. This paper reports a brief overview of some recently developed deep learning tools for EHRs.\n",
            "----\n",
            "Paper 726:\n",
            "Title: A Machine Learning Approach for the Curation of Biomedical Literature\n",
            "Abstract: None\n",
            "----\n",
            "Paper 727:\n",
            "Title: AI 2004: Advances in Artificial Intelligence, 17th Australian Joint Conference on Artificial Intelligence, Cairns, Australia, December 4-6, 2004, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 728:\n",
            "Title: Critical Computation: Digital Automata and General Artificial Thinking\n",
            "Abstract: As machines have become increasingly smart and have entangled human thinking with artificial intelligences, it seems no longer possible to distinguish among levels of decision-making that occur in the newly formed space between critical reasoning, logical inference and sheer calculation. Since the 1980s, computational systems of information processing have evolved to include not only deductive methods of decision, whereby results are already implicated in their premises, but have crucially shifted towards an adaptive practice of learning from data, an inductive method of retrieving information from the environment and establishing general premises. This shift in logical methods of decision-making does not simply concern technical apparatuses, but is a symptom of a transformation in logical thinking activated with and through machines. This article discusses the pioneering work of Katherine Hayles, whose study of the cybernetic and computational infrastructures of our culture particularly clarifies this epistemological transformation of thinking in relation to machines.\n",
            "----\n",
            "Paper 729:\n",
            "Title: Introduction to the Special Issue on Causal Inference for Recommender Systems\n",
            "Abstract: A significant proportion of machine learning methodologies for recommendation systems are grounded in the fundamental principle of matching, utilizing perceptual and similarity-based learning approaches. These methods include both the extraction of features from data through representation learning and the derivation of similarity matching functions via neural function learning. While these models are important for recommendation systems, their foundational design philosophy primarily captures correlational signals within the data. Transitioning from correlation-based learning to causal learning in recommendation systems represents a critical area to explore, as causal models enable extrapolation beyond observational data in both representation learning and ranking tasks. Specifically, causal learning offers potential enhancements to the recommender system community across multiple dimensions, including, but not limited to, explainable, unbiased, fairness-aware, robust, and cognitive reasoning models for recommendation. This special issue is dedicated to exploring the research and practical applications of causal inference within the realms of recommendation and broader ranking scenarios. It has attracted interest from an array of researchers and practitioners on disseminating the latest developments in causal modeling for recommender systems. Moreover, it has attracted the interest of professionals from various fields such as Information Retrieval, Machine Learning, Artificial Intelligence, Natural Language Processing, Data Science, and others.\n",
            "----\n",
            "Paper 730:\n",
            "Title: Critical Care, Critical Data\n",
            "Abstract: As big data, machine learning, and artificial intelligence continue to penetrate into and transform many facets of our lives, we are witnessing the emergence of these powerful technologies within health care. The use and growth of these technologies has been contingent on the availability of reliable and usable data, a particularly robust resource in critical care medicine where continuous monitoring forms a key component of the infrastructure of care. The response to this opportunity has included the development of open databases for research and other purposes; the development of a collaborative form of clinical data science intended to fully leverage these data resources, and the creation of data-driven applications for purposes such as clinical decision support. Most recently, data levels have reached the thresholds required for the development of robust artificial intelligence features for clinical purposes. The systematic capture and analysis of clinical data in both individuals and populations allows us to begin to move toward precision medicine in the intensive care unit (ICU). In this perspective review, we examine the fundamental role of data as we present the current progress that has been made toward an artificial intelligence (AI)-supported, data-driven precision critical care medicine.\n",
            "----\n",
            "Paper 731:\n",
            "Title: 50 Years of Artificial Intelligence, Essays Dedicated to the 50th Anniversary of Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 732:\n",
            "Title: Associative Rule Mining for the Assessment of the Risk of Recidivism\n",
            "Abstract: Information technologies, such as big data analytics (data mining, and predictive analytics) are rapidly developing and penetrating into critically important areas of modern technocratic society, including law enforcement and security sectors. Successful applications of artificial intelligence (AI) are able not only to provide informational support for decision-making, but also to replace the person himself in this process. Global digitization has also affected criminal justice in recent years. The justice system adapts to modern challenges and megatrends using digital platforms and data science. Solving crimes is a complex task and requires significant resources and expertise. Data mining models can provide effective solutions for solving complex non-standard problems of crime detection. AI algorithms are increasingly used in predictive justice to support judicial decision-making, in particular the qualification of crimes and their relevance to the criminal process. Text Mining algorithms provide reliable support in the formation of the evidence base, judicial analytics, and reasonable decision-making in criminal proceedings. Predictive machine learning algorithms are not always 100% accurate, but the criminal justice system is becoming more and more technologically complex. Its effective functioning includes the processing of large volumes of unordered and unstructured data. Nowadays, association rules, i. e. one of the important machine learning models, are widely used to identify patterns and discover new knowledge in such masses of information. The work uses associative rule mining for the extract correlations and co-occurrences between the historical crime information of convicted people. An associative rule mining model was built to search for non-obvious interesting connections between historical crime information of convicted and repeated offenses. The frequent item sets, which are a combination of individual characteristics of convicts who commit repeated criminal offenses, and the strong association rules, have been revealed. The obtained results give grounds for asserting that early dismissals and suspended convictions are the main factors (antecedents), that cause the risk of recidivism (consequent).\n",
            "----\n",
            "Paper 733:\n",
            "Title: Citizen Science in the Digital World of Apps\n",
            "Abstract: None\n",
            "----\n",
            "Paper 734:\n",
            "Title: A Study about Future Prospects of JupyterHub in MOOCs\n",
            "Abstract: The Hasso Plattner Institute (HPI) has been successfully delivering courses on several MOOC (Massive Open Online Course) platforms for the last 10 years, offering courses on various topics in the context of Artificial Intelligence (AI), Machine Learning (ML), and Data Science. In recent years, Jupyter Notebooks have become one of the most widely used tools for data science applications, a platform for learning and practicing various programming languages. We want to integrate JupyterHub into our learning platform in order to provide students with hands-on experience in AI. We have conducted a survey with a series of research questions in order to understand the needs of instructors in their courses at different institutions. In this paper, we present a detailed analysis of our survey results and we discuss our future approach to using JupyterHub as an infrastructure to solve hands-on programming exercises on our platform. We propose the idea of creating a tool to automate server and environment creation for students to work on. This tool would give instructors a platform to operate from and allow them to customize their courses. Moreover, it would help them automate assignment submissions, grading, and provide feedback to their students.\n",
            "----\n",
            "Paper 735:\n",
            "Title: Student’s performance prediction model and affecting factors using classification techniques\n",
            "Abstract: None\n",
            "----\n",
            "Paper 736:\n",
            "Title: Advances in Brain, Vision, and Artificial Intelligence, Second International Symposium, BVAI 2007, Naples, Italy, October 10-12, 2007, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 737:\n",
            "Title: Emerging Legal Informatics Towards Legal Innovation: Current Status and Future Challenges and Opportunities\n",
            "Abstract: Abstract In recent years, the legal informatics area has slowly begun to develop as artificial intelligence and its related techniques and technologies expand their reach in the field of law. The legal, computational and data science communities are collaborating to build computational and data-driven innovative legal models to improve and advance all aspects of the existing legal system with the effective use of modern computer technologies such as machine learning, deep learning and natural language processing. In this research paper, the authors - Sugam Sharma, Samia Gamoura, Deva Prasad and Arti Aneja - explore these important factors with the potential to transform the existing approach to jurisprudence into a smart and intelligent legal system utilising automation. Such a justice system could be envisioned, in the near future which would be faster, fairer and economically more feasible, even for the highly marginalized, underprivileged and poor societies in the world.\n",
            "----\n",
            "Paper 738:\n",
            "Title: Using Deep Neural Networks to Reconstruct Non-uniformly Sampled NMR Spectra\n",
            "Abstract: None\n",
            "----\n",
            "Paper 739:\n",
            "Title: Resilient Digital Twins\n",
            "Abstract: None\n",
            "----\n",
            "Paper 740:\n",
            "Title: Deep-LSTM ensemble framework to forecast Covid-19: an insight to the global pandemic\n",
            "Abstract: None\n",
            "----\n",
            "Paper 741:\n",
            "Title: A Multi-Perspective Framework for Research on (Sustainable) Autonomous Systems\n",
            "Abstract: None\n",
            "----\n",
            "Paper 742:\n",
            "Title: Evaluation of important phenotypic parameters of tea plantations using multi-source remote sensing data\n",
            "Abstract: Tea height, leaf area index, canopy water content, leaf chlorophyll, and nitrogen concentrations are important phenotypic parameters to reflect the status of tea growth and guide the management of tea plantation. UAV multi-source remote sensing is an emerging technology, which can obtain more abundant multi-source information and enhance dynamic monitoring ability of crops. To monitor the phenotypic parameters of tea canopy more efficiently, we first deploy UAVs equipped with multispectral, thermal infrared, RGB, LiDAR, and tilt photography sensors to acquire phenotypic remote sensing data of tea canopy, and then, we utilize four machine learning algorithms to model the single-source and multi-source data, respectively. The results show that, on the one hand, using multi-source data sets to evaluate H, LAI, W, and LCC can greatly improve the accuracy and robustness of the model. LiDAR + TC data sets are suggested for assessing H, and the SVM model delivers the best estimation (Rp2 = 0.82 and RMSEP = 0.078). LiDAR + TC + MS data sets are suggested for LAI assessment, and the SVM model delivers the best estimation (Rp2 = 0.90 and RMSEP = 0.40). RGB + TM data sets are recommended for evaluating W, and the SVM model delivers the best estimation (Rp2 = 0.62 and RMSEP = 1.80). The MS +RGB data set is suggested for studying LCC, and the RF model offers the best estimation (Rp2 = 0.87 and RMSEP = 1.80). On the other hand, using single-source data sets to evaluate LNC can greatly improve the accuracy and robustness of the model. MS data set is suggested for assessing LNC, and the RF model delivers the best estimation (Rp2 = 0.65 and RMSEP = 0.85). The work revealed an effective technique for obtaining high-throughput tea crown phenotypic information and the best model for the joint analysis of diverse phenotypes, and it has significant importance as a guiding principle for the future use of artificial intelligence in the management of tea plantations.\n",
            "----\n",
            "Paper 743:\n",
            "Title: SMU-DDI Cyber Autonomy Range\n",
            "Abstract: The Southern Methodist University-Darwin Deason Institute for Cybersecurity (SMU-DDI) Cyber Autonomy Range (CAR) addresses the incorporation of increased resiliency, reliability, and cyber security of the autonomous systems (AS) cyberinfrastructure; an issue with widespread concern and broad impact on society. The advances of data science and Machine Learning/Artificial Intelligence (ML/AI) methods coupled with their integration into autonomous subsystems is an enabling trend that supports AS maturity. Likewise, these same aspects of ML/AI present entirely new aspects of cyber security, many of which have only been analyzed in a preliminary sense or for special cases. The ML/AI aspects of cyber security are critically important, with significant ramifications in human safety and well-being. The CAR is a collaborative facility that supports the assessment of AS when faced with cyber threats by assessing their attack surface, vulnerability, and their degree of resistance to such threats. It is instrumented to simulate and/or emulate the external environment of an AS and can subject the AS to a variety of controlled cyber-attacks. Because the decision-making capabilities of many AS are based upon data-driven ML/AI-enabled technologies, the threat surface surrounding ML/AI subsystems is of particular concern. The CAR is especially configured to investigate and simulate (or emulate) cyber-attacks on ML/AI-equipped subsystems; particularly ML/AI subsystems that depend upon data sources derived from sensor suites or other data sources.\n",
            "----\n",
            "Paper 744:\n",
            "Title: Reimagining data responsibility: 10 new approaches toward a culture of trust in re-using data to address critical public needs\n",
            "Abstract: Abstract Data and data science offer tremendous potential to address some of our most intractable public problems (including the Covid-19 pandemic). At the same time, recent years have shown some of the risks of existing and emerging technologies. An updated framework is required to balance potential and risk, and to ensure that data is used responsibly. Data responsibility is not itself a new concept. However, amid a rapidly changing technology landscape, it has become increasingly clear that the concept may need updating, in order to keep up with new trends such as big data, open data, the Internet of things, and artificial intelligence, and machine learning. This paper seeks to outline 10 approaches and innovations for data responsibility in the 21st century. The 10 emerging concepts we have identified include: End-to-end data responsibility Decision provenance Professionalizing data stewardship From data science to question science Contextual consent Responsibility by design Data asymmetries and data collaboratives Personally identifiable inference Group privacy Data assemblies Each of these is described at greater length in the paper, and illustrated with examples from around the world. Put together, they add up to a framework or outline for policy makers, scholars, and activists who seek to harness the potential of data to solve complex social problems and advance the public good. Needless to say, the 10 approaches outlined here represent just a start. We envision this paper more as an exercise in agenda-setting than a comprehensive survey.\n",
            "----\n",
            "Paper 745:\n",
            "Title: Hybrid Artificial Intelligence Systems, 5th International Conference, HAIS 2010, San Sebastián, Spain, June 23-25, 2010. Proceedings, Part II\n",
            "Abstract: None\n",
            "----\n",
            "Paper 746:\n",
            "Title: Data Sovereignty and Data Space Ecosystems\n",
            "Abstract: None\n",
            "----\n",
            "Paper 747:\n",
            "Title: Semi-Supervised Active Learning for COVID-19 Lung Ultrasound Multi-symptom Classification\n",
            "Abstract: Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for the lung's multi-symptom (multi-label) classification. To overcome these difficulties, we propose a novel semi-supervised Two-Stream Active Learning (TSAL) method to model complicated features and reduce labeling costs in an iterative manner. The core component of TSAL is the multi-label learning mechanism, in which label correlation information is used to design a multi-label margin (MLM) strategy and a confidence validation for automatically selecting informative samples and confident labels. In this framework, a multi-symptom multi-label (MSML) classification network is proposed to learn discriminative features of lung symptoms, and a human-machine interaction (HMI) is exploited to confirm the final annotations that are used to fine-tune MSML. Moreover, a novel lung US dataset named COVID19-LUSMS is built, currently containing 71 clinical patients with 6,836 images sampled from 678 videos. Experimental evaluations show that TSAL can achieve superior performance to the baseline and the state-of-the-art using only 20% data. Qualitatively, visualization of the attention map confirms a good consistency between the model prediction and the clinical knowledge.\n",
            "----\n",
            "Paper 748:\n",
            "Title: Design and Deployment of a Data Lake at a Pilot Plant Scale for a Smart Electropolishing Process\n",
            "Abstract: In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.\n",
            " Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.\n",
            " Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.\n",
            " Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2\n",
            " \n",
            " In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.\n",
            " References: \n",
            " \n",
            " N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).\n",
            " \n",
            " \n",
            " H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " Figure 1\n",
            "\n",
            "----\n",
            "Paper 749:\n",
            "Title: Explainable AI for the Choquet Integral\n",
            "Abstract: The modern era of machine learning is focused on data-driven solutions. While this has resulted in astonishing leaps in numerous applications, explainability has not witnessed the same growth. The reality is, most machine learning solutions are black boxes. Herein, we focus on data/information fusion in machine learning. Specifically, we explore four eXplainable Artificial Intelligence (XAI) questions relative to Choquet integral; (i) what is the quality of our inputs and their interactions, (ii) how is the information being combined, (iii) what is the quality of our training data (and thus our learned models), and (iv) what trust do we place in an output? Previously, we derived an initial set of indices for (i)–(iv) on the premise of perfect knowledge. Herein, we make XAI more accurate by taking into consideration what the machine learned. A combination of synthetic data and real-world experiments from remote sensing for fusing deep learners in the context of classification are explored. Our approach leads to performance gain, insights into what was learned, and it helps us realize better future solutions.\n",
            "----\n",
            "Paper 750:\n",
            "Title: Understanding AI’s Role in Endometriosis Patient Education and Evaluating Its Information and Accuracy: Systematic Review\n",
            "Abstract: Background Endometriosis is a chronic gynecological condition that affects a significant portion of women of reproductive age, leading to debilitating symptoms such as chronic pelvic pain and infertility. Despite advancements in diagnosis and management, patient education remains a critical challenge. With the rapid growth of digital platforms, artificial intelligence (AI) has emerged as a potential tool to enhance patient education and access to information. Objective This systematic review aims to explore the role of AI in facilitating education and improving information accessibility for individuals with endometriosis. Methods This review followed the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines to ensure rigorous and transparent reporting. We conducted a comprehensive search of PubMed; Embase; the Regional Online Information System for Scientific Journals of Latin America, the Caribbean, Spain and Portugal (LATINDEX); Latin American and Caribbean Literature in Health Sciences (LILACS); Institute of Electrical and Electronics Engineers (IEEE) Xplore, and the Cochrane Central Register of Controlled Trials using the terms “endometriosis” and “artificial intelligence.” Studies were selected based on their focus on AI applications in patient education or information dissemination regarding endometriosis. We included studies that evaluated AI-driven tools for assessing patient knowledge and addressed frequently asked questions related to endometriosis. Data extraction and quality assessment were conducted independently by 2 authors, with discrepancies resolved through consensus. Results Out of 400 initial search results, 11 studies met the inclusion criteria and were fully reviewed. We ultimately included 3 studies, 1 of which was an abstract. The studies examined the use of AI models, such as ChatGPT (OpenAI), machine learning, and natural language processing, in providing educational resources and answering common questions about endometriosis. The findings indicated that AI tools, particularly large language models, offer accurate responses to frequently asked questions with varying degrees of sufficiency across different categories. AI’s integration with social media platforms also highlights its potential to identify patients’ needs and enhance information dissemination. Conclusions AI holds promise in advancing patient education and information access for endometriosis, providing accurate and comprehensive answers to common queries, and facilitating a better understanding of the condition. However, challenges remain in ensuring ethical use, equitable access, and maintaining accuracy across diverse patient populations. Future research should focus on developing standardized approaches for evaluating AI’s impact on patient education and exploring its integration into clinical practice to enhance support for individuals with endometriosis.\n",
            "----\n",
            "Paper 751:\n",
            "Title: The Impact of 4IR Digital Technologies and Circular Thinking on the United Nations Sustainable Development Goals\n",
            "Abstract: As we stand at the cusp of the fourth industrial revolution, digital technologies such as artificial intelligence, machine learning, the Internet of Things, Big Data, Blockchain, Robotics, 3D technologies, and many more have become the means and solutions to many of the world’s problems. Most recently, these technologies have assisted in the global fight of the COVID-19 pandemic and other societal problems. Together with these innovative techniques, the concept of circular economy and its relevant tools such as life cycle costing, life cycle impact assessment, materials passports, and circularity measurements have been implemented in a number of sectors in different countries for the transition from a linear “take, make, and dispose” model towards a more circular model, which has shown positive results for the environment and economy. In this article, with the help of implementation, prototyping, and case studies, we explore how these technological advancements and innovative techniques are used in different sectors such as information and communications technology, the built environment, mining and manufacturing, education, healthcare, the public sectors, and others to provide an opportunity to understand and resolve the agreed upon framework in 2015 by 193 countries, that is, the 17 United Nations Sustainable Development Goals.\n",
            "----\n",
            "Paper 752:\n",
            "Title: Ranking defects and solving countermeasures for Pythagorean fuzzy sets with hesitant degree\n",
            "Abstract: None\n",
            "----\n",
            "Paper 753:\n",
            "Title: Predicting Stock Prices using Ensemble Learning and Sentiment Analysis\n",
            "Abstract: The recent success of the application of Artificial Intelligence in the financial sector has resulted in more firms relying on stochastic models for predicting the behaviour of the market. Everyday, quantitative analysts strive to attain better accuracies from their machine learning models for forecasting returns from stocks. Support Vector Machine (SVM) and Random Forest based regression models are known for their effectiveness in accurately predicting closing prices. In this work, we propose a technique for analyzing and predicting stock prices of companies using the aforementioned algorithms as an ensemble. Datasets from India's National Stock Exchange (NSE) containing basic market price information are preprocessed to include well known leading technical indicators as features. Feature selection, which ranks features based on their degree of influence on the final closing price has been incorporated to reduce the size of the training dataset. Additionally, we evaluate the effectiveness of considering the public opinion of a company by employing sentiment analysis. Using a trained Word2Vec model, company specific hash-tagged posts from Twitter are classified as positive or negative. Our proposed ensemble model is then trained on a new dataset which combines the technical indicator data along with the aggregated number of positive/negative tweets of a company over time. Our experiments indicate that in some scenarios, the ensemble model performs better than the constituent models and is highly dependent of the nature and size of the training data. However, combining technical indicator data with aggregated positive/negative tweet counts has a negligible effect on the performance of the ensemble model.\n",
            "----\n",
            "Paper 754:\n",
            "Title: A Comparative Analysis of Unsupervised Machine Techniques for Liver Disease Prediction\n",
            "Abstract: Machine learning is a branch of Artificial Intelligence(AI) which is heavily used in the field of data science. It has a strong potential in health-related data analysis for automated disease prediction. The work focuses on three different machine learning techniques, i.e., DBSCAN, K-Means, and Affinity Propagation to compare their prediction accuracy and computational complexity. The study concentrates on liver disease-related health care data set and uses the Silhouette coefficient for comparative performance measurement of the three techniques mentioned above. The Silhouette coefficient determines prediction accuracy giving K-Means as the optimal method. The overall results will then be analyzed on the basis of prediction accuracy and computational complexity to determine the best technique for prediction of liver diseases using unsupervised machine learning.\n",
            "----\n",
            "Paper 755:\n",
            "Title: The Pedagogical Pentagon: A Conceptual Framework for Artificial Pedagogy\n",
            "Abstract: None\n",
            "----\n",
            "Paper 756:\n",
            "Title: A Design-to-Device Pipeline for Data-Driven Materials Discovery.\n",
            "Abstract: The world needs new materials to stimulate the chemical industry in key sectors of our economy: environment and sustainability, information storage, optical telecommunications, and catalysis. Yet, nearly all functional materials are still discovered by \"trial-and-error\", of which the lack of predictability affords a major materials bottleneck to technological innovation. The average \"molecule-to-market\" lead time for materials discovery is currently 20 years. This is far too long for industrial needs, as highlighted by the Materials Genome Initiative, which has ambitious targets of up to 4-fold reductions in average molecule-to-market lead times. Such a large step change in progress can only be realistically achieved if one adopts an entirely new approach to materials discovery. Fortunately, a fundamentally new approach to materials discovery has been emerging, whereby data science with artificial intelligence offers a prospective solution to speed up these average molecule-to-market lead times. This approach is known as data-driven materials discovery. Its broad prospects have only recently become a reality, given the timely and major advances in \"big data\", artificial intelligence, and high-performance computing (HPC). Access to massive data sets has been stimulated by government-regulated open-access requirements for data and literature. Natural-language processing (NLP) and machine-learning (ML) tools that can mine data and find patterns therein are becoming mainstream. Exascale HPC capabilities that can aid data mining and pattern recognition and also generate their own data from calculations are now within our grasp. These timely advances present an ideal opportunity to develop data-driven materials-discovery strategies to systematically design and predict new chemicals for a given device application. This Account shows how data science can afford materials discovery via a four-step \"design-to-device\" pipeline that entails (1) data extraction, (2) data enrichment, (3) material prediction, and (4) experimental validation. Massive databases of cognate chemical and property information are first forged from \"chemistry-aware\" natural-language-processing tools, such as ChemDataExtractor, and enriched using machine-learning methods and high-throughput quantum-chemical calculations. New materials for a bespoke application can then be predicted by mining these databases with algorithmic encodings of relationships between chemical structures and physical properties that are known to deliver functional materials. These may take the form of classification, enumeration, or machine-learning algorithms. A data-mining workflow short-lists these predictions to a handful of lead candidate materials that go forward to experimental validation. This design-to-device approach is being developed to offer a roadmap for the accelerated discovery of new chemicals for functional applications. Case studies presented demonstrate its utility for photovoltaic, optical, and catalytic applications. While this Account is focused on applications in the physical sciences, the generic pipeline discussed is readily transferable to other scientific disciplines such as biology and medicine.\n",
            "----\n",
            "Paper 757:\n",
            "Title: Privacy and Security of Big Data in AI Systems: A Research and Standards Perspective\n",
            "Abstract: The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness.\n",
            "----\n",
            "Paper 758:\n",
            "Title: Deep Learning\n",
            "Abstract: Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.\n",
            "----\n",
            "Paper 759:\n",
            "Title: Rolling Bearing Fault Diagnosis Based on Horizontal Visibility Graph and Graph Neural Networks\n",
            "Abstract: The automatic extraction and learning features relying on artificial intelligence algorithms replace traditional manual features. More effective feature expression improves the performance of machine fault diagnosis with fewer requirements for labor and expertise. However, the present models only can process the data in Euclidean space. The relations between data points are ignored for a long time, which can play a significant role in distinguishing diverse faults patterns. To combat this issue, a novel model for bearing faults diagnosis is proposed by incorporating the horizontal visibility graph (HVG) and graph neural networks (GNN). In the proposed model, time series is converted to graph retaining invariant dynamic characteristics through the HVG algorithm, and the generated graphs are fed into a designed GNN model for feature learning and faults classification further. Finally, the proposed model is tested on two actual bearing datasets, and it shows state-of-the-art performance in the bearing faults diagnosis. The experimental results demonstrate that extracting relation information using HVG benefits bearing faults diagnosis.\n",
            "----\n",
            "Paper 760:\n",
            "Title: The Concept of Developing a Decision Support System for the Epidemic Morbidity Control\n",
            "Abstract: Paper presents concept model of intelligent information technology of epidemic process control. The project is an interdisciplinary research that combines scientific results obtained by specialists in the field of biosafety, systems and means of artificial intelligence, mathematical modeling, epidemiology, information technology, and public health. The development of a conceptual model provides the analysis of epidemic threats and problems of the biosafety of society; preprocessing of the initial data; development of machine learning models for analyzing the epidemic process; creation of a bank of simulation models; improvement of methods of intelligent interaction of agents of multiagent systems of population dynamics; predicting morbidity; analysis of factors influencing the epidemic process; development of information technology specification and testing of an intelligent decision support system in the field of biosafety. The implementation of the research results will increase the efficiency of management decisions to ensure the biosafety of the population and the development of scientifically based strategies for anti-epidemic and preventive measures.\n",
            "----\n",
            "Paper 761:\n",
            "Title: Employing Explainable AI to Optimize the Return Target Function of a Loan Portfolio\n",
            "Abstract: In the recent years, data science methods have been developed considerably and have consequently found their way into many business processes in banking and finance. One example is the review and approval process of credit applications where they are employed with the aim to reduce rare but costly credit defaults in portfolios of loans. But there are challenges. Since defaults are rare events, it is—even with machine learning (ML) techniques—difficult to improve prediction accuracy and improvements are often marginal. Furthermore, while from an event prediction point of view, a non-default is the same as a default, from an economic point of view much more relevant to the end user it is not due to the high asymmetry in cost. Last, there are regulatory constraints when it comes to the adoption of advanced ML, hence the call for explainable artificial intelligence (XAI) issued by regulatory bodies like FINMA and BaFin. In our study, we will address these challenges. In particular, based on an exemplary use case, we show how ML methods can be adapted to the specific needs of credit assessment and how, in the case of strongly asymmetric costs of wrong forecasts, it makes sense to optimize not for accuracy but for an economic target function. We showcase this for two simple and ad hoc explainable ML algorithms, finding that in the case of credit approval, surprisingly high rejection rates contribute to maximizing profit.\n",
            "----\n",
            "Paper 762:\n",
            "Title: Explainable AI (XAI) In Biomedical Signal and Image Processing: Promises and Challenges\n",
            "Abstract: Artificial intelligence has become pervasive across disciplines and fields, and biomedical image and signal processing is no exception. The growing and widespread interest on the topic has triggered a vast research activity that is reflected in an exponential research effort. Through study of massive and diverse biomedical data, machine and deep learning models have revolutionized various tasks such as modeling, segmentation, registration, classification and synthesis, outperforming traditional techniques. However, the difficulty in translating the results into biologically/clinically interpretable information is preventing their full exploitation in the field. Explainable AI (XAI) attempts to fill this translational gap by providing means to make the models interpretable and providing explanations. Different solutions have been proposed so far and are gaining increasing interest from the community. This paper aims at providing an overview on XAI in biomedical data processing and points to an upcoming Special Issue on Deep Learning in Biomedical Image and Signal Processing of the IEEE Signal Processing Magazine that is going to appear in March 2022.\n",
            "----\n",
            "Paper 763:\n",
            "Title: MICAI 2006: Advances in Artificial Intelligence, 5th Mexican International Conference on Artificial Intelligence, Apizaco, Mexico, November 13-17, 2006, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 764:\n",
            "Title: AI*IA 2007: Artificial Intelligence and Human-Oriented Computing, 10th Congress of the Italian Association for Artificial Intelligence, Rome, Italy, September 10-13, 2007, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 765:\n",
            "Title: Credibility in Information Retrieval\n",
            "Abstract: Credibility, as the general concept covering trustworthiness and expertise, but also quality and reliability, is strongly debated in philosophy, psychology, and sociology, and its adoption in computer science is therefore fraught with difficulties. Yet its importance has grown in the information access community because of two complementing factors: on one hand, it is relatively difficult to precisely point to the source of a piece of information, and on the other hand, complex algorithms, statistical machine learning, artificial intelligence, make decisions on behalf of the users, with little oversight from the users themselves.This survey presents a detailed analysis of existing credibility models from different information seeking research areas, with focus on the Web and its pervasive social component. It shows that there is a very rich body of work pertaining to different aspects and interpretations of credibility, particularly for different types of textual content e.g., Web sites, blogs, tweets, but also to other modalities videos, images, audio and topics e.g., health care. After an introduction placing credibility in the context of other sciences and relating it to trust, we argue for a quartic decomposition of credibility: expertise and trustworthiness, well documented in the literature and predominantly related to information source, and quality and reliability, raised to the status of equal partners because the source is often impossible to detect, and predominantly related to the content.The second half of the survey provides the reader with access points to the literature, grouped by research interests. Section 3 reviews general research directions: the factors that contribute to credibility assessment in human consumers of information; the models used to combine these factors; the methods to predict credibility. A smaller section is dedicated to informing users about the credibility learned from the data. Sections 4, 5, and 6 go further into details, with domain-specific credibility, social media credibility, and multimedia credibility, respectively. While each of them is best understood in the context of Sections 1 and 2, they can be read independently of each other.The last section of this survey addresses a topic not commonly considered under \"credibility\": the credibility of the system itself, independent of the data creators. This is a topic of particular importance in domains where the user is professionally motivated and where there are no concerns about the credibility of the data e.g. e-discovery and patent search. While there is little explicit work in this direction, we argue that this is an open research direction that is worthy of future exploration.Finally, as an additional help to the reader, an appendix lists the existing test collections that cater specifically to some aspect of credibility.Overall, this review will provide the reader with an organised and comprehensive reference guide to the state of the art and the problems at hand, rather than a final answer to the question of what credibility is for computer science. Even within the relatively limited scope of an exact science, such an answer is not possible for a concept that is itself widely debated in philosophy and social sciences.\n",
            "----\n",
            "Paper 766:\n",
            "Title: A Twitter Tale of Three Hurricanes: Harvey, Irma, and Maria\n",
            "Abstract: People increasingly use microblogging platforms such as Twitter during natural disasters and emergencies. Research studies have revealed the usefulness of the data available on Twitter for several disaster response tasks. However, making sense of social media data is a challenging task due to several reasons such as limitations of available tools to analyze high-volume and high-velocity data streams. This work presents an extensive multidimensional analysis of textual and multimedia content from millions of tweets shared on Twitter during the three disaster events. Specifically, we employ various Artificial Intelligence techniques from Natural Language Processing and Computer Vision fields, which exploit different machine learning algorithms to process the data generated during the disaster events. Our study reveals the distributions of various types of useful information that can inform crisis managers and responders as well as facilitate the development of future automated systems for disaster management.\n",
            "----\n",
            "Paper 767:\n",
            "Title: Scalability and sparsity issues in recommender datasets: a survey\n",
            "Abstract: None\n",
            "----\n",
            "Paper 768:\n",
            "Title: Challenges and Applications of Data Analytics in Social Perspectives\n",
            "Abstract: This chapter introduces data science with its history and importance in this modern era briefly. This chapter also elaborates the discussion by relating data science to various modern fields like big data analytics, artificial intelligence, deep learning, and machine learning. This chapter also discuss the necessary of data analytics in this big data era. This chapter also briefly introduces another emerging field, Internet of Things (IoT) and explores the contribution IoT towards big data analytics and data science in research perspective. It also briefly introduces the programming and non-programming tools used in the data science field.\n",
            "----\n",
            "Paper 769:\n",
            "Title: A nonlinear dynamics approach to data-enabled science: Reconstructing soil-moisture dynamics from big data collected by wireless sensor networks\n",
            "Abstract: \n",
            " <p>The complex soil biome is a center piece in providing essential ecosystem services that humans rely on (carbon sequestration, food security, one-health interactions).&#160; Agricultural engineers and soil scientists are developing wireless sensor networks (WSN) that collect large/big data on the soil key state variables (water content, temperature, chemistry) to better understand the soil biome primary environmental drivers. The profession extracts information from WSN records with methods including soil-process modeling and artificial-intelligence (AI) algorithms.&#160; However, these approaches carry their own limitations.&#160; A recent review article faulted current soil-process modeling for inadequately detecting and resolving model structural (abstraction) errors.&#160; AI experts themselves caution against indiscriminant use of AI methods because of: a) problems including replication of past results due to inconsistent experimental methods; b) difficulty in explaining how a particular method arrives at its conclusions (the black box problem) and thus in correcting algorithms that learn &#8216;bad lessons&#8217;; and c) lack of rigorous criteria for selecting AI architectures.&#160; An alternative approach to address these limitations is to investigate new strategies for reducing large/big data problems into smaller, more interpretable causal abstractions of the soil system.&#160;&#160;</p><p>We develop an innovative data diagnostics framework&#8212;based on empirical nonlinear dynamics techniques from physics&#8212;that addresses the above concerns over soil-process modeling and AI algorithms.&#160; We diagnose whether WSN and other similar environmental large/big data are likely generated by dimension-reducing (i.e., dissipative) nonlinear dynamics.&#160; An n-dimensional nonlinear dynamic system is dissipative if long-term dynamics are bounded within m<<n dimensions, so that the problem of modeling long-term dynamics shrinks by the n-m inactive degrees of freedom.&#160; If so, long-term system dynamics can be investigated with relatively few degrees of freedom that capture the complexity of the overall system generating observed data. &#160;To make this diagnosis, we first apply signal processing to isolate structured variation (signal) from unstructured variation (noise) in large/big data time series records, and test signals for nonlinear stationarity.&#160; We resolve the structure of isolated signals by distinguishing between stochastic-forcing and deterministic nonlinear dynamics; reconstruct phase space dynamics most likely generating signals, and test the statistical significance of reconstructed dynamics with surrogate data.&#160; If the reconstructed phase space is dimension-reducing, we can formulate low-dimensional (phenomenological) ODE models to investigate nonlinear causal interactions between key soil environmental driving factors.&#160; When we do not diagnose dimension-reducing nonlinear real-world dynamics, then underlying dynamics are most likely high dimensional and the information-extraction problem cannot be shrunk without losing essential dynamic information. In this case, other high-dimensional analysis techniques like AI offer a better modeling alternative for mapping out interactions.&#160; Our framework supplies a decision-support tool for data practitioners toward the most informative and parsimonious information-extraction method&#8212;a win-win result.&#160;&#160;&#160;&#160;&#160;&#160;&#160;</p><p>We will share preliminary results applying this empirical framework to three soil moisture sensor time series records analyzed with machine learning methods in Bean, Huffaker, and Migliaccio (2018).</p>\n",
            "\n",
            "----\n",
            "Paper 770:\n",
            "Title: A systematic review of applications of natural language processing and future challenges with special emphasis in text-based emotion detection\n",
            "Abstract: None\n",
            "----\n",
            "Paper 771:\n",
            "Title: PRICAI 2010: Trends in Artificial Intelligence, 11th Pacific Rim International Conference on Artificial Intelligence, Daegu, Korea, August 30-September 2, 2010. Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 772:\n",
            "Title: Approaches Towards AI-Based Recommender System\n",
            "Abstract: Recommender Systems (RS) have application in all areas where digital information is generating at high rate. They are used to filter large volume of digital data containing user preferences, interests and behaviour patterns for products and services. Today artificial intelligence has achieved rapid development in automation, computation, knowledge-engineering, information retrieval and processing. Techniques such as fuzzy logic, neural networks, natural language processing, transfer learning, machine learning are also contributing to enhance the performance, accuracy of recommendation system and reducing problems like cold-start, data-sparsity, privacy and scalability. This paper explains the development stages and different categories of recommender system. This paper further explores the challenges and recent trends in building recommendation system.\n",
            "----\n",
            "Paper 773:\n",
            "Title: The promise of phase-change materials\n",
            "Abstract: Improved nonvolatile memory materials could open up new applications The emergence of artificial intelligence and machine learning techniques and their rapid proliferation across a myriad of industries provide a driving force for improving computing, data storage, and telecommunication hardware architectures. The current generation and use of huge volumes of data will only increase as smart devices, such as autonomous vehicles, utility meters, and medical implants, report out data. To manage this torrent of information, fast data storage and processing devices with low noise and drift that can be manufactured with small physical footprints, in increasingly more intelligent computing architectures, are needed. On page 210 of this issue, Ding et al. (1) demonstrate chalcogenide phasechange heterostructures that take advantage of thin phase-change layers interspaced with nanoscale diffusion barriers. The resulting phase-change electronic memories have ultralow noise, drift, and high endurance and may have wide-ranging applications beyond traditional data storage.\n",
            "----\n",
            "Paper 774:\n",
            "Title: Greed is Still Good: Maximizing Monotone Submodular+Supermodular Functions\n",
            "Abstract: We analyze the performance of the greedy algorithm, and also a discrete semi-gradient based algorithm, for maximizing the sum of a suBmodular and suPermodular (BP) function (both of which are non-negative monotone non-decreasing) under two types of constraints, either a cardinality constraint or $p\\geq 1$ matroid independence constraints. These problems occur naturally in several real-world applications in data science, machine learning, and artificial intelligence. The problems are ordinarily inapproximable to any factor (as we show). Using the curvature $\\kappa_f$ of the submodular term, and introducing $\\kappa^g$ for the supermodular term (a natural dual curvature for supermodular functions), however, both of which are computable in linear time, we show that BP maximization can be efficiently approximated by both the greedy and the semi-gradient based algorithm. The algorithms yield multiplicative guarantees of $\\frac{1}{\\kappa_f}\\left[1-e^{-(1-\\kappa^g)\\kappa_f}\\right]$ and $\\frac{1-\\kappa^g}{(1-\\kappa^g)\\kappa_f + p}$ for the two types of constraints respectively. For pure monotone supermodular constrained maximization, these yield $1-\\kappa^g$ and $(1-\\kappa^g)/p$ for the two types of constraints respectively. We also analyze the hardness of BP maximization and show that our guarantees match hardness by a constant factor and by $O(\\ln(p))$ respectively. Computational experiments are also provided supporting our analysis.\n",
            "----\n",
            "Paper 775:\n",
            "Title: Countering misinformation: A multidisciplinary approach\n",
            "Abstract: The article explores the concept of infodemics during the COVID-19 pandemic, focusing on the propagation of false or inaccurate information proliferating worldwide throughout the SARS-CoV-2 health crisis. We provide an overview of disinformation, misinformation and malinformation and discuss the notion of “fake news”, and highlight the threats these phenomena bear for health policies and national and international security. We discuss the mis-/disinformation as a significant challenge to the public health, intelligence, and policymaking communities and highlight the necessity to design measures enabling the prevention, interdiction, and mitigation of such threats. We then present an overview of selected opportunities for applying technology to study and combat disinformation, outlining several approaches currently being used to understand, describe, and model the phenomena of misinformation and disinformation. We focus specifically on complex networks, machine learning, data- and text-mining methods in misinformation detection, sentiment analysis, and agent-based models of misinformation spreading and the detection of misinformation sources in the network. We conclude with the set of recommendations supporting the World Health Organization’s initiative on infodemiology. We support the implementation of integrated preventive procedures and internationalization of infodemic management. We also endorse the application of the cross-disciplinary methodology of Crime Science discipline, supplemented by Big Data analysis and related information technologies to prevent, disrupt, and detect mis- and disinformation efficiently.\n",
            "----\n",
            "Paper 776:\n",
            "Title: MICAI 2007: Advances in Artificial Intelligence, 6th Mexican International Conference on Artificial Intelligence, Aguascalientes, Mexico, November 4-10, 2007, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 777:\n",
            "Title: Editorial introduction: Towards a machinic anthropology\n",
            "Abstract: Bringing together a motley crew of social scientists and data scientists, the aim of this special theme issue is to explore what an integration or even fusion between anthropology and data science might look like. Going beyond existing work on the complementarity between ‘thick’ qualitative and ‘big’ quantitative data, the ambition is to unsettle and push established disciplinary, methodological and epistemological boundaries by creatively and critically probing various computational methods for augmenting and automatizing the collection, processing and analysis of ethnographic data, and vice versa. Can ethnographic and other qualitative data and methods be integrated with natural language processing tools and other machine-learning techniques, and if so, to what effect? Does the rise of data science allow for the realization of Levi-Strauss’ old dream of a computational structuralism, and even if so, should it? Might one even go as far as saying that computers are now becoming agents of social scientific analysis or even thinking: are we about to witness the birth of distinctly anthropological forms of artificial intelligence? By exploring these questions, the hope is not only to introduce scholars and students to computational anthropological methods, but also to disrupt predominant norms and assumptions among computational social scientists and data science writ large.\n",
            "----\n",
            "Paper 778:\n",
            "Title: Predicting Stock Market Price of Bangladesh: A Comparative Study of Linear Classification Models\n",
            "Abstract: None\n",
            "----\n",
            "Paper 779:\n",
            "Title: MICAI 2005: Advances in Artificial Intelligence, 4th Mexican International Conference on Artificial Intelligence, Monterrey, Mexico, November 14-18, 2005, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 780:\n",
            "Title: NAttack! Adversarial Attacks to bypass a GAN based classifier trained to detect Network intrusion\n",
            "Abstract: With the recent developments in artificial intelligence and machine learning, anomalies in network traffic can be detected using machine learning approaches. Before the rise of machine learning, network anomalies which could imply an attack, were detected using well-crafted rules. An attacker who has knowledge in the field of cyber-defence, could make educated guesses to sometimes accurately predict which particular features of network traffic data the cyber-defence mechanism is looking at. With this information, the attacker can circumvent a rulebased cyber-defense system. However, after the advancements of machine learning for network anomaly, it is not easy for a human to understand how to bypass a cyber-defence system. Recently, adversarial attacks have become increasingly common to defeat machine learning algorithms. In this paper, we show that even if we build a classifier and train it with adversarial examples for network data, we can use adversarial attacks and successfully break the system. We propose a Generative Adversarial Network (GAN) based algorithm to generate data to train an efficient neural network based classifier, and we subsequently break the system using adversarial attacks.\n",
            "----\n",
            "Paper 781:\n",
            "Title: A Review of Microsoft Academic Services for Science of Science Studies\n",
            "Abstract: Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.\n",
            "----\n",
            "Paper 782:\n",
            "Title: Unassisted noise reduction of chemical reaction datasets\n",
            "Abstract: None\n",
            "----\n",
            "Paper 783:\n",
            "Title: Bioinformatics and medicine in the era of deep learning\n",
            "Abstract: Many of the current scientific advances in the life sciences have their origin in the intensive use of data for knowledge discovery. In no area this is so clear as in bioinformatics, led by technological breakthroughs in data acquisition technologies. It has been argued that bioinformatics could quickly become the field of research generating the largest data repositories, beating other data-intensive areas such as high-energy physics or astroinformatics. Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large. In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic.\n",
            "----\n",
            "Paper 784:\n",
            "Title: A Systematic Review on the Use of AI and ML for Fighting the COVID-19 Pandemic\n",
            "Abstract: Artificial intelligence (AI) and machine learning (ML) have caused a paradigm shift in healthcare that can be used for decision support and forecasting by exploring medical data. Recent studies have shown that AI and ML can be used to fight COVID-19. The objective of this article is to summarize the recent AI- and ML-based studies that have addressed the pandemic. From an initial set of 634 articles, a total of 49 articles were finally selected through an inclusion-exclusion process. In this article, we have explored the objectives of the existing studies (i.e., the role of AI/ML in fighting the COVID-19 pandemic); the context of the studies (i.e., whether it was focused on a specific country-context or with a global perspective; the type and volume of the dataset; and the methodology, algorithms, and techniques adopted in the prediction or diagnosis processes). We have mapped the algorithms and techniques with the data type by highlighting their prediction/classification accuracy. From our analysis, we categorized the objectives of the studies into four groups: disease detection, epidemic forecasting, sustainable development, and disease diagnosis. We observed that most of these studies used deep learning algorithms on image-data, more specifically on chest X-rays and CT scans. We have identified six future research opportunities that we have summarized in this paper. Impact Statement: Artificial intelligence (AI) and machine learning(ML) methods have been widely used to assist in the fight against COVID-19 pandemic. A very few in-depth literature reviews have been conducted to synthesize the knowledge and identify future research agenda including a previously published review on data science for COVID-19 in this article. In this article, we synthesized reviewed recent literature that focuses on the usages and applications of AI and ML to fight against COVID-19. We have identified seven future research directions that would guide researchers to conduct future research. The most significant of these are: develop new treatment options, explore the contextual effect and variation in research outcomes, support the health care workforce, and explore the effect and variation in research outcomes based on different types of data.\n",
            "----\n",
            "Paper 785:\n",
            "Title: Against generalisation: Data-driven decisions need context to be human-compatible\n",
            "Abstract: During the past two decades, there have been a number of breakthroughs in the fields of data science and artificial intelligence, made possible by advanced machine learning algorithms trained through access to massive volumes of data. However, their adoption and use in real-world applications remains a challenge. This paper posits that a key limitation in making AI applicable has been a failure to modernise the theoretical frameworks needed to evaluate and adopt outcomes. Such a need was anticipated with the arrival of the digital computer in the 1950s but has remained unrealised. This paper reviews how the field of data science emerged and led to rapid breakthroughs in algorithms underpinning research into artificial intelligence. It then discusses the contextual framework now needed to advance the use of AI in real-world decisions that impact human lives and livelihoods.\n",
            "----\n",
            "Paper 786:\n",
            "Title: Knowledge, Skill and Artificial Intelligence\n",
            "Abstract: None\n",
            "----\n",
            "Paper 787:\n",
            "Title: Human intervention in automated decision-making: Toward the construction of contestable systems\n",
            "Abstract: Concerns about \"black box\" machine learning algorithms have influenced why modern data protection laws and regulations on their establishment of a right to human intervention on decision-making supported by artificial intelligence. Such interventions provide data subjects with means to protect their rights, freedoms, and legitimate interests, either as a bare minimum requirement for data processing or as a central norm governing decision-aiding artificial intelligence. In this paper, I present contestability by design as an approach to two kinds of issues with current legal implementations of the right to human intervention. The first kind is the uncertainty about what kind of decision should be covered by this right: should intervention be restricted to those decisions with no human involvement, or should it be interpreted in a broader sense, encompassing all decisions that are effectively shaped by automated processing? The second class of issues ensues from practical limitations of this right to intervention: even within a clear conceptual framework, data subjects might still lack the information they need to the concrete exercise of their right, or the human intervention itself might introduce biases and limitations that result in undesirable outcomes. After discussing how those effects can be identified and measured, I then advance the thesis that proper protection of the rights of data subjects is feasible only if there are means for contesting decisions based solely on automated processing is not an afterthought, but instead a requirement at each stage of an artificial intelligence system's lifecycle.\n",
            "----\n",
            "Paper 788:\n",
            "Title: Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks\n",
            "Abstract: A novel financial time-series analysis method based on deep learning technique is proposed in this paper. In recent years, the explosive growth of deep learning researches have led to several successful applications in various artificial intelligence and multimedia fields, such as visual recognition, robot vision, and natural language processing. In this paper, we focus on the time-series data processing and prediction in financial markets. Traditional feature extraction approaches in intelligent trading decision support system are used to applying several technical indicators and expert rules to extract numerical features. The major contribution of this paper is to improve the algorithmic trading framework with the proposed planar feature representation methods and deep convolutional neural networks (CNN). The proposed system is implemented and benchmarked in the historical datasets of Taiwan Stock Index Futures. The experimental results show that the deep learning technique is effective in our trading simulation application, and may have greater potentialities to model the noisy financial data and complex social science problems. In the future, we expected that the proposed methods and deep learning framework could be applied to more innovative applications in the next financial technology (FinTech) generation.\n",
            "----\n",
            "Paper 789:\n",
            "Title: AI-Natural Language Processing (NLP)\n",
            "Abstract: Natural Language Processing (NLP) could be a branch of Artificial Intelligence (AI) that allows machines to know the human language. Its goal is to form systems that can make sense of text and automatically perform tasks like translation, spell check, or topic classification. Natural language processing (NLP) has recently gained much attention for representing and analysing human language computationally. It's spread its applications in various fields like computational linguistics, email spam detection, information extraction, summarization, medical, and question answering etc. The goal of the Natural Language Processing is to style and build software system which will analyze, understand, and generate languages that humans use naturally, so as that you just could also be ready to address your computer as if you were addressing another person. Because it’s one amongst the oldest area of research in machine learning it’s employed in major fields like artificial intelligence speech recognition and text processing. Natural language processing has brought major breakthrough within the sector of COMPUTATION AND AI.\n",
            "----\n",
            "Paper 790:\n",
            "Title: Long-term hail risk assessment with deep neural networks\n",
            "Abstract: Hail risk assessment is necessary to estimate and reduce damage to crops, orchards, and infrastructure. Also, it helps to estimate and reduce consequent losses for businesses and, particularly, insurance companies. But hail forecasting is challenging. Data used for designing models for this purpose are tree-dimensional geospatial time series. Hail is a very local event with respect to the resolution of available datasets. Also, hail events are rare - only 1% of targets in observations are marked as\"hail\". Models for nowcasting and short-term hail forecasts are improving. Introducing machine learning models to the meteorology field is not new. There are also various climate models reflecting possible scenarios of climate change in the future. But there are no machine learning models for data-driven forecasting of changes in hail frequency for a given area. The first possible approach for the latter task is to ignore spatial and temporal structure and develop a model capable of classifying a given vertical profile of meteorological variables as favorable to hail formation or not. Although such an approach certainly neglects important information, it is very light weighted and easily scalable because it treats observations as independent from each other. The more advanced approach is to design a neural network capable to process geospatial data. Our idea here is to combine convolutional layers responsible for the processing of spatial data with recurrent neural network blocks capable to work with temporal structure. This study compares two approaches and introduces a model suitable for the task of forecasting changes in hail frequency for ongoing decades.\n",
            "----\n",
            "Paper 791:\n",
            "Title: Experience of Developing and Implementation of the Virtual Case Environment in Physics Learning by Google Services\n",
            "Abstract: The development of information and communication technologies (ICT), artificial intelligence, cloud computing, data science, digital platforms etc., causes changes in methodical means of teaching to increase the use of educational computer systems. Today, open information systems, global educational networks, digital resources going beyond the boundaries of separate educational institutions are developed. For example, Google services allow teachers to design the virtual educational environment with electronic educational resources, taking into account the individual characteristics of the students, the geographic region and the social structure of the country. In recent years, the education of Ukraine is oriented on the STEM education. The STEM centers, courses, curricula, information resources are developed. It promotes the choice of young people in engineering and technological professions. In the article, we analyze the particularity of Physics as a STEM discipline, and describe the experience of developing and implementation of the virtual case environment in the Physics learning by means of Google services. It promotes the involvement of students in research, the use of digital technologies in Physics learning. The implementation of the ideas of STEM-education on the basis of the Laboratory of Physics and Educational Technologies of the Kherson State University is presented.\n",
            "----\n",
            "Paper 792:\n",
            "Title: Supervised and unsupervised pattern recognition: feature extraction and computational intelligence\n",
            "Abstract: classifiers-an overview Criteria for optimal classifier design Categorizing the Classifiers Classifiers Neural Networks Comparison of Experimental Results System Performance Assessment Analysis of Prediction Rates from Bootstrapping Assessment ARTIFICIAL NEURAL NETWORKS: DEFINITIONS, METHODS, APPLICATIONS Definitions Training Algorithm Some Applications A SYSTEM FOR HANDWRITTEN DIGIT RECOGNITION Preprocessing of Handwritten Digit Images Zernike Moments (ZM) for Characterization of Image Patterns Dimensionality Reduction Analysis of Prediction Error Rates from Bootstrapping Assessment Summary OTHER TYPES OF FEATURE EXTRACTION METHODS Introduction Wavelets Invariant Moments Entropy Cepstrum Analysis Fractal Dimension Entropy SGLD Texture Features FUZZY NEURAL NETWORKS Pattern Recognition Optimization System Design Clustering APPLICATION TO HANDWRITTEN DIGITS Introduction to Character Recognition Data Collection Results Discussion Summary A UNSUPERVISED NEURAL NETWORK SYSTEM FOR VISUAL EVOKED POTENTIALS Data Collection and Preprocessing System Design Results Discussion CLASSIFICATION OF MAMMOGRAMS USING A MODULAR NEURAL NETWORK Methods and System Overview Modular Neural Networks Neural Network Training Classification Results The Process of Obtaining Results ALOPEX Parameters Generalization Conclusions \"VISUAL OPHTHALMOLOGIST\": AN AUTOMATED SYSTEM FOR CLASSIFICATION OF RETINAL DAMAGE System Overview Modular Neural Networks Applications to Ophthalmology Results Discussion A THREE-DIMENSIONAL NEURAL NETWORK ARCHITECTURE The Neural Network Architecture Simulations Discussion A FEATURE EXTRACTION ALGORITHM USING CONNECTIVITY STRENGTHS AND MOMENT INVARIANTS ALOPEX Algorithms Moment Invariants and ALOPEX Results and Discussion MULTILAYER PERCEPTRONS WITH ALOPEX: 2D-TEMPLATE MATCHING AND VLSI IMPLEMENTATION Multilayer Perceptron and Template Matching VLSI Implementation of ALOPEX IMPLEMENTING NEURAL NETWORKS IN SILICON The Living Neuron Neuromorphic Models Neurological Process Modeling SPEAKER IDENTIFICATION THROUGH WAVELET MULTIRESOLUTION DECOMPOSITION AND ALOPEX Multiresolution Analysis through Wavelet Decomposition Pattern Recognition with ALOPEX Methods Results Discussion FACE RECOGNITION IN ALZHEIMER'S DISEASE: A SIMULATION Methods Results Discussion SELF-LEARNING LAYERED NEURAL NETWORKS Neocognition and Pattern Classification Objectives Methods Study A Study B Summary and Discussion BIOLOGICAL AND MACHINE VISION Distributed Representation The Model A Modified ALOPEX Algorithm Application to Template Matching Brain-to-Computer Link Discussion Each section also has an introduction and references\n",
            "----\n",
            "Paper 793:\n",
            "Title: REFORM: Error-Aware Few-Shot Knowledge Graph Completion\n",
            "Abstract: Knowledge graphs (KGs) are of great importance in various artificial intelligence systems, such as question answering, relation extraction, and recommendation. Nevertheless, most real-world KGs are highly incomplete, with many missing relations between entities. To discover new triples (i.e., head entity, relation, tail entity), many KG completion algorithms have been proposed in recent years. However, a vast majority of existing studies often require a large number of training triples for each relation, which contradicts the fact that the frequency distribution of relations in KGs often follows a long tail distribution, meaning a majority of relations have only very few triples. Meanwhile, since most existing large-scale KGs are constructed automatically by extracting information from crowd-sourcing data using heuristic algorithms, plenty of errors could be inevitably incorporated due to the lack of human verification, which greatly reduces the performance for KG completion. To tackle the aforementioned issues, in this paper, we study a novel problem of error-aware few-shot KG completion and present a principled KG completion framework REFORM. Specifically, we formulate the problem under the few-shot learning framework, and our goal is to accumulate meta-knowledge across different meta-tasks and generalize the accumulated knowledge to the meta-test task for error-aware few-shot KG completion. To address the associated challenges resulting from insufficient training samples and inevitable errors, we propose three essential modules neighbor encoder, cross-relation aggregation, and error mitigation in each meta-task. Extensive experiments on three widely used KG datasets demonstrate the superiority of the proposed framework REFORM over competitive baseline methods.\n",
            "----\n",
            "Paper 794:\n",
            "Title: MICAI 2009: Advances in Artificial Intelligence, 8th Mexican International Conference on Artificial Intelligence, Guanajuato, Mexico, November 9-13, 2009. Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Paper 795:\n",
            "Title: Remaining Useful Life Prediction of Lithium-ion Battery based on Attention Mechanism with Positional Encoding\n",
            "Abstract: The rising demands of more reliable and stable electrical systems attach importance to accurate Remaining Useful Life (RUL) prediction of the lithium-ion batteries. As artificial intelligence and machine learning techniques advance, data-driven methods especially deep learning algorithms have become the rising star in RUL prediction. Recurrent Neural Networks (RNNs) and their variants such as Long Short Term Memory have proven effectiveness in various sequential tasks. However, due to its iterative nature along the time axis, RNNs take much time for information to flow through the network for prediction. Inspired by recent advance brought by Transformer in sequence transduction tasks, we proposed the attention mechanism based Convolutional Neural Network (CNN) with positional encoding to tackle this problem. The attention mechanism enables the network to focus on specific parts of sequences and positional encoding injects position information while utilizing the parallelization merits of CNN on GPUs. Empirical experiments show that the proposed approach is both time effective and accurate in battery RUL prediction.\n",
            "----\n",
            "Paper 796:\n",
            "Title: Classification of Deep-SAT Images under Label Noise\n",
            "Abstract: ABSTRACT One of the challenges of training artificial intelligence models for classifying satellite images is the presence of label noise in the datasets that are sometimes crowd-source labeled and as a result, somewhat error prone. In our work, we have utilized three labeled satellite image datasets namely, SAT-6, SAT-4, and EuroSAT. The combined dataset consists of over 900,000 image patches that belong to a land cover class. We have applied some standard pixel-based feature extraction algorithms to extract features from the images and then trained those features with various machine learning algorithms. In our experiment, three types of artificial label noises are injected – Noise Completely at Random (NCAR), Noise at Random (NAR) and Noise Not at Random (NNAR) to the training datasets. The noisy data are used to train the algorithms, and the effect of noise on the algorithm performance are compared with noise-free test sets. From our study, the Random Forest and the Back-propagation Neural Network classifiers are found to be the least sensitive to label noises. As label noise is a common scenario in human-labeled image datasets, the current research initiative will help the development of noise robust classification methods for various relevant applications.\n",
            "----\n",
            "Paper 797:\n",
            "Title: A Comprehensive Survey of AI, Blockchain Technology and Big Data Applications in Medical Field and Global Health\n",
            "Abstract: AI (Artificial Intelligence) technologies help with diagnostic procedures, patient monitoring, drug development, personalised medicine, and global health pandemic prediction. Without explicit programming, machine learning enables the technology to enhance and expand. Moreover, machine learning algorithms may use e-health records to evaluate vast quantities of data, dubbed big-data, for illness detection and prediction. Medical devices are used to continually monitor and record a person’s health condition. In light of recent research, the goal of this study is to investigate the potential advantages of extensive data analytics and machine learning (ML). We looked for articles in the literature using multiple high-quality databases, including Google Scholar, Scopus, Web of science (WoS), Scopus and PubMed. This article covers the ML (machine learning), Big Data, and Blockchain Technology in health-care, population health, and medical surveillance.\n",
            "----\n",
            "Paper 798:\n",
            "Title: Orange Software Usage in Data Mining Classification Method on The Dataset Lenses\n",
            "Abstract: Data Mining is a process that uses statistical, mathematical, artificial intelligence, and machine learning techniques to extract and identify useful information and related knowledge from various large databases. Decision tree is a very interesting classification method that involves the construction of a decision tree consisting of decision nodes which are connected by branches from the root node to the leaf node (end). The problem that will be examined in this study is about classification of dataset lenses obtained from using orange software. The method used in this paper is the method of classification process is performed on a decision tree using the orange software. Tree construction begins with the formation of roots (located at the top). Then the data is divided based on attributes that are suitable to be used as leaves. Decision rule information is to make decision rules from trees that have been formed.\n",
            "----\n",
            "Paper 799:\n",
            "Title: Autonomous driving: cognitive construction and situation understanding\n",
            "Abstract: None\n",
            "----\n",
            "Paper 800:\n",
            "Title: Machine Learning: ECML 2005, 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceedings\n",
            "Abstract: None\n",
            "----\n",
            "Rate limit exceeded. Retrying in 160 seconds... (Attempt 1/5)\n",
            "Rate limit exceeded. Retrying in 320 seconds... (Attempt 2/5)\n",
            "Rate limit exceeded. Retrying in 640 seconds... (Attempt 3/5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ6m07XoG_me"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(papers)\n",
        "data = data.drop(columns=[\"paperId\", \"title\"])\n",
        "data.to_csv(\"papers.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpva84I_IIAW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjf9DM3IK5b"
      },
      "source": [
        "1.Remove noise, such as special characters and punctuations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfhl8roeISH1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load CSV file\n",
        "data = pd.read_csv(\"papers.csv\")\n",
        "\n",
        "# Function to remove noise\n",
        "def remove_noise(text):\n",
        "    if isinstance(text, str):  # Only clean strings\n",
        "        return re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply function to all columns\n",
        "data = data.map(remove_noise)  # Use `map()` instead of `applymap()`\n",
        "\n",
        "# Save cleaned data\n",
        "data.to_csv(\"papers_cleaned.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdemFN3VJfVL"
      },
      "source": [
        "#2.Remove numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCUuMnxZJVxn",
        "outputId": "7ade8945-00c6-44c9-b7e6-53388f98c2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numbers and special characters removed. Saved as 'papers_final_cleaned.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load cleaned CSV file\n",
        "data = pd.read_csv(\"papers_cleaned.csv\")\n",
        "\n",
        "# Function to remove special characters, punctuation, and numbers\n",
        "def remove_noise(text):\n",
        "    if isinstance(text, str):  # Only process strings\n",
        "        return re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply cleaning function to all string columns\n",
        "data = data.map(remove_noise)\n",
        "\n",
        "# Save the final cleaned data\n",
        "data.to_csv(\"papers_no_numbers.csv\", index=False)\n",
        "\n",
        "print(\"Numbers and special characters removed. Saved as 'papers_n.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove stopwords by using the stopwords list"
      ],
      "metadata": {
        "id": "swsaWn8ypCvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCvYyX7hKIho",
        "outputId": "f571a3d8-6ba1-48bd-a047-67afc11a450b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords removed. Saved as 'papers_no_stopwords.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data = pd.read_csv(\"papers_no_numbers.csv\")\n",
        "\n",
        "# Get the stopwords list from NLTK, not from the DataFrame\n",
        "stopwords_list = set(stopwords.words('english'))  # Use NLTK's stopwords\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):  # Ensure text processing only for strings\n",
        "        words = text.split()  # Split text into words\n",
        "        filtered_words = [word for word in words if word.lower() not in stopwords_list]  # Remove stopwords\n",
        "        return \" \".join(filtered_words)  # Join words back into a sentence\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply function to all string columns\n",
        "data = data.map(remove_stopwords) # Use map to apply the function element-wise\n",
        "\n",
        "\n",
        "# Save the final cleaned data\n",
        "data.to_csv(\"papers_no_stopwords.csv\", index=False)\n",
        "\n",
        "print(\"Stopwords removed. Saved as 'papers_no_stopwords.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LowerCase\n"
      ],
      "metadata": {
        "id": "vobis-Y8ojlq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQkHTHmlK2l3",
        "outputId": "fdd7c4fc-efe9-4b54-9688-a82166d4ee2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text converted to lowercase and stopwords removed. Saved as 'papers_lowercase_no_stopwords.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data = pd.read_csv(\"papers_no_stopwords.csv\")\n",
        "\n",
        "# Load stopwords (either from NLTK or from a column in the dataset)\n",
        "stopwords_list = set(stopwords.words('english'))  # Use NLTK's English stopwords\n",
        "\n",
        "# Function to process text: lowercase + remove stopwords\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):  # Ensure only strings are processed\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        words = text.split()  # Split text into words\n",
        "        filtered_words = [word for word in words if word not in stopwords_list]  # Remove stopwords\n",
        "        return \" \".join(filtered_words)  # Join words back into a sentence\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply function to all string columns\n",
        "data = data.map(clean_text)\n",
        "\n",
        "# Save the final cleaned data\n",
        "data.to_csv(\"papers_lowercase_no_stopwords.csv\", index=False)\n",
        "\n",
        "print(\"Text converted to lowercase and stopwords removed. Saved as 'papers_lowercase_no_stopwords.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming\n"
      ],
      "metadata": {
        "id": "J2Y8aNKyocn6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIDqEEz5LtQ_",
        "outputId": "271bb7eb-dfda-4a79-d854-55d9b315d227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text converted to lowercase, stopwords removed, and stemming applied. Saved as 'papers_stemmed.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required resources if not already available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data = pd.read_csv(\"papers_lowercase_no_stopwords.csv\")\n",
        "\n",
        "# Initialize stopwords list (using NLTK)\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to process text: lowercase + remove stopwords + stemming\n",
        "def clean_and_stem(text):\n",
        "    if isinstance(text, str):  # Ensure only strings are processed\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        words = text.split()  # Split text into words\n",
        "        filtered_words = [stemmer.stem(word) for word in words if word not in stopwords_list]  # Remove stopwords & stem words\n",
        "        return \" \".join(filtered_words)  # Join words back into a sentence\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply function to all string columns\n",
        "data = data.map(clean_and_stem)\n",
        "\n",
        "# Save the final cleaned data\n",
        "data.to_csv(\"papers_stemmed.csv\", index=False)\n",
        "\n",
        "print(\"Text converted to lowercase, stopwords removed, and stemming applied. Saved as 'papers_stemmed.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lemmatization\n"
      ],
      "metadata": {
        "id": "hbZPc-CooUMl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKahH0uoLyns",
        "outputId": "26f40517-14ed-4be3-e7be-3242912700a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text converted to lowercase, stopwords removed, and lemmatization applied. Saved as 'papers_lemmatized.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required resources if not already available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data = pd.read_csv(\"papers_lowercase_no_stopwords.csv\")\n",
        "\n",
        "# Initialize stopwords list (using NLTK)\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to process text: lowercase + remove stopwords + lemmatization\n",
        "def clean_and_lemmatize(text):\n",
        "    if isinstance(text, str):  # Ensure only strings are processed\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        words = text.split()  # Split text into words\n",
        "        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords_list]  # Remove stopwords & lemmatize words\n",
        "        return \" \".join(filtered_words)  # Join words back into a sentence\n",
        "    return text  # Return non-string values unchanged\n",
        "\n",
        "# Apply function to all string columns\n",
        "data = data.map(clean_and_lemmatize)\n",
        "\n",
        "# Save the final cleaned data\n",
        "data.to_csv(\"papers_lemmatized.csv\", index=False)\n",
        "\n",
        "print(\"Text converted to lowercase, stopwords removed, and lemmatization applied. Saved as 'papers_lemmatized.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK5XO-xJKha9",
        "outputId": "b47725ae-06be-4db9-c87a-4cdefdad9682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging complete and counts saved as 'papers_with_pos_counts.csv'\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK resources if not already available\n",
        "# nltk.download('punkt') # This was causing issues with lookup\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the required data package\n",
        "\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(\"papers_lemmatized.csv\")\n",
        "\n",
        "# Function to perform POS tagging and count the occurrences of Nouns, Verbs, Adjectives, Adverbs\n",
        "def pos_count(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string\n",
        "        tokens = word_tokenize(text)  # Tokenize the text\n",
        "        tagged_tokens = pos_tag(tokens)  # POS tagging\n",
        "\n",
        "        # Initialize counters\n",
        "        noun_count = 0\n",
        "        verb_count = 0\n",
        "        adj_count = 0\n",
        "        adv_count = 0\n",
        "\n",
        "        # Iterate through POS tags and count Nouns, Verbs, Adjectives, and Adverbs\n",
        "        for word, tag in tagged_tokens:\n",
        "            if tag.startswith('NN'):  # Nouns: NN, NNS, NNP, NNPS\n",
        "                noun_count += 1\n",
        "            elif tag.startswith('VB'):  # Verbs: VB, VBD, VBG, VBN, VBP, VBZ\n",
        "                verb_count += 1\n",
        "            elif tag.startswith('JJ'):  # Adjectives: JJ, JJR, JJS\n",
        "                adj_count += 1\n",
        "            elif tag.startswith('RB'):  # Adverbs: RB, RBR, RBS\n",
        "                adv_count += 1\n",
        "\n",
        "        return noun_count, verb_count, adj_count, adv_count\n",
        "    return 0, 0, 0, 0  # Return zeros if not a string\n",
        "\n",
        "# Apply POS tagging and counting to the \"abstract\" column\n",
        "data[['Nouns', 'Verbs', 'Adjectives', 'Adverbs']] = data['abstract'].apply(lambda x: pd.Series(pos_count(x)))\n",
        "\n",
        "# Save the updated CSV with the POS counts\n",
        "data.to_csv(\"papers_with_pos_counts.csv\", index=False)\n",
        "\n",
        "print(\"POS tagging complete and counts saved as 'papers_with_pos_counts.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "Y0oOSlsOS0cq",
        "outputId": "058eb300-2be9-4ae9-d3bc-aef0df1a1184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing Tree:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"67fc8ae63fb844f7b884ace0b6ab2986-0\" class=\"displacy\" width=\"3725\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">goal</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">precipit</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">nowcast</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">predict</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">futur</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">rainfal</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">intens</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">local</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">region</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">rel</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">short</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">period</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">time</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">previou</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">studi</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">examin</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">crucial</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">challeng</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">weather</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">forecast</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">problem.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,352.0 380.0,352.0 380.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,439.5 375.0,439.5 375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-3\" stroke-width=\"2px\" d=\"M770,527.0 C770,439.5 900.0,439.5 900.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,529.0 L762,517.0 778,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-4\" stroke-width=\"2px\" d=\"M945,527.0 C945,439.5 1075.0,439.5 1075.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,529.0 L937,517.0 953,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-5\" stroke-width=\"2px\" d=\"M595,527.0 C595,352.0 1080.0,352.0 1080.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1080.0,529.0 L1088.0,517.0 1072.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,439.5 1425.0,439.5 1425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-7\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,439.5 1600.0,439.5 1600.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,529.0 L1462,517.0 1478,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-8\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,352.0 1605.0,352.0 1605.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1605.0,529.0 L1613.0,517.0 1597.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-9\" stroke-width=\"2px\" d=\"M1820,527.0 C1820,439.5 1950.0,439.5 1950.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,529.0 L1812,517.0 1828,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-10\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,439.5 2125.0,439.5 2125.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1995,529.0 L1987,517.0 2003,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-11\" stroke-width=\"2px\" d=\"M2170,527.0 C2170,89.5 3545.0,89.5 3545.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2170,529.0 L2162,517.0 2178,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-12\" stroke-width=\"2px\" d=\"M2345,527.0 C2345,352.0 2655.0,352.0 2655.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2345,529.0 L2337,517.0 2353,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,439.5 2650.0,439.5 2650.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-14\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,177.0 3540.0,177.0 3540.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2695,529.0 L2687,517.0 2703,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-15\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,264.5 3535.0,264.5 3535.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,529.0 L2862,517.0 2878,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-16\" stroke-width=\"2px\" d=\"M3045,527.0 C3045,352.0 3530.0,352.0 3530.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3045,529.0 L3037,517.0 3053,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-17\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,439.5 3350.0,439.5 3350.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3220,529.0 L3212,517.0 3228,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-18\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,439.5 3525.0,439.5 3525.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3395,529.0 L3387,517.0 3403,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-19\" stroke-width=\"2px\" d=\"M595,527.0 C595,2.0 3550.0,2.0 3550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-67fc8ae63fb844f7b884ace0b6ab2986-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3550.0,529.0 L3558.0,517.0 3542.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the pre-trained spaCy model for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a sample sentence\n",
        "sentence = \"goal precipit nowcast predict futur rainfal intens local region rel short period time previou studi examin crucial challeng weather forecast problem.\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Display the Dependency Parsing Tree\n",
        "print(\"Dependency Parsing Tree:\")\n",
        "displacy.serve(doc, style=\"dep\")  # Open in a browser window\n",
        "\n",
        "# For Constituency Parsing, spaCy doesn't provide a direct visualization like dependency trees,\n",
        "# but you can use other libraries (e.g., `nltk` or `bert`) to generate constituency trees.\n",
        "# However, spaCy's 'parse' can be interpreted as a basic constituency structure in tree format.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the pre-trained spaCy model for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract and count entities\n",
        "def extract_entities(texts):\n",
        "    # Initialize counters for different types of entities\n",
        "    person_count = Counter()\n",
        "    org_count = Counter()\n",
        "    loc_count = Counter()\n",
        "    product_count = Counter()\n",
        "    date_count = Counter()\n",
        "\n",
        "    # Process each text using spaCy\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Extract and count entities\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == 'PERSON':\n",
        "                person_count[ent.text] += 1\n",
        "            elif ent.label_ == 'ORG':\n",
        "                org_count[ent.text] += 1\n",
        "            elif ent.label_ == 'GPE':  # Geopolitical entity (countries, cities, states)\n",
        "                loc_count[ent.text] += 1\n",
        "            elif ent.label_ == 'PRODUCT':\n",
        "                product_count[ent.text] += 1\n",
        "            elif ent.label_ == 'DATE':\n",
        "                date_count[ent.text] += 1\n",
        "\n",
        "    # Print counts for each category\n",
        "    print(f\"Persons Count: {dict(person_count)}\")\n",
        "    print(f\"Organizations Count: {dict(org_count)}\")\n",
        "    print(f\"Locations Count: {dict(loc_count)}\")\n",
        "    print(f\"Product Names Count: {dict(product_count)}\")\n",
        "    print(f\"Dates Count: {dict(date_count)}\")\n",
        "\n",
        "    return person_count, org_count, loc_count, product_count, date_count\n",
        "\n",
        "\n",
        "# Sample list of clean text data\n",
        "texts = [\n",
        "    \"tensorflow machin learn system oper larg scale heterogen environ tensorflow use dataflow graph repres comput share state oper mutat state map node dataflow graph across mani machin cluster within machin across multipl comput devic includ multicor cpu generalpurpos gpu customdesign asic known tensor process unit tpu architectur give flexibl applic develop wherea previou paramet server design manag share state built system tensorflow enabl develop experi novel optim train algorithm tensorflow support varieti applic focu train infer deep neural network sever googl servic use tensorflow product releas opensourc project becom wide use machin learn research paper describ tensorflow dataflow model demonstr compel perform tensorflow achiev sever realworld applic.\"\n",
        "]\n",
        "\n",
        "# Extract entities and count them\n",
        "extract_entities(texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3wIjYcEZ28_",
        "outputId": "7f36cd22-d9bc-4c33-d8fc-d3df718ae544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persons Count: {'varieti applic': 1, 'realworld applic': 1}\n",
            "Organizations Count: {'heterogen': 1}\n",
            "Locations Count: {}\n",
            "Product Names Count: {}\n",
            "Dates Count: {'flexibl': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Counter({'varieti applic': 1, 'realworld applic': 1}),\n",
              " Counter({'heterogen': 1}),\n",
              " Counter(),\n",
              " Counter(),\n",
              " Counter({'flexibl': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2FIa-vrRfzs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy's English model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "data = pd.read_csv(\"papers_lemmatized.csv\")\n",
        "\n",
        "# Initialize counters for different entity types\n",
        "person_counter = Counter()\n",
        "organization_counter = Counter()\n",
        "location_counter = Counter()\n",
        "product_counter = Counter()\n",
        "date_counter = Counter()\n",
        "\n",
        "# Function to extract and count named entities\n",
        "def extract_named_entities(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities and count them\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            person_counter[ent.text] += 1\n",
        "        elif ent.label_ == \"ORG\":\n",
        "            organization_counter[ent.text] += 1\n",
        "        elif ent.label_ == \"GPE\":  # Geopolitical entity, often locations/countries\n",
        "            location_counter[ent.text] += 1\n",
        "        elif ent.label_ == \"PRODUCT\":\n",
        "            product_counter[ent.text] += 1\n",
        "        elif ent.label_ == \"DATE\":\n",
        "            date_counter[ent.text] += 1\n",
        "\n",
        "# Iterate over the abstracts and extract entities\n",
        "for abstract in data[\"abstract\"].dropna():\n",
        "    extract_named_entities(abstract)\n",
        "\n",
        "# Print out the counts for each entity type\n",
        "print(f\"Persons: {person_counter}\")\n",
        "print(f\"Organizations: {organization_counter}\")\n",
        "print(f\"Locations: {location_counter}\")\n",
        "print(f\"Products: {product_counter}\")\n",
        "print(f\"Dates: {date_counter}\")\n",
        "\n",
        "# Optionally, save the entity counts to a CSV file\n",
        "entity_data = {\n",
        "    \"Person\": person_counter,\n",
        "    \"Organization\": organization_counter,\n",
        "    \"Location\": location_counter,\n",
        "    \"Product\": product_counter,\n",
        "    \"Date\": date_counter\n",
        "}\n",
        "\n",
        "# Convert counts to DataFrame and save\n",
        "entity_df = pd.DataFrame({key: list(value.items()) for key, value in entity_data.items()})\n",
        "entity_df.to_csv(\"named_entities_counts.csv\", index=False)\n",
        "\n",
        "print(\"Named entities and their counts have been saved to 'named_entities_counts.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract__links(page_number):\n",
        "    base_url = \"https://github.com/marketplace?page=2&type=actions&page=\"\n",
        "    action_links = []\n",
        "\n",
        "    # Construct the URL for the current page\n",
        "    url = base_url + str(page_number)\n",
        "\n",
        "    # Make the request\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req)\n",
        "    soup = BeautifulSoup(webpage, \"html.parser\")\n",
        "\n",
        "    # Find all non-featured action items\n",
        "    for action in soup.find_all(\"div\", attrs={'data-testid': 'non-featured-item'}):\n",
        "        # Iterate through the results of find_all\n",
        "        for tag in action.find_all(\"a\", class_='marketplace-common-module__marketplace-item-link--jrIHf'):\n",
        "            if tag:\n",
        "                href = tag['href']  # Access href for each tag\n",
        "                text = tag.get_text()\n",
        "                action_links.append({\n",
        "                    'Action Name': text.strip(),  # Use extracted text\n",
        "                    'Link': f\"https://github.com{href}\",\n",
        "                    'Description': \"N/A\"\n",
        "                })\n",
        "\n",
        "    return action_links  # Return the action links found on this page\n",
        "\n",
        "def extract_description(action_links):\n",
        "    # This function loops through each action link and extracts the description\n",
        "    for action in action_links:\n",
        "        try:\n",
        "            req = Request(action['Link'], headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            webpage = urlopen(req)\n",
        "            soup = BeautifulSoup(webpage, \"html.parser\")\n",
        "\n",
        "            # Example: Find the description element using a specific class or tag\n",
        "            description_element = soup.find(\"p\", {\"dir\":\"auto\"})  # Replace with actual selector\n",
        "            if description_element:\n",
        "                action['Description'] = description_element.text.strip()\n",
        "            else:\n",
        "                action['Description'] = \"Description not found\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting description for {action['Link']}: {e}\")\n",
        "            action['Description'] = \"Error extracting description\"\n",
        "\n",
        "    return action_links  # Return the updated action_links list\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['Action Name', 'Link', 'Description'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    all_action_links = []\n",
        "    max_actions = 1000  # Limit to 1000 actions\n",
        "    action_count = 0\n",
        "    total_pages = 100  # Adjust the number of pages you want to scrape\n",
        "\n",
        "    # Step 1: Extract action links from all pages using for loop\n",
        "    for page_number in range(1, total_pages + 1):  # Loop through pages 1 to total_pages\n",
        "        print(f\"Scraping page {page_number}...\")\n",
        "\n",
        "        action_links = extract__links(page_number)\n",
        "\n",
        "        # Update the action count and stop if we reach the limit\n",
        "        for action in action_links:\n",
        "            action_count += 1\n",
        "            if action_count > max_actions:\n",
        "                break  # Stop once we've reached 1000 actions\n",
        "            all_action_links.append(action)\n",
        "\n",
        "        time.sleep(1)  # Sleep for 1 second to avoid overloading the server\n",
        "\n",
        "        # Exit if we reach the desired max actions\n",
        "        if action_count >= max_actions:\n",
        "            break  # Exit the loop if we've already collected 1000 actions\n",
        "\n",
        "    # Step 2: Extract descriptions from each action's page\n",
        "    updated_action_links = extract_description(all_action_links)\n",
        "\n",
        "    # Step 3: Save the final data to a CSV file\n",
        "    save_to_csv(updated_action_links, \"github_actions_final_data.csv\")\n",
        "    print(f\"Data saved to github_actions_final_data.csv. Total actions scraped: {len(updated_action_links)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtIjOgElddgO",
        "outputId": "02618995-1ebd-4ff5-d14d-c941f534146d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Data saved to github_actions_final_data.csv. Total actions scraped: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Download NLTK stopwords and punkt_tab for tokenization\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the missing data package\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters (optional but helps in cleaning)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Return cleaned text (optional: join tokens back to string)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('github_actions_final_data.csv')\n",
        "\n",
        "# Assuming 'Action Name' is the column you want to clean\n",
        "df['Cleaned Action Name'] = df['Action Name'].apply(clean_text)\n",
        "\n",
        "# Check the cleaned data\n",
        "print(df[['Action Name', 'Cleaned Action Name']].head())\n",
        "\n",
        "# Save cleaned data back to a new CSV file\n",
        "df.to_csv('github_actions_cleaned.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggg-5wUDUpsk",
        "outputId": "aebc7144-d999-4042-e92f-28cd6ba77fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Action Name         Cleaned Action Name\n",
            "0                TruffleHog OSS              trufflehog oss\n",
            "1                 Metrics embed               metrics embed\n",
            "2  yq - portable yaml processor  yq portable yaml processor\n",
            "3                  Super-Linter                 superlinter\n",
            "4        Gosec Security Checker      gosec security checker\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('github_actions_cleaned.csv')\n",
        "\n",
        "# 1. Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Handle missing values\n",
        "# If any missing values are found, you can either:\n",
        "# a. Fill them with a default value (e.g., \"N/A\" for 'Description' column)\n",
        "# b. Drop rows with missing values (e.g., drop rows where 'Action Name' or 'Link' is missing)\n",
        "df = df.dropna(subset=['Action Name', 'Link'])  # Drop rows where 'Action Name' or 'Link' is NaN\n",
        "\n",
        "# 2. Check for duplicates\n",
        "duplicates = df.duplicated(subset=['Action Name', 'Link']).sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Remove duplicate rows based on 'Action Name' and 'Link'\n",
        "df = df.drop_duplicates(subset=['Action Name', 'Link'])\n",
        "\n",
        "# 3. Check URL consistency (ensure that the 'Link' column contains valid GitHub URLs)\n",
        "def validate_url(url):\n",
        "    return bool(re.match(r'^https://github\\.com/', url))\n",
        "\n",
        "df['Link Valid'] = df['Link'].apply(validate_url)\n",
        "\n",
        "# Print the number of invalid URLs\n",
        "invalid_urls = df[df['Link Valid'] == False]\n",
        "print(f\"Number of invalid URLs: {len(invalid_urls)}\")\n",
        "\n",
        "# Optionally, we could drop rows with invalid URLs or fill them with a default value\n",
        "df = df[df['Link Valid'] == True]\n",
        "\n",
        "# 4. Ensure correct data types (e.g., 'Action Name' should be string, 'Link' should be string)\n",
        "df['Action Name'] = df['Action Name'].astype(str)\n",
        "df['Link'] = df['Link'].astype(str)\n",
        "\n",
        "# 5. Check if all required columns are filled (Action Name, Link, Description)\n",
        "missing_required_columns = df[['Action Name', 'Link', 'Description']].isnull().sum()\n",
        "print(\"Missing required values in 'Action Name', 'Link', or 'Description':\")\n",
        "print(missing_required_columns)\n",
        "\n",
        "# Optional: Fill missing descriptions with 'N/A' if necessary\n",
        "df['Description'] = df['Description'].fillna('N/A')\n",
        "\n",
        "# 6. Save the cleaned data to a new CSV file\n",
        "df.to_csv('github_actions_cleaned_data.csv', index=False)\n",
        "print(\"Data quality checks are complete. Cleaned data saved to 'github_actions_cleaned_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS5bUtiCWDlC",
        "outputId": "a0ace83c-9fb9-4cf0-cbfe-d02e19a7cfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            "Action Name            0\n",
            "Link                   0\n",
            "Description            0\n",
            "Cleaned Action Name    0\n",
            "dtype: int64\n",
            "Number of duplicate rows: 0\n",
            "Number of invalid URLs: 0\n",
            "Missing required values in 'Action Name', 'Link', or 'Description':\n",
            "Action Name    0\n",
            "Link           0\n",
            "Description    0\n",
            "dtype: int64\n",
            "Data quality checks are complete. Cleaned data saved to 'github_actions_cleaned_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jw25s4dA9kK",
        "outputId": "007db019-c9a0-4f75-c876-daf692c34baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from snscrape) (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from snscrape) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from snscrape) (3.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy"
      ],
      "metadata": {
        "id": "rOQ0fTCaDdMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Set your keys and tokens here\n",
        "api_key = 'Eh5eWKCLrrA7h35i87ZhtfSSi'\n",
        "api_key_secret = 'TqchFWa4VBrOfreYcuHF5k3SnSofUPDZTf8ZNorCGxXuUmA0LP'\n",
        "access_token = '1891998538302881792-LT5oyzN2ndjWitXSnbrAqMT8PupIWe'\n",
        "access_token_secret = 'mnq12NuaeI5UX6CGlLFVZBjNG8ULYjFnaD8lccEiDWOyG'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuth1UserHandler(\n",
        "    consumer_key=api_key,\n",
        "    consumer_secret=api_key_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret\n",
        ")\n",
        "api = tweepy.API(auth)\n"
      ],
      "metadata": {
        "id": "Ygv8KY_Yt0OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import time\n",
        "\n",
        "# Replace with your bearer token\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAEl%2BzQEAAAAAxHBrp5%2FZHDIOdoFf0lm%2FjolkWmE%3D8qG3eBO70dcMckqOyjRXZs7QfPb7aYPgc9MsttsRKQaqcD0Kun\"\n",
        "\n",
        "# Create a tweepy client using the bearer token\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Define your search query\n",
        "query = \"#machinelearning -is:retweet\"  # Exclude retweets\n",
        "\n",
        "# Initialize a list to hold the collected tweet data\n",
        "tweet_data = []\n",
        "\n",
        "# Function to handle rate limit and retry logic\n",
        "def get_tweets_with_retry(query, max_results=100):\n",
        "    while True:\n",
        "        try:\n",
        "            # Make the API call to fetch tweets\n",
        "            tweets = client.search_recent_tweets(\n",
        "                query=query,\n",
        "                tweet_fields=[\"created_at\", \"text\", \"author_id\"],  # Select desired fields\n",
        "                max_results=max_results  # Adjust the number of results as needed\n",
        "            )\n",
        "\n",
        "            return tweets  # Return the tweet data if successful\n",
        "\n",
        "        except tweepy.TooManyRequests as e:\n",
        "            print(f\"Rate limit exceeded. Waiting for 15 minutes... (Error: {e})\")\n",
        "            time.sleep(15 * 60)  # Sleep for 15 minutes to reset the rate limit\n",
        "            continue  # Retry the request after waiting\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "            break  # Exit if any other error occurs\n",
        "\n",
        "# Get tweets with retry logic\n",
        "tweets = get_tweets_with_retry(query)\n",
        "\n",
        "# Process the tweets\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        tweet_data.append({\n",
        "            'tweet_id': tweet.id,\n",
        "            'created_at': tweet.created_at,\n",
        "            'user': tweet.author_id,  # Store author ID instead of screen name\n",
        "            'text': tweet.text\n",
        "        })\n",
        "\n",
        "    # Display the collected tweet data\n",
        "    for tweet in tweet_data:\n",
        "        print(f\"User ID: {tweet['user']}: {tweet['text']}\")\n",
        "else:\n",
        "    print(\"No tweets found or error occurred during fetching.\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tweet_data)\n",
        "df.to_csv(\"tweets_ML.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "NbPioQV3Q3cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the scraped data from the CSV file\n",
        "tweets_df = pd.read_csv(\"tweets_ML.csv\")\n",
        "\n",
        "# 1. Remove duplicates\n",
        "tweets_df.drop_duplicates(subset=['Tweet ID'], inplace=True)\n",
        "\n",
        "# 2. Handle missing values\n",
        "# Check for missing values in critical columns like 'Text' and 'Username'\n",
        "print(\"Missing values before cleaning:\")\n",
        "print(tweets_df.isnull().sum())\n",
        "\n",
        "# If there are missing values in 'Text' or 'Username', we'll fill or drop them.\n",
        "# Example: Fill missing tweet text with \"No text available\" and drop rows with missing usernames\n",
        "tweets_df['Text'].fillna('No text available', inplace=True)\n",
        "tweets_df.dropna(subset=['Username'], inplace=True)  # Drop rows with missing Username\n",
        "\n",
        "# 3. Clean the tweet text\n",
        "# Remove URLs, mentions (@username), hashtags, and non-alphanumeric characters\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'Text' column\n",
        "tweets_df['Cleaned Text'] = tweets_df['Text'].apply(clean_text)\n",
        "\n",
        "# 4. Check for consistency\n",
        "# Normalize the 'Username' column (strip any leading/trailing spaces, lowercase)\n",
        "tweets_df['Username'] = tweets_df['Username'].str.strip().str.lower()\n",
        "\n",
        "# 5. Perform a final data quality check\n",
        "print(\"Data Quality Check after cleaning:\")\n",
        "print(tweets_df.isnull().sum())  # Ensure no missing values\n",
        "print(f\"Total rows after cleaning: {tweets_df.shape[0]}\")\n",
        "\n",
        "# Ensure consistency in columns (e.g., no missing 'Text' or 'Username')\n",
        "assert not tweets_df['Username'].isnull().any(), \"Missing values found in 'Username'.\"\n",
        "assert not tweets_df['Text'].isnull().any(), \"Missing values found in 'Text'.\"\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file for further analysis\n",
        "tweets_df.to_csv(\"cleaned_tweets_ML.csv\", index=False)\n",
        "\n",
        "# Print confirmation message\n",
        "print(\"Cleaned data saved to 'cleaned_tweets_ML.csv'.\")\n"
      ],
      "metadata": {
        "id": "YZt5GvemPCHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "\n",
        "\n",
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#It was easy to do but I faced some issues while scraping data from twitter, i enjoyed scraping data but getting issues"
      ],
      "metadata": {
        "id": "UxhLeVJtMqup"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}